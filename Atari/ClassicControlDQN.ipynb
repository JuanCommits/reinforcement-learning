{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN sobre ambientes de Classic Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/classic_control/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc6t9etEt9I2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cwHCw6PMt9I3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySRyzNz8t9I3"
   },
   "source": [
    "### Seteamos los devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zcyB176t9I3",
    "outputId": "4239691d-04a7-47de-9898-ee53cf047a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "Cuda Available: False\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on {DEVICE}\")\n",
    "print(\"Cuda Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcfjdDuQt9I4"
   },
   "source": [
    "### Seteo de seeds\n",
    "Siempre es buena práctica hacer el seteo de seeds para la reproducibilidad de los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bxW_5r15t9I5"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bYVG_TKt9I5"
   },
   "source": [
    "### Creamos el ambiente y probamos algunas de sus funciones.\n",
    "\n",
    "En este caso elegimos el CartPole pero pueden cambiarlo en la variable *ENV_NAME*.\n",
    "El ambiente CartPole tiene la ventaja de que las recompensas son positivas y es mas fácil propagar estas hacia los estados iniciales. Mountain Car tiene como recompensa -1 por cada paso que damos y esta limitado a 200 pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loVxQPrwt9I5",
    "outputId": "18b7ed97-88dd-4b1e-a2cc-b2636686bfc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions # Discrete(2)\n",
      "(4,)\n",
      "(4,),\n",
      " 1.0,\n",
      " False,\n",
      " {}\n"
     ]
    }
   ],
   "source": [
    "ENVS = [\"MountainCar-v0\", \"CartPole-v1\", \"Acrobot-v1\"]\n",
    "ENV_NAME = ENVS[1]\n",
    "\n",
    "env = gymnasium.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Actions #\",env.action_space)\n",
    "print(env.observation_space.shape)\n",
    "env.reset()\n",
    "next_state, reward, terminated, truncated, info = env.step(action=0)\n",
    "\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {terminated},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seteamos los hyperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oegpMg25t9I9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_state(obs, device):\n",
    "    return torch.tensor(obs, device=device).unsqueeze(0)\n",
    "\n",
    "#Hiperparámetros de entrenamiento del agente DQN\n",
    "TOTAL_STEPS = 1000000\n",
    "EPISODES = 1000\n",
    "STEPS = 200\n",
    "\n",
    "EPSILON_INI = .9\n",
    "EPSILON_MIN = 0.05\n",
    "EPSILON_DECAY = 50000\n",
    "EPISODE_BLOCK = 100\n",
    "EPSILON_TIME = 100000\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el ambiente que vamos a estar usando para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BsTl-pFqt10b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dim: 4, Output dim: 2\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(ENV_NAME)\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "print(f\"Input dim: {input_dim}, Output dim: {output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juan1\\anaconda3\\envs\\TallerIA\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from actor_critic_models import ActorModel, CriticModel\n",
    "from actor_critic_agent import ActorCriticAgent\n",
    "\n",
    "actor = ActorModel(input_dim, output_dim).to(DEVICE)\n",
    "critic = CriticModel(input_dim).to(DEVICE)\n",
    "\n",
    "agent = ActorCriticAgent(env, actor, process_state, BUFFER_SIZE, BATCH_SIZE, \n",
    "                LEARNING_RATE, GAMMA, epsilon_i= EPSILON_INI, \n",
    "                epsilon_f=EPSILON_MIN, epsilon_decay = EPSILON_DECAY, \n",
    "                episode_block = EPISODE_BLOCK, device=DEVICE, critic_model=critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dqn_model import DQN_Model\n",
    "# from dqn_agent import DQNAgent\n",
    "# net = DQN_Model(input_dim, output_dim).to(DEVICE)\n",
    "\n",
    "# agent = DQNAgent(env, net, process_state, BUFFER_SIZE, BATCH_SIZE, \n",
    "#                 LEARNING_RATE, GAMMA, epsilon_i= EPSILON_INI, \n",
    "#                 epsilon_f=EPSILON_MIN, epsilon_anneal_time=EPSILON_TIME,\n",
    "#                 epsilon_decay = EPSILON_DECAY, episode_block = EPISODE_BLOCK, device=DEVICE, second_model_update=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dqn_model import DQN_Model\n",
    "# from double_dqn_agent import DoubleDQNAgent\n",
    "# net = DQN_Model(input_dim, output_dim).to(DEVICE)\n",
    "# net2 = DQN_Model(input_dim, output_dim).to(DEVICE)\n",
    "\n",
    "# agent = DoubleDQNAgent(env, net, net2, process_state, BUFFER_SIZE, BATCH_SIZE, \n",
    "#                 LEARNING_RATE, GAMMA, epsilon_i= EPSILON_INI, epsilon_f=EPSILON_MIN, \n",
    "#                 epsilon_decay = EPSILON_DECAY, episode_block = EPISODE_BLOCK, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<14:54,  1.12episode/s]c:\\Users\\juan1\\Ort\\Agentes Inteligentes\\reinforcement-learning\\Atari\\actor_critic_agent.py:61: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(rewards + self.gamma * next_state_val.detach(), state_val)\n",
      "  1%|          | 9/1000 [00:01<01:24, 11.70episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Avg. Reward over the last 100 episodes 15.0 epsilon 0.9 total steps 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 104/1000 [00:06<00:44, 20.00episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 - Avg. Reward over the last 100 episodes 13.48 epsilon 0.9 total steps 1363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 203/1000 [00:10<00:34, 23.17episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200 - Avg. Reward over the last 100 episodes 9.57 epsilon 0.9 total steps 2320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 305/1000 [00:15<00:31, 22.38episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300 - Avg. Reward over the last 100 episodes 9.38 epsilon 0.9 total steps 3258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 337/1000 [00:16<00:33, 19.91episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPISODES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOTAL_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juan1\\Ort\\Agentes Inteligentes\\reinforcement-learning\\Atari\\abstract_agent.py:62\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self, number_episodes, max_steps, use_wandb)\u001b[0m\n\u001b[0;32m     60\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state, total_steps)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Ejecutar la accion, observar resultado y procesarlo como indica el algoritmo.\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m obs, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m())\n\u001b[0;32m     63\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m     65\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_processing_function(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "rewards = agent.train(EPISODES, TOTAL_STEPS, use_wandb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos las recompensas obtenidas durante el entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "average_range = EPISODE_BLOCK\n",
    "episode_ticks = int(len(rewards) / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "plt.plot(range(len(avg_rewards)), avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos un video para ver la performance del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import io\n",
    "import base64\n",
    "\n",
    "def show_video():\n",
    "  \"\"\"\n",
    "  Utility function to enable video recording of gym environment and displaying it\n",
    "  To enable video, just do \"env = wrap_env(env)\"\"\n",
    "  \"\"\"\n",
    "  mp4list = glob.glob('./video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else:\n",
    "    print(\"Could not find video\")\n",
    "\n",
    "\n",
    "def wrap_env(env):\n",
    "  \"\"\"\n",
    "  Wrapper del ambiente donde definimos un Monitor que guarda la visualizacion como un archivo de video.\n",
    "  \"\"\"\n",
    "\n",
    "  #env = Monitor(env, './video', force=True)\n",
    "  env = RecordVideo(env,video_folder='./videos', name_prefix='video', episode_trigger=lambda x: x % 2 == 1)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap_env(gymnasium.make(ENV_NAME, render_mode=\"rgb_array\"))\n",
    "observation,_ = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = agent.select_action(process_state(observation, DEVICE), train=False)\n",
    "    observation, reward, done, truncated, _ = env.step(action.item())\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env = gymnasium.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "agent.env.reset()\n",
    "agent.make_video(\"video_test.mp4\", True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
