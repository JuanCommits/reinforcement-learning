{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN sobre ambientes de Classic Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/classic_control/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc6t9etEt9I2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cwHCw6PMt9I3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySRyzNz8t9I3"
   },
   "source": [
    "### Seteamos los devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zcyB176t9I3",
    "outputId": "4239691d-04a7-47de-9898-ee53cf047a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "Cuda Available: False\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on {DEVICE}\")\n",
    "print(\"Cuda Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcfjdDuQt9I4"
   },
   "source": [
    "### Seteo de seeds\n",
    "Siempre es buena pr치ctica hacer el seteo de seeds para la reproducibilidad de los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bxW_5r15t9I5"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bYVG_TKt9I5"
   },
   "source": [
    "### Creamos el ambiente y probamos algunas de sus funciones.\n",
    "\n",
    "En este caso elegimos el CartPole pero pueden cambiarlo en la variable *ENV_NAME*.\n",
    "El ambiente CartPole tiene la ventaja de que las recompensas son positivas y es mas f치cil propagar estas hacia los estados iniciales. Mountain Car tiene como recompensa -1 por cada paso que damos y esta limitado a 200 pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loVxQPrwt9I5",
    "outputId": "18b7ed97-88dd-4b1e-a2cc-b2636686bfc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions # Discrete(2)\n",
      "(4,)\n",
      "(4,),\n",
      " 1.0,\n",
      " False,\n",
      " {}\n"
     ]
    }
   ],
   "source": [
    "ENVS = [\"MountainCar-v0\", \"CartPole-v1\"]\n",
    "ENV_NAME = ENVS[1]\n",
    "\n",
    "env = gymnasium.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Actions #\",env.action_space)\n",
    "print(env.observation_space.shape)\n",
    "env.reset()\n",
    "next_state, reward, terminated, truncated, info = env.step(action=0)\n",
    "\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {terminated},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seteamos los hyperpar치metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oegpMg25t9I9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_state(obs, device):\n",
    "    return torch.tensor(obs, device=device).unsqueeze(0)\n",
    "\n",
    "#Hiperpar치metros de entrenamiento del agente DQN\n",
    "TOTAL_STEPS = 1000000\n",
    "EPISODES = 1000\n",
    "STEPS = 200\n",
    "\n",
    "EPSILON_INI = 1\n",
    "EPSILON_MIN = 0.1\n",
    "EPSILON_DECAY = .99999\n",
    "EPISODE_BLOCK = 100\n",
    "EPSILON_TIME = 100000\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "BUFFER_SIZE = 3\n",
    "\n",
    "GAMMA = 0.999\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el ambiente que vamos a estar usando para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BsTl-pFqt10b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dim: 4, Output dim: 2\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(ENV_NAME)\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "print(f\"Input dim: {input_dim}, Output dim: {output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from actor_critic_models import ActorModel, CriticModel\n",
    "# from actor_critic_agent import ActorCriticAgent\n",
    "\n",
    "# actor = ActorModel(input_dim, output_dim).to(DEVICE)\n",
    "# critic = CriticModel(input_dim).to(DEVICE)\n",
    "\n",
    "# agent = ActorCriticAgent(env, actor, process_state, BUFFER_SIZE, BATCH_SIZE, \n",
    "#                 LEARNING_RATE, GAMMA, epsilon_i= EPSILON_INI, \n",
    "#                 epsilon_f=EPSILON_MIN, epsilon_anneal_time=EPSILON_TIME,\n",
    "#                 epsilon_decay = EPSILON_DECAY, episode_block = EPISODE_BLOCK, device=DEVICE, critic_model=critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_model import DQN_Model\n",
    "from dqn_agent import DQNAgent\n",
    "net = DQN_Model(input_dim, output_dim).to(DEVICE)\n",
    "\n",
    "agent = DQNAgent(env, net, process_state, BUFFER_SIZE, BATCH_SIZE, \n",
    "                LEARNING_RATE, GAMMA, epsilon_i= EPSILON_INI, \n",
    "                epsilon_f=EPSILON_MIN, epsilon_anneal_time=EPSILON_TIME,\n",
    "                epsilon_decay = EPSILON_DECAY, episode_block = EPISODE_BLOCK, device=DEVICE, second_model_update=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ? episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States tensor([[-0.0497, -0.0368, -0.0233, -0.0012]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.05047343 -0.23161155 -0.02331775  0.2840587 ] -\n",
      "States tensor([[-0.0505, -0.2316, -0.0233,  0.2841]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.05510566 -0.4263933  -0.01763657  0.5692971 ] -\n",
      "States tensor([[-0.0551, -0.4264, -0.0176,  0.5693]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06363352 -0.23102848 -0.00625063  0.27111056] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.0216, 0.0035],\n",
      "        [0.0224, 0.0045]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0216, 0.0224]) - torch.Size([2])\n",
      "P2 tensor([0.0216, 0.0224]) - torch.Size([2])\n",
      "P3 tensor([0.0216, 0.0224]) - torch.Size([2])\n",
      "P4 tensor([1.0216, 1.0224]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0497, -0.0368, -0.0233, -0.0012],\n",
      "        [-0.0551, -0.4264, -0.0176,  0.5693]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0505, -0.2316, -0.0233,  0.2841],\n",
      "        [-0.0636, -0.2310, -0.0063,  0.2711]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0216, 1.0224]) - torch.Size([2])\n",
      "Q-values tensor([[ 0.0286, -0.0113],\n",
      "        [ 0.0115,  0.0056]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0286],\n",
      "        [0.0056]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0636, -0.2310, -0.0063,  0.2711]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06825409 -0.0358179  -0.00082842 -0.02353728] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0302, -0.0107],\n",
      "        [ 0.0115,  0.0056]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0302, 0.0115]) - torch.Size([2])\n",
      "P2 tensor([0.0302, 0.0115]) - torch.Size([2])\n",
      "P3 tensor([0.0302, 0.0115]) - torch.Size([2])\n",
      "P4 tensor([1.0302, 1.0115]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0636, -0.2310, -0.0063,  0.2711],\n",
      "        [-0.0505, -0.2316, -0.0233,  0.2841]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0683, -0.0358, -0.0008, -0.0235],\n",
      "        [-0.0551, -0.4264, -0.0176,  0.5693]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0302, 1.0115]) - torch.Size([2])\n",
      "Q-values tensor([[0.0262, 0.0083],\n",
      "        [0.0253, 0.0072]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0083],\n",
      "        [0.0253]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0683, -0.0358, -0.0008, -0.0235]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06897045  0.15931591 -0.00129917 -0.31648147] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0224,  0.0045],\n",
      "        [ 0.0286, -0.0167]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0224, 0.0286]) - torch.Size([2])\n",
      "P2 tensor([0.0224, 0.0286]) - torch.Size([2])\n",
      "P3 tensor([0.0224, 0.0286]) - torch.Size([2])\n",
      "P4 tensor([1.0224, 1.0286]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0551, -0.4264, -0.0176,  0.5693],\n",
      "        [-0.0683, -0.0358, -0.0008, -0.0235]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0636, -0.2310, -0.0063,  0.2711],\n",
      "        [-0.0690,  0.1593, -0.0013, -0.3165]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0224, 1.0286]) - torch.Size([2])\n",
      "Q-values tensor([[ 0.0181,  0.0134],\n",
      "        [ 0.0388, -0.0034]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[ 0.0134],\n",
      "        [-0.0034]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0690,  0.1593, -0.0013, -0.3165]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06578413  0.35445637 -0.00762879 -0.60957384] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0302, -0.0107],\n",
      "        [ 0.0161, -0.0158]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0302, 0.0161]) - torch.Size([2])\n",
      "P2 tensor([0.0302, 0.0161]) - torch.Size([2])\n",
      "P3 tensor([0.0302, 0.0161]) - torch.Size([2])\n",
      "P4 tensor([1.0302, 1.0161]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0636, -0.2310, -0.0063,  0.2711],\n",
      "        [-0.0690,  0.1593, -0.0013, -0.3165]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0683, -0.0358, -0.0008, -0.0235],\n",
      "        [-0.0658,  0.3545, -0.0076, -0.6096]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0302, 1.0161]) - torch.Size([2])\n",
      "Q-values tensor([[ 0.0301,  0.0175],\n",
      "        [ 0.0373, -0.0039]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[ 0.0175],\n",
      "        [-0.0039]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0658,  0.3545, -0.0076, -0.6096]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.058695    0.5496841  -0.01982027 -0.9046498 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0172, -0.0128],\n",
      "        [ 0.0161, -0.0158]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0172, 0.0161]) - torch.Size([2])\n",
      "P2 tensor([0.0172, 0.0161]) - torch.Size([2])\n",
      "P3 tensor([0.0172, 0.0161]) - torch.Size([2])\n",
      "P4 tensor([1.0172, 1.0161]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0658,  0.3545, -0.0076, -0.6096],\n",
      "        [-0.0690,  0.1593, -0.0013, -0.3165]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0587,  0.5497, -0.0198, -0.9046],\n",
      "        [-0.0658,  0.3545, -0.0076, -0.6096]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0172, 1.0161]) - torch.Size([2])\n",
      "Q-values tensor([[0.0241, 0.0036],\n",
      "        [0.0366, 0.0030]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0036],\n",
      "        [0.0030]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0587,  0.5497, -0.0198, -0.9046]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04770132  0.3548361  -0.03791327 -0.618262  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0140, -0.0168],\n",
      "        [ 0.0172, -0.0128]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0140, 0.0172]) - torch.Size([2])\n",
      "P2 tensor([0.0139, 0.0172]) - torch.Size([2])\n",
      "P3 tensor([0.0139, 0.0172]) - torch.Size([2])\n",
      "P4 tensor([1.0139, 1.0172]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0587,  0.5497, -0.0198, -0.9046],\n",
      "        [-0.0658,  0.3545, -0.0076, -0.6096]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0477,  0.3548, -0.0379, -0.6183],\n",
      "        [-0.0587,  0.5497, -0.0198, -0.9046]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0139, 1.0172]) - torch.Size([2])\n",
      "Q-values tensor([[0.0237, 0.0164],\n",
      "        [0.0229, 0.0114]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0237],\n",
      "        [0.0114]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0477,  0.3548, -0.0379, -0.6183]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0406046   0.16026369 -0.05027851 -0.3377569 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0172, -0.0128],\n",
      "        [ 0.0230, -0.0183]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0172, 0.0230]) - torch.Size([2])\n",
      "P2 tensor([0.0172, 0.0229]) - torch.Size([2])\n",
      "P3 tensor([0.0172, 0.0229]) - torch.Size([2])\n",
      "P4 tensor([1.0172, 1.0229]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0658,  0.3545, -0.0076, -0.6096],\n",
      "        [-0.0477,  0.3548, -0.0379, -0.6183]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0587,  0.5497, -0.0198, -0.9046],\n",
      "        [-0.0406,  0.1603, -0.0503, -0.3378]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0172, 1.0229]) - torch.Size([2])\n",
      "Q-values tensor([[0.0287, 0.0159],\n",
      "        [0.0270, 0.0152]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0159],\n",
      "        [0.0270]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0406,  0.1603, -0.0503, -0.3378]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03739933  0.35606372 -0.05703364 -0.64586145] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0230, -0.0183],\n",
      "        [ 0.0125, -0.0181]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0230, 0.0125]) - torch.Size([2])\n",
      "P2 tensor([0.0229, 0.0125]) - torch.Size([2])\n",
      "P3 tensor([0.0229, 0.0125]) - torch.Size([2])\n",
      "P4 tensor([1.0229, 1.0125]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0477,  0.3548, -0.0379, -0.6183],\n",
      "        [-0.0406,  0.1603, -0.0503, -0.3378]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0406,  0.1603, -0.0503, -0.3378],\n",
      "        [-0.0374,  0.3561, -0.0570, -0.6459]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0229, 1.0125]) - torch.Size([2])\n",
      "Q-values tensor([[0.0327, 0.0200],\n",
      "        [0.0399, 0.0166]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0327],\n",
      "        [0.0166]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0374,  0.3561, -0.0570, -0.6459]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03027805  0.5519321  -0.06995087 -0.9559451 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0125, -0.0181],\n",
      "        [ 0.0230, -0.0183]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0125, 0.0230]) - torch.Size([2])\n",
      "P2 tensor([0.0125, 0.0229]) - torch.Size([2])\n",
      "P3 tensor([0.0125, 0.0229]) - torch.Size([2])\n",
      "P4 tensor([1.0125, 1.0229]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0406,  0.1603, -0.0503, -0.3378],\n",
      "        [-0.0477,  0.3548, -0.0379, -0.6183]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0374,  0.3561, -0.0570, -0.6459],\n",
      "        [-0.0406,  0.1603, -0.0503, -0.3378]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0125, 1.0229]) - torch.Size([2])\n",
      "Q-values tensor([[0.0451, 0.0210],\n",
      "        [0.0391, 0.0238]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0210],\n",
      "        [0.0391]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0303,  0.5519, -0.0700, -0.9559]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01923941  0.7479216  -0.08906978 -1.2697592 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0125, -0.0181],\n",
      "        [ 0.0139, -0.0125]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0125, 0.0139]) - torch.Size([2])\n",
      "P2 tensor([0.0125, 0.0139]) - torch.Size([2])\n",
      "P3 tensor([0.0125, 0.0139]) - torch.Size([2])\n",
      "P4 tensor([1.0125, 1.0139]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0406,  0.1603, -0.0503, -0.3378],\n",
      "        [-0.0374,  0.3561, -0.0570, -0.6459]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0374,  0.3561, -0.0570, -0.6459],\n",
      "        [-0.0303,  0.5519, -0.0700, -0.9559]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0125, 1.0139]) - torch.Size([2])\n",
      "Q-values tensor([[0.0503, 0.0255],\n",
      "        [0.0448, 0.0270]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0255],\n",
      "        [0.0270]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0192,  0.7479, -0.0891, -1.2698]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00428098  0.55404264 -0.11446496 -1.0062455 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0107, -0.0116],\n",
      "        [ 0.0126, -0.0036]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0107, 0.0126]) - torch.Size([2])\n",
      "P2 tensor([0.0106, 0.0126]) - torch.Size([2])\n",
      "P3 tensor([0.0106, 0.0126]) - torch.Size([2])\n",
      "P4 tensor([1.0106, 1.0126]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0192,  0.7479, -0.0891, -1.2698],\n",
      "        [-0.0303,  0.5519, -0.0700, -0.9559]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0043,  0.5540, -0.1145, -1.0062],\n",
      "        [-0.0192,  0.7479, -0.0891, -1.2698]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0106, 1.0126]) - torch.Size([2])\n",
      "Q-values tensor([[0.0526, 0.0592],\n",
      "        [0.0482, 0.0435]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0526],\n",
      "        [0.0435]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0043,  0.5540, -0.1145, -1.0062]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00679987  0.36061975 -0.13458987 -0.75158906] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0080, -0.0212],\n",
      "        [ 0.0107, -0.0116]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0080, 0.0107]) - torch.Size([2])\n",
      "P2 tensor([0.0080, 0.0106]) - torch.Size([2])\n",
      "P3 tensor([0.0080, 0.0106]) - torch.Size([2])\n",
      "P4 tensor([1.0080, 1.0106]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0043,  0.5540, -0.1145, -1.0062],\n",
      "        [-0.0192,  0.7479, -0.0891, -1.2698]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0068,  0.3606, -0.1346, -0.7516],\n",
      "        [-0.0043,  0.5540, -0.1145, -1.0062]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0080, 1.0106]) - torch.Size([2])\n",
      "Q-values tensor([[0.0529, 0.0505],\n",
      "        [0.0618, 0.0646]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0529],\n",
      "        [0.0618]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0068,  0.3606, -0.1346, -0.7516]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01401227  0.16758515 -0.14962165 -0.5041058 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0080, -0.0212],\n",
      "        [ 0.0107, -0.0116]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0080, 0.0107]) - torch.Size([2])\n",
      "P2 tensor([0.0080, 0.0106]) - torch.Size([2])\n",
      "P3 tensor([0.0080, 0.0106]) - torch.Size([2])\n",
      "P4 tensor([1.0080, 1.0106]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0043,  0.5540, -0.1145, -1.0062],\n",
      "        [-0.0192,  0.7479, -0.0891, -1.2698]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0068,  0.3606, -0.1346, -0.7516],\n",
      "        [-0.0043,  0.5540, -0.1145, -1.0062]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0080, 1.0106]) - torch.Size([2])\n",
      "Q-values tensor([[0.0628, 0.0495],\n",
      "        [0.0732, 0.0637]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0628],\n",
      "        [0.0732]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0140,  0.1676, -0.1496, -0.5041]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01736397 -0.02514649 -0.15970376 -0.2620631 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0175, -0.0266],\n",
      "        [ 0.0091, -0.0260]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0175, 0.0091]) - torch.Size([2])\n",
      "P2 tensor([0.0175, 0.0091]) - torch.Size([2])\n",
      "P3 tensor([0.0175, 0.0091]) - torch.Size([2])\n",
      "P4 tensor([1.0175, 1.0091]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0140,  0.1676, -0.1496, -0.5041],\n",
      "        [ 0.0068,  0.3606, -0.1346, -0.7516]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0174, -0.0251, -0.1597, -0.2621],\n",
      "        [ 0.0140,  0.1676, -0.1496, -0.5041]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0175, 1.0091]) - torch.Size([2])\n",
      "Q-values tensor([[0.0570, 0.0299],\n",
      "        [0.0624, 0.0370]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0570],\n",
      "        [0.0624]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0174, -0.0251, -0.1597, -0.2621]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01686104 -0.2176712  -0.16494502 -0.02370455] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0142, -0.0197],\n",
      "        [ 0.0091, -0.0260]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0142, 0.0091]) - torch.Size([2])\n",
      "P2 tensor([0.0141, 0.0091]) - torch.Size([2])\n",
      "P3 tensor([0.0141, 0.0091]) - torch.Size([2])\n",
      "P4 tensor([1.0141, 1.0091]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0174, -0.0251, -0.1597, -0.2621],\n",
      "        [ 0.0068,  0.3606, -0.1346, -0.7516]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0169, -0.2177, -0.1649, -0.0237],\n",
      "        [ 0.0140,  0.1676, -0.1496, -0.5041]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0141, 1.0091]) - torch.Size([2])\n",
      "Q-values tensor([[0.0645, 0.0256],\n",
      "        [0.0711, 0.0360]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0645],\n",
      "        [0.0711]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0169, -0.2177, -0.1649, -0.0237]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01250762 -0.0206152  -0.16541912 -0.36355093] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0175, -0.0266],\n",
      "        [ 0.0142, -0.0197]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0175, 0.0142]) - torch.Size([2])\n",
      "P2 tensor([0.0175, 0.0141]) - torch.Size([2])\n",
      "P3 tensor([0.0175, 0.0141]) - torch.Size([2])\n",
      "P4 tensor([1.0175, 1.0141]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0140,  0.1676, -0.1496, -0.5041],\n",
      "        [ 0.0174, -0.0251, -0.1597, -0.2621]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0174, -0.0251, -0.1597, -0.2621],\n",
      "        [ 0.0169, -0.2177, -0.1649, -0.0237]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0175, 1.0141]) - torch.Size([2])\n",
      "Q-values tensor([[0.0732, 0.0284],\n",
      "        [0.0716, 0.0256]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0732],\n",
      "        [0.0716]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0125, -0.0206, -0.1654, -0.3636]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01209531 -0.21304728 -0.17269014 -0.12725475] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0142, -0.0197],\n",
      "        [ 0.0110, -0.0259]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0142, 0.0110]) - torch.Size([2])\n",
      "P2 tensor([0.0141, 0.0110]) - torch.Size([2])\n",
      "P3 tensor([0.0141, 0.0110]) - torch.Size([2])\n",
      "P4 tensor([1.0141, 1.0110]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0174, -0.0251, -0.1597, -0.2621],\n",
      "        [ 0.0125, -0.0206, -0.1654, -0.3636]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0169, -0.2177, -0.1649, -0.0237],\n",
      "        [ 0.0121, -0.2130, -0.1727, -0.1273]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0141, 1.0110]) - torch.Size([2])\n",
      "Q-values tensor([[0.0787, 0.0256],\n",
      "        [0.0802, 0.0226]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0787],\n",
      "        [0.0802]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0121, -0.2130, -0.1727, -0.1273]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00783437 -0.01592625 -0.17523523 -0.4690573 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0110, -0.0259],\n",
      "        [ 0.0101, -0.0368]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0110, 0.0101]) - torch.Size([2])\n",
      "P2 tensor([0.0110, 0.0100]) - torch.Size([2])\n",
      "P3 tensor([0.0110, 0.0100]) - torch.Size([2])\n",
      "P4 tensor([1.0110, 1.0100]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0125, -0.0206, -0.1654, -0.3636],\n",
      "        [ 0.0121, -0.2130, -0.1727, -0.1273]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0121, -0.2130, -0.1727, -0.1273],\n",
      "        [ 0.0078, -0.0159, -0.1752, -0.4691]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0110, 1.0100]) - torch.Size([2])\n",
      "Q-values tensor([[0.0878, 0.0220],\n",
      "        [0.0730, 0.0247]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0878],\n",
      "        [0.0247]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0078, -0.0159, -0.1752, -0.4691]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00751584  0.18118192 -0.18461637 -0.81144756] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0101, -0.0368],\n",
      "        [ 0.0110, -0.0259]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0101, 0.0110]) - torch.Size([2])\n",
      "P2 tensor([0.0100, 0.0110]) - torch.Size([2])\n",
      "P3 tensor([0.0100, 0.0110]) - torch.Size([2])\n",
      "P4 tensor([1.0100, 1.0110]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0121, -0.2130, -0.1727, -0.1273],\n",
      "        [ 0.0125, -0.0206, -0.1654, -0.3636]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0078, -0.0159, -0.1752, -0.4691],\n",
      "        [ 0.0121, -0.2130, -0.1727, -0.1273]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0100, 1.0110]) - torch.Size([2])\n",
      "Q-values tensor([[0.0777, 0.0293],\n",
      "        [0.0936, 0.0258]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0293],\n",
      "        [0.0936]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<03:47,  4.40 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States tensor([[ 0.0075,  0.1812, -0.1846, -0.8114]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01113948 -0.01099619 -0.20084533 -0.5820484 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0076, -0.0441],\n",
      "        [ 0.0101, -0.0368]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0076, 0.0101]) - torch.Size([2])\n",
      "P2 tensor([0.0076, 0.0100]) - torch.Size([2])\n",
      "P3 tensor([0.0076, 0.0100]) - torch.Size([2])\n",
      "P4 tensor([1.0076, 1.0100]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0075,  0.1812, -0.1846, -0.8114],\n",
      "        [ 0.0121, -0.2130, -0.1727, -0.1273]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0111, -0.0110, -0.2008, -0.5820],\n",
      "        [ 0.0078, -0.0159, -0.1752, -0.4691]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0076, 1.0100]) - torch.Size([2])\n",
      "Q-values tensor([[0.1107, 0.0279],\n",
      "        [0.0824, 0.0338]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1107],\n",
      "        [0.0338]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0111, -0.0110, -0.2008, -0.5820]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.01091956  0.18628888 -0.2124863  -0.9306797 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0052, -0.0348],\n",
      "        [ 0.0058, -0.0389]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0052, 0.0058]) - torch.Size([2])\n",
      "P2 tensor([0.0052, 0.0058]) - torch.Size([2])\n",
      "P3 tensor([0.0052, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.0052, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0078, -0.0159, -0.1752, -0.4691],\n",
      "        [ 0.0111, -0.0110, -0.2008, -0.5820]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0075,  0.1812, -0.1846, -0.8114],\n",
      "        [ 0.0109,  0.1863, -0.2125, -0.9307]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0052, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.1069, 0.0289],\n",
      "        [0.1096, 0.0225]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0289],\n",
      "        [0.0225]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "Episode 0 - Avg. Reward over the last 100 episodes 23.0 epsilon 0.9998020217784038 total steps 23\n",
      "States tensor([[ 6.2248e-03, -8.2096e-06,  4.0203e-02, -4.3474e-02]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00622467 -0.19568291  0.039334    0.26161778] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0058, -0.0389],\n",
      "        [ 0.0142, -0.0022]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0058, 0.0142]) - torch.Size([2])\n",
      "P2 tensor([0.0058, 0.0142]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.0142]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.0142]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 1.1139e-02, -1.0996e-02, -2.0085e-01, -5.8205e-01],\n",
      "        [ 6.2248e-03, -8.2096e-06,  4.0203e-02, -4.3474e-02]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0109,  0.1863, -0.2125, -0.9307],\n",
      "        [ 0.0062, -0.1957,  0.0393,  0.2616]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.0142]) - torch.Size([2])\n",
      "Q-values tensor([[0.1089, 0.0311],\n",
      "        [0.0970, 0.0540]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0311],\n",
      "        [0.0970]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0062, -0.1957,  0.0393,  0.2616]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00231101 -0.39134362  0.04456636  0.566443  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0073,  0.0021],\n",
      "        [ 0.0058, -0.0389]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0073, 0.0058]) - torch.Size([2])\n",
      "P2 tensor([0.0073, 0.0058]) - torch.Size([2])\n",
      "P3 tensor([0.0073, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.0073, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0062, -0.1957,  0.0393,  0.2616],\n",
      "        [ 0.0111, -0.0110, -0.2008, -0.5820]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0023, -0.3913,  0.0446,  0.5664],\n",
      "        [ 0.0109,  0.1863, -0.2125, -0.9307]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0073, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.0751, 0.0587],\n",
      "        [0.1119, 0.0385]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0751],\n",
      "        [0.0385]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0023, -0.3913,  0.0446,  0.5664]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00551586 -0.5870615   0.05589521  0.8728265 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0142, -0.0022],\n",
      "        [-0.0004, -0.0002]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([ 0.0142, -0.0002]) - torch.Size([2])\n",
      "P2 tensor([ 0.0142, -0.0002]) - torch.Size([2])\n",
      "P3 tensor([ 0.0142, -0.0002]) - torch.Size([2])\n",
      "P4 tensor([1.0142, 0.9998]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 6.2248e-03, -8.2096e-06,  4.0203e-02, -4.3474e-02],\n",
      "        [ 2.3110e-03, -3.9134e-01,  4.4566e-02,  5.6644e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0062, -0.1957,  0.0393,  0.2616],\n",
      "        [-0.0055, -0.5871,  0.0559,  0.8728]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0142, 0.9998]) - torch.Size([2])\n",
      "Q-values tensor([[0.1051, 0.0618],\n",
      "        [0.0606, 0.0619]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1051],\n",
      "        [0.0606]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0055, -0.5871,  0.0559,  0.8728]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01725709 -0.39274237  0.07335175  0.5982277 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0004, -0.0002],\n",
      "        [ 0.0064,  0.0037]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([-0.0002,  0.0064]) - torch.Size([2])\n",
      "P2 tensor([-0.0002,  0.0064]) - torch.Size([2])\n",
      "P3 tensor([-0.0002,  0.0064]) - torch.Size([2])\n",
      "P4 tensor([0.9998, 1.0064]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0023, -0.3913,  0.0446,  0.5664],\n",
      "        [-0.0055, -0.5871,  0.0559,  0.8728]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0055, -0.5871,  0.0559,  0.8728],\n",
      "        [-0.0173, -0.3927,  0.0734,  0.5982]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([0.9998, 1.0064]) - torch.Size([2])\n",
      "Q-values tensor([[0.0660, 0.0619],\n",
      "        [0.0522, 0.0569]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0660],\n",
      "        [0.0569]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0173, -0.3927,  0.0734,  0.5982]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02511194 -0.19871928  0.0853163   0.32952172] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0004, -0.0002],\n",
      "        [ 0.0064,  0.0037]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([-0.0002,  0.0064]) - torch.Size([2])\n",
      "P2 tensor([-0.0002,  0.0064]) - torch.Size([2])\n",
      "P3 tensor([-0.0002,  0.0064]) - torch.Size([2])\n",
      "P4 tensor([0.9998, 1.0064]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0023, -0.3913,  0.0446,  0.5664],\n",
      "        [-0.0055, -0.5871,  0.0559,  0.8728]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0055, -0.5871,  0.0559,  0.8728],\n",
      "        [-0.0173, -0.3927,  0.0734,  0.5982]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([0.9998, 1.0064]) - torch.Size([2])\n",
      "Q-values tensor([[0.0700, 0.0650],\n",
      "        [0.0556, 0.0611]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0700],\n",
      "        [0.0611]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0251, -0.1987,  0.0853,  0.3295]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02908633 -0.39494556  0.09190673  0.6478432 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.0140, 0.0006],\n",
      "        [0.0047, 0.0055]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0140, 0.0055]) - torch.Size([2])\n",
      "P2 tensor([0.0140, 0.0055]) - torch.Size([2])\n",
      "P3 tensor([0.0140, 0.0055]) - torch.Size([2])\n",
      "P4 tensor([1.0140, 1.0055]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0173, -0.3927,  0.0734,  0.5982],\n",
      "        [-0.0251, -0.1987,  0.0853,  0.3295]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0251, -0.1987,  0.0853,  0.3295],\n",
      "        [-0.0291, -0.3949,  0.0919,  0.6478]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0140, 1.0055]) - torch.Size([2])\n",
      "Q-values tensor([[0.0717, 0.0686],\n",
      "        [0.0878, 0.0672]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0686],\n",
      "        [0.0878]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0291, -0.3949,  0.0919,  0.6478]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03698524 -0.59121966  0.1048636   0.9679943 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0046,  0.0012],\n",
      "        [ 0.0047,  0.0055]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0012, 0.0055]) - torch.Size([2])\n",
      "P2 tensor([0.0012, 0.0055]) - torch.Size([2])\n",
      "P3 tensor([0.0012, 0.0055]) - torch.Size([2])\n",
      "P4 tensor([1.0012, 1.0055]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0291, -0.3949,  0.0919,  0.6478],\n",
      "        [-0.0251, -0.1987,  0.0853,  0.3295]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0370, -0.5912,  0.1049,  0.9680],\n",
      "        [-0.0291, -0.3949,  0.0919,  0.6478]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0012, 1.0055]) - torch.Size([2])\n",
      "Q-values tensor([[0.0722, 0.0731],\n",
      "        [0.0917, 0.0704]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0722],\n",
      "        [0.0917]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0370, -0.5912,  0.1049,  0.9680]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04880963 -0.7875814   0.12422348  1.2916924 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0047,  0.0055],\n",
      "        [-0.0093, -0.0111]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([ 0.0055, -0.0093]) - torch.Size([2])\n",
      "P2 tensor([ 0.0055, -0.0093]) - torch.Size([2])\n",
      "P3 tensor([ 0.0055, -0.0093]) - torch.Size([2])\n",
      "P4 tensor([1.0055, 0.9907]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0251, -0.1987,  0.0853,  0.3295],\n",
      "        [-0.0370, -0.5912,  0.1049,  0.9680]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0291, -0.3949,  0.0919,  0.6478],\n",
      "        [-0.0488, -0.7876,  0.1242,  1.2917]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0055, 0.9907]) - torch.Size([2])\n",
      "Q-values tensor([[0.0971, 0.0704],\n",
      "        [0.0642, 0.0713]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0971],\n",
      "        [0.0642]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0488, -0.7876,  0.1242,  1.2917]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06456126 -0.5942382   0.15005733  1.0403402 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0073,  0.0005],\n",
      "        [-0.0093, -0.0111]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([ 0.0005, -0.0093]) - torch.Size([2])\n",
      "P2 tensor([ 0.0005, -0.0093]) - torch.Size([2])\n",
      "P3 tensor([ 0.0005, -0.0093]) - torch.Size([2])\n",
      "P4 tensor([1.0005, 0.9907]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0488, -0.7876,  0.1242,  1.2917],\n",
      "        [-0.0370, -0.5912,  0.1049,  0.9680]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0646, -0.5942,  0.1501,  1.0403],\n",
      "        [-0.0488, -0.7876,  0.1242,  1.2917]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0005, 0.9907]) - torch.Size([2])\n",
      "Q-values tensor([[0.0692, 0.0663],\n",
      "        [0.0703, 0.0716]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0663],\n",
      "        [0.0703]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0646, -0.5942,  0.1501,  1.0403]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.07644602 -0.40139365  0.17086414  0.7982755 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0073,  0.0005],\n",
      "        [-0.0007,  0.0089]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0005, 0.0089]) - torch.Size([2])\n",
      "P2 tensor([0.0005, 0.0089]) - torch.Size([2])\n",
      "P3 tensor([0.0005, 0.0089]) - torch.Size([2])\n",
      "P4 tensor([1.0005, 1.0089]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0488, -0.7876,  0.1242,  1.2917],\n",
      "        [-0.0646, -0.5942,  0.1501,  1.0403]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0646, -0.5942,  0.1501,  1.0403],\n",
      "        [-0.0764, -0.4014,  0.1709,  0.7983]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0005, 1.0089]) - torch.Size([2])\n",
      "Q-values tensor([[0.0732, 0.0718],\n",
      "        [0.0729, 0.0762]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0718],\n",
      "        [0.0762]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0764, -0.4014,  0.1709,  0.7983]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.08447389 -0.20897606  0.18682964  0.563842  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0073,  0.0005],\n",
      "        [-0.0007,  0.0089]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0005, 0.0089]) - torch.Size([2])\n",
      "P2 tensor([0.0005, 0.0089]) - torch.Size([2])\n",
      "P3 tensor([0.0005, 0.0089]) - torch.Size([2])\n",
      "P4 tensor([1.0005, 1.0089]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0488, -0.7876,  0.1242,  1.2917],\n",
      "        [-0.0646, -0.5942,  0.1501,  1.0403]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0646, -0.5942,  0.1501,  1.0403],\n",
      "        [-0.0764, -0.4014,  0.1709,  0.7983]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0005, 1.0089]) - torch.Size([2])\n",
      "Q-values tensor([[0.0727, 0.0785],\n",
      "        [0.0725, 0.0822]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0785],\n",
      "        [0.0822]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0845, -0.2090,  0.1868,  0.5638]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.08865342 -0.40616012  0.19810648  0.9090799 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0007,  0.0089],\n",
      "        [-0.0041,  0.0098]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0089, 0.0098]) - torch.Size([2])\n",
      "P2 tensor([0.0089, 0.0098]) - torch.Size([2])\n",
      "P3 tensor([0.0089, 0.0098]) - torch.Size([2])\n",
      "P4 tensor([1.0089, 1.0098]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0646, -0.5942,  0.1501,  1.0403],\n",
      "        [-0.0845, -0.2090,  0.1868,  0.5638]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0764, -0.4014,  0.1709,  0.7983],\n",
      "        [-0.0887, -0.4062,  0.1981,  0.9091]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0089, 1.0098]) - torch.Size([2])\n",
      "Q-values tensor([[0.0722, 0.0882],\n",
      "        [0.0912, 0.0894]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0882],\n",
      "        [0.0912]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0887, -0.4062,  0.1981,  0.9091]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [-0.09677662 -0.21419096  0.21628807  0.6846214 ] -\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<02:39,  6.24 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P0 tensor([[ 0.0093,  0.0105],\n",
      "        [-0.0041,  0.0098]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0105, 0.0098]) - torch.Size([2])\n",
      "P2 tensor([0.0104, 0.0098]) - torch.Size([2])\n",
      "P3 tensor([0.0104, 0.0098]) - torch.Size([2])\n",
      "P4 tensor([1.0104, 1.0098]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0764, -0.4014,  0.1709,  0.7983],\n",
      "        [-0.0845, -0.2090,  0.1868,  0.5638]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0845, -0.2090,  0.1868,  0.5638],\n",
      "        [-0.0887, -0.4062,  0.1981,  0.9091]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0104, 1.0098]) - torch.Size([2])\n",
      "Q-values tensor([[0.0817, 0.0941],\n",
      "        [0.0948, 0.0925]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0941],\n",
      "        [0.0948]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0106,  0.0250,  0.0344, -0.0210]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01010207 -0.17061456  0.033988    0.28231433] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0042,  0.0143],\n",
      "        [ 0.0174, -0.0007]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0143, 0.0174]) - torch.Size([2])\n",
      "P2 tensor([0.0143, 0.0173]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.0173]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.0173]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0887, -0.4062,  0.1981,  0.9091],\n",
      "        [-0.0106,  0.0250,  0.0344, -0.0210]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0968, -0.2142,  0.2163,  0.6846],\n",
      "        [-0.0101, -0.1706,  0.0340,  0.2823]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.0173]) - torch.Size([2])\n",
      "Q-values tensor([[0.0814, 0.1005],\n",
      "        [0.1416, 0.0806]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1005],\n",
      "        [0.1416]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0101, -0.1706,  0.0340,  0.2823]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01351436 -0.36620438  0.03963428  0.5855202 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0174, -0.0007],\n",
      "        [ 0.0086,  0.0034]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0174, 0.0086]) - torch.Size([2])\n",
      "P2 tensor([0.0173, 0.0086]) - torch.Size([2])\n",
      "P3 tensor([0.0173, 0.0086]) - torch.Size([2])\n",
      "P4 tensor([1.0173, 1.0086]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0106,  0.0250,  0.0344, -0.0210],\n",
      "        [-0.0101, -0.1706,  0.0340,  0.2823]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0101, -0.1706,  0.0340,  0.2823],\n",
      "        [-0.0135, -0.3662,  0.0396,  0.5855]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0173, 1.0086]) - torch.Size([2])\n",
      "Q-values tensor([[0.1463, 0.0826],\n",
      "        [0.1205, 0.0877]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1463],\n",
      "        [0.1205]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0135, -0.3662,  0.0396,  0.5855]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02083845 -0.17165937  0.05134469  0.30558124] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0174, -0.0007],\n",
      "        [ 0.0170,  0.0003]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0174, 0.0170]) - torch.Size([2])\n",
      "P2 tensor([0.0173, 0.0170]) - torch.Size([2])\n",
      "P3 tensor([0.0173, 0.0170]) - torch.Size([2])\n",
      "P4 tensor([1.0173, 1.0170]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0106,  0.0250,  0.0344, -0.0210],\n",
      "        [-0.0135, -0.3662,  0.0396,  0.5855]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0101, -0.1706,  0.0340,  0.2823],\n",
      "        [-0.0208, -0.1717,  0.0513,  0.3056]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0173, 1.0170]) - torch.Size([2])\n",
      "Q-values tensor([[0.1518, 0.0825],\n",
      "        [0.1069, 0.0943]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1518],\n",
      "        [0.0943]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0208, -0.1717,  0.0513,  0.3056]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02427164  0.02269475  0.05745631  0.02952299] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0276, -0.0098],\n",
      "        [ 0.0170,  0.0003]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0276, 0.0170]) - torch.Size([2])\n",
      "P2 tensor([0.0276, 0.0170]) - torch.Size([2])\n",
      "P3 tensor([0.0276, 0.0170]) - torch.Size([2])\n",
      "P4 tensor([1.0276, 1.0170]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0208, -0.1717,  0.0513,  0.3056],\n",
      "        [-0.0135, -0.3662,  0.0396,  0.5855]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0243,  0.0227,  0.0575,  0.0295],\n",
      "        [-0.0208, -0.1717,  0.0513,  0.3056]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0276, 1.0170]) - torch.Size([2])\n",
      "Q-values tensor([[0.1271, 0.0910],\n",
      "        [0.1095, 0.0986]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0910],\n",
      "        [0.0986]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0243,  0.0227,  0.0575,  0.0295]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02381774 -0.17320208  0.05804677  0.3397662 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0276, -0.0098],\n",
      "        [ 0.0170,  0.0003]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0276, 0.0170]) - torch.Size([2])\n",
      "P2 tensor([0.0276, 0.0170]) - torch.Size([2])\n",
      "P3 tensor([0.0276, 0.0170]) - torch.Size([2])\n",
      "P4 tensor([1.0276, 1.0170]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0208, -0.1717,  0.0513,  0.3056],\n",
      "        [-0.0135, -0.3662,  0.0396,  0.5855]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0243,  0.0227,  0.0575,  0.0295],\n",
      "        [-0.0208, -0.1717,  0.0513,  0.3056]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0276, 1.0170]) - torch.Size([2])\n",
      "Q-values tensor([[0.1267, 0.0962],\n",
      "        [0.1090, 0.1040]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.0962],\n",
      "        [0.1040]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0238, -0.1732,  0.0580,  0.3398]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02728178  0.02104796  0.0648421   0.06593806] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0276, -0.0098],\n",
      "        [ 0.0162,  0.0009]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0276, 0.0162]) - torch.Size([2])\n",
      "P2 tensor([0.0276, 0.0162]) - torch.Size([2])\n",
      "P3 tensor([0.0276, 0.0162]) - torch.Size([2])\n",
      "P4 tensor([1.0276, 1.0162]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0208, -0.1717,  0.0513,  0.3056],\n",
      "        [-0.0243,  0.0227,  0.0575,  0.0295]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0243,  0.0227,  0.0575,  0.0295],\n",
      "        [-0.0238, -0.1732,  0.0580,  0.3398]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0276, 1.0162]) - torch.Size([2])\n",
      "Q-values tensor([[0.1262, 0.1013],\n",
      "        [0.1532, 0.0954]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1013],\n",
      "        [0.1532]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0273,  0.0210,  0.0648,  0.0659]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02686083 -0.17494084  0.06616086  0.37835366] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0156,  0.0014],\n",
      "        [ 0.0276, -0.0080]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0156, 0.0276]) - torch.Size([2])\n",
      "P2 tensor([0.0156, 0.0276]) - torch.Size([2])\n",
      "P3 tensor([0.0156, 0.0276]) - torch.Size([2])\n",
      "P4 tensor([1.0156, 1.0276]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0273,  0.0210,  0.0648,  0.0659],\n",
      "        [-0.0238, -0.1732,  0.0580,  0.3398]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0269, -0.1749,  0.0662,  0.3784],\n",
      "        [-0.0273,  0.0210,  0.0648,  0.0659]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0156, 1.0276]) - torch.Size([2])\n",
      "Q-values tensor([[0.1551, 0.0987],\n",
      "        [0.1267, 0.1049]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1551],\n",
      "        [0.1049]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0269, -0.1749,  0.0662,  0.3784]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03035964 -0.37093696  0.07372794  0.6911417 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0276, -0.0080],\n",
      "        [ 0.0051,  0.0061]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0276, 0.0061]) - torch.Size([2])\n",
      "P2 tensor([0.0276, 0.0061]) - torch.Size([2])\n",
      "P3 tensor([0.0276, 0.0061]) - torch.Size([2])\n",
      "P4 tensor([1.0276, 1.0061]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0238, -0.1732,  0.0580,  0.3398],\n",
      "        [-0.0269, -0.1749,  0.0662,  0.3784]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0273,  0.0210,  0.0648,  0.0659],\n",
      "        [-0.0304, -0.3709,  0.0737,  0.6911]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0276, 1.0061]) - torch.Size([2])\n",
      "Q-values tensor([[0.1299, 0.1087],\n",
      "        [0.1273, 0.1087]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1087],\n",
      "        [0.1273]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0304, -0.3709,  0.0737,  0.6911]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03777838 -0.56700027  0.08755077  1.0060941 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.0051, 0.0061],\n",
      "        [0.0156, 0.0014]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.0061, 0.0156]) - torch.Size([2])\n",
      "P2 tensor([0.0061, 0.0156]) - torch.Size([2])\n",
      "P3 tensor([0.0061, 0.0156]) - torch.Size([2])\n",
      "P4 tensor([1.0061, 1.0156]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0269, -0.1749,  0.0662,  0.3784],\n",
      "        [-0.0273,  0.0210,  0.0648,  0.0659]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0304, -0.3709,  0.0737,  0.6911],\n",
      "        [-0.0269, -0.1749,  0.0662,  0.3784]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0061, 1.0156]) - torch.Size([2])\n",
      "Q-values tensor([[0.1310, 0.1122],\n",
      "        [0.1628, 0.1053]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1310],\n",
      "        [0.1628]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0378, -0.5670,  0.0876,  1.0061]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04911838 -0.7631753   0.10767265  1.3249367 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[ 0.0051,  0.0061],\n",
      "        [-0.0093, -0.0110]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([ 0.0061, -0.0093]) - torch.Size([2])\n",
      "P2 tensor([ 0.0061, -0.0093]) - torch.Size([2])\n",
      "P3 tensor([ 0.0061, -0.0093]) - torch.Size([2])\n",
      "P4 tensor([1.0061, 0.9907]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0269, -0.1749,  0.0662,  0.3784],\n",
      "        [-0.0378, -0.5670,  0.0876,  1.0061]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0304, -0.3709,  0.0737,  0.6911],\n",
      "        [-0.0491, -0.7632,  0.1077,  1.3249]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0061, 0.9907]) - torch.Size([2])\n",
      "Q-values tensor([[0.1363, 0.1120],\n",
      "        [0.1049, 0.1286]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1363],\n",
      "        [0.1049]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0491, -0.7632,  0.1077,  1.3249]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06438189 -0.95947963  0.13417138  1.6492826 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0093, -0.0110],\n",
      "        [-0.0046,  0.0008]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([-0.0093,  0.0008]) - torch.Size([2])\n",
      "P2 tensor([-0.0093,  0.0008]) - torch.Size([2])\n",
      "P3 tensor([-0.0093,  0.0008]) - torch.Size([2])\n",
      "P4 tensor([0.9907, 1.0008]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0378, -0.5670,  0.0876,  1.0061],\n",
      "        [-0.0304, -0.3709,  0.0737,  0.6911]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0491, -0.7632,  0.1077,  1.3249],\n",
      "        [-0.0378, -0.5670,  0.0876,  1.0061]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([0.9907, 1.0008]) - torch.Size([2])\n",
      "Q-values tensor([[0.1111, 0.1290],\n",
      "        [0.1216, 0.1218]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1111],\n",
      "        [0.1216]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0644, -0.9595,  0.1342,  1.6493]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.08357149 -0.766157    0.16715704  1.401233  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0169, -0.0207],\n",
      "        [-0.0133, -0.0108]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([-0.0169, -0.0108]) - torch.Size([2])\n",
      "P2 tensor([-0.0169, -0.0108]) - torch.Size([2])\n",
      "P3 tensor([-0.0169, -0.0108]) - torch.Size([2])\n",
      "P4 tensor([0.9831, 0.9892]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0491, -0.7632,  0.1077,  1.3249],\n",
      "        [-0.0644, -0.9595,  0.1342,  1.6493]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0644, -0.9595,  0.1342,  1.6493],\n",
      "        [-0.0836, -0.7662,  0.1672,  1.4012]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([0.9831, 0.9892]) - torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:00<02:35,  6.43 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values tensor([[0.1187, 0.1322],\n",
      "        [0.1193, 0.1392]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1187],\n",
      "        [0.1392]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0836, -0.7662,  0.1672,  1.4012]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.09889463 -0.5734595   0.1951817   1.1651312 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0169, -0.0207],\n",
      "        [-0.0105, -0.0013]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([-0.0169, -0.0013]) - torch.Size([2])\n",
      "P2 tensor([-0.0169, -0.0013]) - torch.Size([2])\n",
      "P3 tensor([-0.0169, -0.0013]) - torch.Size([2])\n",
      "P4 tensor([0.9831, 0.9987]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0491, -0.7632,  0.1077,  1.3249],\n",
      "        [-0.0836, -0.7662,  0.1672,  1.4012]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0644, -0.9595,  0.1342,  1.6493],\n",
      "        [-0.0989, -0.5735,  0.1952,  1.1651]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([0.9831, 0.9987]) - torch.Size([2])\n",
      "Q-values tensor([[0.1238, 0.1374],\n",
      "        [0.1216, 0.1416]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1238],\n",
      "        [0.1416]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0989, -0.5735,  0.1952,  1.1651]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [-0.11036382 -0.7705109   0.21848431  1.5121157 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[-0.0105, -0.0013],\n",
      "        [-0.0133, -0.0108]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([-0.0013, -0.0108]) - torch.Size([2])\n",
      "P2 tensor([-0.0013, -0.0108]) - torch.Size([2])\n",
      "P3 tensor([-0.0013, -0.0108]) - torch.Size([2])\n",
      "P4 tensor([0.9987, 0.9892]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0836, -0.7662,  0.1672,  1.4012],\n",
      "        [-0.0644, -0.9595,  0.1342,  1.6493]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0989, -0.5735,  0.1952,  1.1651],\n",
      "        [-0.0836, -0.7662,  0.1672,  1.4012]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([0.9987, 0.9892]) - torch.Size([2])\n",
      "Q-values tensor([[0.1274, 0.1467],\n",
      "        [0.1312, 0.1508]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1467],\n",
      "        [0.1508]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0406, -0.0129,  0.0232, -0.0032]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04038882  0.18190186  0.02317881 -0.28850156] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1938, 0.1061],\n",
      "        [0.1254, 0.1485]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1938, 0.1485]) - torch.Size([2])\n",
      "P2 tensor([0.1936, 0.1483]) - torch.Size([2])\n",
      "P3 tensor([0.1936, 0.1483]) - torch.Size([2])\n",
      "P4 tensor([1.1936, 1.1483]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0406, -0.0129,  0.0232, -0.0032],\n",
      "        [-0.0836, -0.7662,  0.1672,  1.4012]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0404,  0.1819,  0.0232, -0.2885],\n",
      "        [-0.0989, -0.5735,  0.1952,  1.1651]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1936, 1.1483]) - torch.Size([2])\n",
      "Q-values tensor([[0.1805, 0.1117],\n",
      "        [0.1272, 0.1538]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1117],\n",
      "        [0.1538]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0404,  0.1819,  0.0232, -0.2885]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04402686 -0.01354283  0.01740877  0.01140067] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1938, 0.1061],\n",
      "        [0.1262, 0.1575]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1938, 0.1575]) - torch.Size([2])\n",
      "P2 tensor([0.1936, 0.1574]) - torch.Size([2])\n",
      "P3 tensor([0.1936, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.1936, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0406, -0.0129,  0.0232, -0.0032],\n",
      "        [-0.0989, -0.5735,  0.1952,  1.1651]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0404,  0.1819,  0.0232, -0.2885],\n",
      "        [-0.1104, -0.7705,  0.2185,  1.5121]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1936, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.1806, 0.1169],\n",
      "        [0.1255, 0.1545]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1169],\n",
      "        [0.1255]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0440, -0.0135,  0.0174,  0.0114]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.043756    0.18132518  0.01763679 -0.27573913] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1929, 0.1062],\n",
      "        [0.1788, 0.1115]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1929, 0.1788]) - torch.Size([2])\n",
      "P2 tensor([0.1927, 0.1787]) - torch.Size([2])\n",
      "P3 tensor([0.1927, 0.1787]) - torch.Size([2])\n",
      "P4 tensor([1.1927, 1.1787]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0440, -0.0135,  0.0174,  0.0114],\n",
      "        [ 0.0404,  0.1819,  0.0232, -0.2885]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0438,  0.1813,  0.0176, -0.2757],\n",
      "        [ 0.0440, -0.0135,  0.0174,  0.0114]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1927, 1.1787]) - torch.Size([2])\n",
      "Q-values tensor([[0.1815, 0.1212],\n",
      "        [0.1958, 0.1161]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1212],\n",
      "        [0.1958]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0438,  0.1813,  0.0176, -0.2757]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.0473825   0.37619114  0.012122   -0.5628076 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2001, 0.1067],\n",
      "        [0.1788, 0.1115]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2001, 0.1788]) - torch.Size([2])\n",
      "P2 tensor([0.1999, 0.1787]) - torch.Size([2])\n",
      "P3 tensor([0.1999, 0.1787]) - torch.Size([2])\n",
      "P4 tensor([1.1999, 1.1787]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0438,  0.1813,  0.0176, -0.2757],\n",
      "        [ 0.0404,  0.1819,  0.0232, -0.2885]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0474,  0.3762,  0.0121, -0.5628],\n",
      "        [ 0.0440, -0.0135,  0.0174,  0.0114]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1999, 1.1787]) - torch.Size([2])\n",
      "Q-values tensor([[0.2002, 0.1194],\n",
      "        [0.2012, 0.1193]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1194],\n",
      "        [0.2012]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0474,  0.3762,  0.0121, -0.5628]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05490632  0.5711409   0.00086585 -0.851647  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2001, 0.1067],\n",
      "        [0.2158, 0.1148]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2001, 0.2158]) - torch.Size([2])\n",
      "P2 tensor([0.1999, 0.2156]) - torch.Size([2])\n",
      "P3 tensor([0.1999, 0.2156]) - torch.Size([2])\n",
      "P4 tensor([1.1999, 1.2156]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0438,  0.1813,  0.0176, -0.2757],\n",
      "        [ 0.0474,  0.3762,  0.0121, -0.5628]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0474,  0.3762,  0.0121, -0.5628],\n",
      "        [ 0.0549,  0.5711,  0.0009, -0.8516]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1999, 1.2156]) - torch.Size([2])\n",
      "Q-values tensor([[0.2048, 0.1237],\n",
      "        [0.2136, 0.1259]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1237],\n",
      "        [0.1259]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0549,  0.5711,  0.0009, -0.8516]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06632914  0.37600714 -0.01616709 -0.558692  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1975, 0.1064],\n",
      "        [0.2158, 0.1148]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1975, 0.2158]) - torch.Size([2])\n",
      "P2 tensor([0.1973, 0.2156]) - torch.Size([2])\n",
      "P3 tensor([0.1973, 0.2156]) - torch.Size([2])\n",
      "P4 tensor([1.1973, 1.2156]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0549,  0.5711,  0.0009, -0.8516],\n",
      "        [ 0.0474,  0.3762,  0.0121, -0.5628]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0663,  0.3760, -0.0162, -0.5587],\n",
      "        [ 0.0549,  0.5711,  0.0009, -0.8516]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1973, 1.2156]) - torch.Size([2])\n",
      "Q-values tensor([[0.2318, 0.1427],\n",
      "        [0.2131, 0.1334]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2318],\n",
      "        [0.1334]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0663,  0.3760, -0.0162, -0.5587]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.07384928  0.57135224 -0.02734093 -0.8564243 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2158, 0.1148],\n",
      "        [0.2139, 0.1154]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2158, 0.2139]) - torch.Size([2])\n",
      "P2 tensor([0.2156, 0.2136]) - torch.Size([2])\n",
      "P3 tensor([0.2156, 0.2136]) - torch.Size([2])\n",
      "P4 tensor([1.2156, 1.2136]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0474,  0.3762,  0.0121, -0.5628],\n",
      "        [ 0.0663,  0.3760, -0.0162, -0.5587]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0549,  0.5711,  0.0009, -0.8516],\n",
      "        [ 0.0738,  0.5714, -0.0273, -0.8564]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2156, 1.2136]) - torch.Size([2])\n",
      "Q-values tensor([[0.2183, 0.1385],\n",
      "        [0.2158, 0.1381]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1385],\n",
      "        [0.1381]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0738,  0.5714, -0.0273, -0.8564]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.08527633  0.76683587 -0.04446941 -1.1575773 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2274, 0.1393],\n",
      "        [0.1975, 0.1064]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2274, 0.1975]) - torch.Size([2])\n",
      "P2 tensor([0.2272, 0.1973]) - torch.Size([2])\n",
      "P3 tensor([0.2272, 0.1973]) - torch.Size([2])\n",
      "P4 tensor([1.2272, 1.1973]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0738,  0.5714, -0.0273, -0.8564],\n",
      "        [ 0.0549,  0.5711,  0.0009, -0.8516]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0853,  0.7668, -0.0445, -1.1576],\n",
      "        [ 0.0663,  0.3760, -0.0162, -0.5587]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2272, 1.1973]) - torch.Size([2])\n",
      "Q-values tensor([[0.2365, 0.1555],\n",
      "        [0.2383, 0.1556]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1555],\n",
      "        [0.2383]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0853,  0.7668, -0.0445, -1.1576]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.10061305  0.5723209  -0.06762096 -0.8791629 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2139, 0.1154],\n",
      "        [0.2114, 0.1176]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2139, 0.2114]) - torch.Size([2])\n",
      "P2 tensor([0.2136, 0.2112]) - torch.Size([2])\n",
      "P3 tensor([0.2136, 0.2112]) - torch.Size([2])\n",
      "P4 tensor([1.2136, 1.2112]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0663,  0.3760, -0.0162, -0.5587],\n",
      "        [ 0.0853,  0.7668, -0.0445, -1.1576]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0738,  0.5714, -0.0273, -0.8564],\n",
      "        [ 0.1006,  0.5723, -0.0676, -0.8792]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2136, 1.2112]) - torch.Size([2])\n",
      "Q-values tensor([[0.2205, 0.1505],\n",
      "        [0.2581, 0.1875]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1505],\n",
      "        [0.2581]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1006,  0.5723, -0.0676, -0.8792]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.11205947  0.7682932  -0.08520422 -1.1923151 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2274, 0.1393],\n",
      "        [0.2285, 0.1425]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2274, 0.2285]) - torch.Size([2])\n",
      "P2 tensor([0.2272, 0.2283]) - torch.Size([2])\n",
      "P3 tensor([0.2272, 0.2283]) - torch.Size([2])\n",
      "P4 tensor([1.2272, 1.2283]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0738,  0.5714, -0.0273, -0.8564],\n",
      "        [ 0.1006,  0.5723, -0.0676, -0.8792]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0853,  0.7668, -0.0445, -1.1576],\n",
      "        [ 0.1121,  0.7683, -0.0852, -1.1923]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2272, 1.2283]) - torch.Size([2])\n",
      "Q-values tensor([[0.2492, 0.1668],\n",
      "        [0.2473, 0.1680]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1668],\n",
      "        [0.1680]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1121,  0.7683, -0.0852, -1.1923]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.12742533  0.96440923 -0.10905052 -1.5104417 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2114, 0.1176],\n",
      "        [0.2285, 0.1425]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2114, 0.2285]) - torch.Size([2])\n",
      "P2 tensor([0.2112, 0.2283]) - torch.Size([2])\n",
      "P3 tensor([0.2112, 0.2283]) - torch.Size([2])\n",
      "P4 tensor([1.2112, 1.2283]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0853,  0.7668, -0.0445, -1.1576],\n",
      "        [ 0.1006,  0.5723, -0.0676, -0.8792]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1006,  0.5723, -0.0676, -0.8792],\n",
      "        [ 0.1121,  0.7683, -0.0852, -1.1923]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2112, 1.2283]) - torch.Size([2])\n",
      "Q-values tensor([[0.2675, 0.2010],\n",
      "        [0.2472, 0.1771]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2675],\n",
      "        [0.1771]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1274,  0.9644, -0.1091, -1.5104]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.14671351  0.77076447 -0.13925935 -1.2536973 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2285, 0.1425],\n",
      "        [0.2604, 0.1677]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2285, 0.2604]) - torch.Size([2])\n",
      "P2 tensor([0.2283, 0.2602]) - torch.Size([2])\n",
      "P3 tensor([0.2283, 0.2602]) - torch.Size([2])\n",
      "P4 tensor([1.2283, 1.2602]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1006,  0.5723, -0.0676, -0.8792],\n",
      "        [ 0.1121,  0.7683, -0.0852, -1.1923]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1121,  0.7683, -0.0852, -1.1923],\n",
      "        [ 0.1274,  0.9644, -0.1091, -1.5104]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2283, 1.2602]) - torch.Size([2])\n",
      "Q-values tensor([[0.2532, 0.1834],\n",
      "        [0.2773, 0.2109]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1834],\n",
      "        [0.2109]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1467,  0.7708, -0.1393, -1.2537]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.1621288  0.5776734 -0.1643333 -1.0076746] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2604, 0.1677],\n",
      "        [0.2095, 0.1268]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2604, 0.2095]) - torch.Size([2])\n",
      "P2 tensor([0.2602, 0.2092]) - torch.Size([2])\n",
      "P3 tensor([0.2602, 0.2092]) - torch.Size([2])\n",
      "P4 tensor([1.2602, 1.2092]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1121,  0.7683, -0.0852, -1.1923],\n",
      "        [ 0.1467,  0.7708, -0.1393, -1.2537]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1274,  0.9644, -0.1091, -1.5104],\n",
      "        [ 0.1621,  0.5777, -0.1643, -1.0077]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2602, 1.2092]) - torch.Size([2])\n",
      "Q-values tensor([[0.2774, 0.2213],\n",
      "        [0.2810, 0.2273]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2213],\n",
      "        [0.2810]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1621,  0.5777, -0.1643, -1.0077]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.17368227  0.77456224 -0.18448679 -1.347126  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2378, 0.1530],\n",
      "        [0.2095, 0.1268]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2378, 0.2095]) - torch.Size([2])\n",
      "P2 tensor([0.2376, 0.2092]) - torch.Size([2])\n",
      "P3 tensor([0.2376, 0.2092]) - torch.Size([2])\n",
      "P4 tensor([1.2376, 1.2092]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1621,  0.5777, -0.1643, -1.0077],\n",
      "        [ 0.1467,  0.7708, -0.1393, -1.2537]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1737,  0.7746, -0.1845, -1.3471],\n",
      "        [ 0.1621,  0.5777, -0.1643, -1.0077]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2376, 1.2092]) - torch.Size([2])\n",
      "Q-values tensor([[0.2613, 0.2077],\n",
      "        [0.2888, 0.2349]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2077],\n",
      "        [0.2888]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1737,  0.7746, -0.1845, -1.3471]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.18917352  0.58217543 -0.21142931 -1.1173747 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2378, 0.1530],\n",
      "        [0.2188, 0.1336]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2378, 0.2188]) - torch.Size([2])\n",
      "P2 tensor([0.2376, 0.2185]) - torch.Size([2])\n",
      "P3 tensor([0.2376, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.2376, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1621,  0.5777, -0.1643, -1.0077],\n",
      "        [ 0.1737,  0.7746, -0.1845, -1.3471]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:00<02:31,  6.56 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1737,  0.7746, -0.1845, -1.3471],\n",
      "        [ 0.1892,  0.5822, -0.2114, -1.1174]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2376, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.2685, 0.2138],\n",
      "        [0.3063, 0.2496]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2138],\n",
      "        [0.3063]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0420, 0.0180, 0.0442, 0.0412]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04240169  0.21246704  0.04499513 -0.23720357] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2378, 0.1530],\n",
      "        [0.2188, 0.1336]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2378, 0.2188]) - torch.Size([2])\n",
      "P2 tensor([0.2376, 0.2185]) - torch.Size([2])\n",
      "P3 tensor([0.2376, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.2376, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1621,  0.5777, -0.1643, -1.0077],\n",
      "        [ 0.1737,  0.7746, -0.1845, -1.3471]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1737,  0.7746, -0.1845, -1.3471],\n",
      "        [ 0.1892,  0.5822, -0.2114, -1.1174]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2376, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.2742, 0.2213],\n",
      "        [0.3138, 0.2587]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2213],\n",
      "        [0.3138]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0424,  0.2125,  0.0450, -0.2372]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04665104  0.40691826  0.04025106 -0.5153611 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1917, 0.1058],\n",
      "        [0.1974, 0.1081]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1917, 0.1974]) - torch.Size([2])\n",
      "P2 tensor([0.1915, 0.1972]) - torch.Size([2])\n",
      "P3 tensor([0.1915, 0.1972]) - torch.Size([2])\n",
      "P4 tensor([1.1915, 1.1972]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0420,  0.0180,  0.0442,  0.0412],\n",
      "        [ 0.0424,  0.2125,  0.0450, -0.2372]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0424,  0.2125,  0.0450, -0.2372],\n",
      "        [ 0.0467,  0.4069,  0.0403, -0.5154]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1915, 1.1972]) - torch.Size([2])\n",
      "Q-values tensor([[0.2125, 0.1702],\n",
      "        [0.2328, 0.1772]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1702],\n",
      "        [0.1772]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0467,  0.4069,  0.0403, -0.5154]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.0547894   0.21125329  0.02994384 -0.2102707 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1917, 0.1058],\n",
      "        [0.1896, 0.1063]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1917, 0.1896]) - torch.Size([2])\n",
      "P2 tensor([0.1915, 0.1894]) - torch.Size([2])\n",
      "P3 tensor([0.1915, 0.1894]) - torch.Size([2])\n",
      "P4 tensor([1.1915, 1.1894]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0420,  0.0180,  0.0442,  0.0412],\n",
      "        [ 0.0467,  0.4069,  0.0403, -0.5154]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0424,  0.2125,  0.0450, -0.2372],\n",
      "        [ 0.0548,  0.2113,  0.0299, -0.2103]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1915, 1.1894]) - torch.Size([2])\n",
      "Q-values tensor([[0.2128, 0.1753],\n",
      "        [0.2468, 0.1987]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1753],\n",
      "        [0.2468]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0548,  0.2113,  0.0299, -0.2103]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.05901447 0.01571626 0.02573842 0.09170551] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1723, 0.1110],\n",
      "        [0.1896, 0.1063]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1723, 0.1896]) - torch.Size([2])\n",
      "P2 tensor([0.1722, 0.1894]) - torch.Size([2])\n",
      "P3 tensor([0.1722, 0.1894]) - torch.Size([2])\n",
      "P4 tensor([1.1722, 1.1894]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0548,  0.2113,  0.0299, -0.2103],\n",
      "        [ 0.0467,  0.4069,  0.0403, -0.5154]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0590,  0.0157,  0.0257,  0.0917],\n",
      "        [ 0.0548,  0.2113,  0.0299, -0.2103]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1722, 1.1894]) - torch.Size([2])\n",
      "Q-values tensor([[0.2352, 0.1855],\n",
      "        [0.2536, 0.2014]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2352],\n",
      "        [0.2536]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0590, 0.0157, 0.0257, 0.0917]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05932879 -0.17976497  0.02757253  0.39239648] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1896, 0.1063],\n",
      "        [0.1499, 0.1181]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1896, 0.1499]) - torch.Size([2])\n",
      "P2 tensor([0.1894, 0.1497]) - torch.Size([2])\n",
      "P3 tensor([0.1894, 0.1497]) - torch.Size([2])\n",
      "P4 tensor([1.1894, 1.1497]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0467,  0.4069,  0.0403, -0.5154],\n",
      "        [ 0.0590,  0.0157,  0.0257,  0.0917]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0548,  0.2113,  0.0299, -0.2103],\n",
      "        [ 0.0593, -0.1798,  0.0276,  0.3924]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1894, 1.1497]) - torch.Size([2])\n",
      "Q-values tensor([[0.2615, 0.2014],\n",
      "        [0.2145, 0.1778]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2615],\n",
      "        [0.2145]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0593, -0.1798,  0.0276,  0.3924]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05573349 -0.37526715  0.03542046  0.69364357] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1367, 0.1280],\n",
      "        [0.1499, 0.1181]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1367, 0.1499]) - torch.Size([2])\n",
      "P2 tensor([0.1366, 0.1497]) - torch.Size([2])\n",
      "P3 tensor([0.1366, 0.1497]) - torch.Size([2])\n",
      "P4 tensor([1.1366, 1.1497]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0593, -0.1798,  0.0276,  0.3924],\n",
      "        [ 0.0590,  0.0157,  0.0257,  0.0917]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0557, -0.3753,  0.0354,  0.6936],\n",
      "        [ 0.0593, -0.1798,  0.0276,  0.3924]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1366, 1.1497]) - torch.Size([2])\n",
      "Q-values tensor([[0.1880, 0.1790],\n",
      "        [0.2196, 0.1782]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1880],\n",
      "        [0.2196]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0557, -0.3753,  0.0354,  0.6936]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04822815 -0.57086205  0.04929333  0.9972634 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1499, 0.1181],\n",
      "        [0.1367, 0.1280]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1499, 0.1367]) - torch.Size([2])\n",
      "P2 tensor([0.1497, 0.1366]) - torch.Size([2])\n",
      "P3 tensor([0.1497, 0.1366]) - torch.Size([2])\n",
      "P4 tensor([1.1497, 1.1366]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0590,  0.0157,  0.0257,  0.0917],\n",
      "        [ 0.0593, -0.1798,  0.0276,  0.3924]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0593, -0.1798,  0.0276,  0.3924],\n",
      "        [ 0.0557, -0.3753,  0.0354,  0.6936]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1497, 1.1366]) - torch.Size([2])\n",
      "Q-values tensor([[0.2247, 0.1787],\n",
      "        [0.1935, 0.1794]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2247],\n",
      "        [0.1935]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0482, -0.5709,  0.0493,  0.9973]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.0368109 -0.7666072  0.0692386  1.3050109] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1367, 0.1280],\n",
      "        [0.1307, 0.1357]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1367, 0.1357]) - torch.Size([2])\n",
      "P2 tensor([0.1366, 0.1356]) - torch.Size([2])\n",
      "P3 tensor([0.1366, 0.1356]) - torch.Size([2])\n",
      "P4 tensor([1.1366, 1.1356]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0593, -0.1798,  0.0276,  0.3924],\n",
      "        [ 0.0557, -0.3753,  0.0354,  0.6936]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0557, -0.3753,  0.0354,  0.6936],\n",
      "        [ 0.0482, -0.5709,  0.0493,  0.9973]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1366, 1.1356]) - torch.Size([2])\n",
      "Q-values tensor([[0.1989, 0.1799],\n",
      "        [0.1806, 0.1812]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1989],\n",
      "        [0.1806]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0368, -0.7666,  0.0692,  1.3050]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.02147876 -0.96253544  0.09533882  1.6185389 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1359, 0.1523],\n",
      "        [0.1307, 0.1357]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1523, 0.1357]) - torch.Size([2])\n",
      "P2 tensor([0.1522, 0.1356]) - torch.Size([2])\n",
      "P3 tensor([0.1522, 0.1356]) - torch.Size([2])\n",
      "P4 tensor([1.1522, 1.1356]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0368, -0.7666,  0.0692,  1.3050],\n",
      "        [ 0.0557, -0.3753,  0.0354,  0.6936]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0215, -0.9625,  0.0953,  1.6185],\n",
      "        [ 0.0482, -0.5709,  0.0493,  0.9973]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1522, 1.1356]) - torch.Size([2])\n",
      "Q-values tensor([[0.1880, 0.1977],\n",
      "        [0.1864, 0.1822]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1880],\n",
      "        [0.1864]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0215, -0.9625,  0.0953,  1.6185]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00222805 -0.76865774  0.1277096   1.3570303 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1308, 0.1446],\n",
      "        [0.1333, 0.1402]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1446, 0.1402]) - torch.Size([2])\n",
      "P2 tensor([0.1445, 0.1400]) - torch.Size([2])\n",
      "P3 tensor([0.1445, 0.1400]) - torch.Size([2])\n",
      "P4 tensor([1.1445, 1.1400]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0215, -0.9625,  0.0953,  1.6185],\n",
      "        [ 0.0482, -0.5709,  0.0493,  0.9973]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0022, -0.7687,  0.1277,  1.3570],\n",
      "        [ 0.0368, -0.7666,  0.0692,  1.3050]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1445, 1.1400]) - torch.Size([2])\n",
      "Q-values tensor([[0.2013, 0.2142],\n",
      "        [0.1878, 0.1903]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2142],\n",
      "        [0.1878]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0022, -0.7687,  0.1277,  1.3570]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0131451  -0.57534814  0.1548502   1.1068718 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1359, 0.1523],\n",
      "        [0.1275, 0.1432]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1523, 0.1432]) - torch.Size([2])\n",
      "P2 tensor([0.1522, 0.1431]) - torch.Size([2])\n",
      "P3 tensor([0.1522, 0.1431]) - torch.Size([2])\n",
      "P4 tensor([1.1522, 1.1431]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0368, -0.7666,  0.0692,  1.3050],\n",
      "        [ 0.0022, -0.7687,  0.1277,  1.3570]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0215, -0.9625,  0.0953,  1.6185],\n",
      "        [-0.0131, -0.5753,  0.1549,  1.1069]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1522, 1.1431]) - torch.Size([2])\n",
      "Q-values tensor([[0.1998, 0.2041],\n",
      "        [0.1982, 0.2095]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1998],\n",
      "        [0.2095]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0131, -0.5753,  0.1549,  1.1069]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02465206 -0.38256282  0.17698763  0.86649823] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1275, 0.1432],\n",
      "        [0.1308, 0.1446]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1432, 0.1446]) - torch.Size([2])\n",
      "P2 tensor([0.1431, 0.1445]) - torch.Size([2])\n",
      "P3 tensor([0.1431, 0.1445]) - torch.Size([2])\n",
      "P4 tensor([1.1431, 1.1445]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0022, -0.7687,  0.1277,  1.3570],\n",
      "        [ 0.0215, -0.9625,  0.0953,  1.6185]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0131, -0.5753,  0.1549,  1.1069],\n",
      "        [ 0.0022, -0.7687,  0.1277,  1.3570]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1431, 1.1445]) - torch.Size([2])\n",
      "Q-values tensor([[0.2039, 0.2146],\n",
      "        [0.2121, 0.2270]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2146],\n",
      "        [0.2270]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0247, -0.3826,  0.1770,  0.8665]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03230332 -0.19023374  0.1943176   0.6342744 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1275, 0.1432],\n",
      "        [0.1357, 0.1327]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1432, 0.1357]) - torch.Size([2])\n",
      "P2 tensor([0.1431, 0.1356]) - torch.Size([2])\n",
      "P3 tensor([0.1431, 0.1356]) - torch.Size([2])\n",
      "P4 tensor([1.1431, 1.1356]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0022, -0.7687,  0.1277,  1.3570],\n",
      "        [-0.0247, -0.3826,  0.1770,  0.8665]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0131, -0.5753,  0.1549,  1.1069],\n",
      "        [-0.0323, -0.1902,  0.1943,  0.6343]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1431, 1.1356]) - torch.Size([2])\n",
      "Q-values tensor([[0.2044, 0.2217],\n",
      "        [0.1931, 0.2061]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2217],\n",
      "        [0.2061]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0323, -0.1902,  0.1943,  0.6343]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03610799 -0.38745898  0.20700309  0.981315  ] -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [00:00<02:26,  6.77 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "P0 tensor([[0.1357, 0.1327],\n",
      "        [0.1264, 0.1417]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1357, 0.1417]) - torch.Size([2])\n",
      "P2 tensor([0.1356, 0.1416]) - torch.Size([2])\n",
      "P3 tensor([0.1356, 0.1416]) - torch.Size([2])\n",
      "P4 tensor([1.1356, 1.1416]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0247, -0.3826,  0.1770,  0.8665],\n",
      "        [-0.0323, -0.1902,  0.1943,  0.6343]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0323, -0.1902,  0.1943,  0.6343],\n",
      "        [-0.0361, -0.3875,  0.2070,  0.9813]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1356, 1.1416]) - torch.Size([2])\n",
      "Q-values tensor([[0.1936, 0.2117],\n",
      "        [0.2018, 0.2033]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2117],\n",
      "        [0.2018]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0361, -0.3875,  0.2070,  0.9813]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [-0.04385718 -0.19562154  0.22662939  0.7601273 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1264, 0.1417],\n",
      "        [0.1293, 0.1363]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1417, 0.1363]) - torch.Size([2])\n",
      "P2 tensor([0.1416, 0.1362]) - torch.Size([2])\n",
      "P3 tensor([0.1416, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.1416, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0323, -0.1902,  0.1943,  0.6343],\n",
      "        [-0.0361, -0.3875,  0.2070,  0.9813]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0361, -0.3875,  0.2070,  0.9813],\n",
      "        [-0.0439, -0.1956,  0.2266,  0.7601]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1416, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.2062, 0.2064],\n",
      "        [0.1974, 0.2182]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2062],\n",
      "        [0.2182]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0031,  0.0319, -0.0431, -0.0424]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00247996 -0.16257481 -0.04394347  0.23639666] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1640, 0.1206],\n",
      "        [0.1293, 0.1363]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1640, 0.1363]) - torch.Size([2])\n",
      "P2 tensor([0.1638, 0.1362]) - torch.Size([2])\n",
      "P3 tensor([0.1638, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.1638, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0031,  0.0319, -0.0431, -0.0424],\n",
      "        [-0.0361, -0.3875,  0.2070,  0.9813]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0025, -0.1626, -0.0439,  0.2364],\n",
      "        [-0.0439, -0.1956,  0.2266,  0.7601]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1638, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.2702, 0.1936],\n",
      "        [0.2017, 0.2225]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2702],\n",
      "        [0.2225]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0025, -0.1626, -0.0439,  0.2364]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00573146  0.0331465  -0.03921554 -0.06981714] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1640, 0.1206],\n",
      "        [0.1293, 0.1363]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1640, 0.1363]) - torch.Size([2])\n",
      "P2 tensor([0.1638, 0.1362]) - torch.Size([2])\n",
      "P3 tensor([0.1638, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.1638, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0031,  0.0319, -0.0431, -0.0424],\n",
      "        [-0.0361, -0.3875,  0.2070,  0.9813]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0025, -0.1626, -0.0439,  0.2364],\n",
      "        [-0.0439, -0.1956,  0.2266,  0.7601]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1638, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.2751, 0.1956],\n",
      "        [0.2047, 0.2276]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2751],\n",
      "        [0.2276]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0057,  0.0331, -0.0392, -0.0698]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00506853  0.22880809 -0.04061188 -0.3746103 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1970, 0.1060],\n",
      "        [0.1640, 0.1206]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1970, 0.1640]) - torch.Size([2])\n",
      "P2 tensor([0.1968, 0.1638]) - torch.Size([2])\n",
      "P3 tensor([0.1968, 0.1638]) - torch.Size([2])\n",
      "P4 tensor([1.1968, 1.1638]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0057,  0.0331, -0.0392, -0.0698],\n",
      "        [-0.0031,  0.0319, -0.0431, -0.0424]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0051,  0.2288, -0.0406, -0.3746],\n",
      "        [-0.0025, -0.1626, -0.0439,  0.2364]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1968, 1.1638]) - torch.Size([2])\n",
      "Q-values tensor([[0.2831, 0.1982],\n",
      "        [0.2799, 0.1976]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.1982],\n",
      "        [0.2799]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0051,  0.2288, -0.0406, -0.3746]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00049237  0.03428584 -0.04810409 -0.09500411] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1873, 0.1077],\n",
      "        [0.1878, 0.1067]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1873, 0.1878]) - torch.Size([2])\n",
      "P2 tensor([0.1871, 0.1876]) - torch.Size([2])\n",
      "P3 tensor([0.1871, 0.1876]) - torch.Size([2])\n",
      "P4 tensor([1.1871, 1.1876]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0025, -0.1626, -0.0439,  0.2364],\n",
      "        [-0.0051,  0.2288, -0.0406, -0.3746]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0057,  0.0331, -0.0392, -0.0698],\n",
      "        [-0.0005,  0.0343, -0.0481, -0.0950]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1871, 1.1876]) - torch.Size([2])\n",
      "Q-values tensor([[0.2515, 0.2119],\n",
      "        [0.3107, 0.2162]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2119],\n",
      "        [0.3107]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0005,  0.0343, -0.0481, -0.0950]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 1.9334922e-04  2.3006307e-01 -5.0004169e-02 -4.0246740e-01] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1968, 0.1060],\n",
      "        [0.1970, 0.1060]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1968, 0.1970]) - torch.Size([2])\n",
      "P2 tensor([0.1966, 0.1968]) - torch.Size([2])\n",
      "P3 tensor([0.1966, 0.1968]) - torch.Size([2])\n",
      "P4 tensor([1.1966, 1.1968]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0005,  0.0343, -0.0481, -0.0950],\n",
      "        [-0.0057,  0.0331, -0.0392, -0.0698]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 1.9335e-04,  2.3006e-01, -5.0004e-02, -4.0247e-01],\n",
      "        [-5.0685e-03,  2.2881e-01, -4.0612e-02, -3.7461e-01]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1966, 1.1968]) - torch.Size([2])\n",
      "Q-values tensor([[0.2931, 0.2059],\n",
      "        [0.2908, 0.2056]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2059],\n",
      "        [0.2056]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 1.9335e-04,  2.3006e-01, -5.0004e-02, -4.0247e-01]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00479461  0.03568469 -0.05805352 -0.12595937] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1885, 0.1055],\n",
      "        [0.1878, 0.1067]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1885, 0.1878]) - torch.Size([2])\n",
      "P2 tensor([0.1883, 0.1876]) - torch.Size([2])\n",
      "P3 tensor([0.1883, 0.1876]) - torch.Size([2])\n",
      "P4 tensor([1.1883, 1.1876]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 1.9335e-04,  2.3006e-01, -5.0004e-02, -4.0247e-01],\n",
      "        [-5.0685e-03,  2.2881e-01, -4.0612e-02, -3.7461e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0048,  0.0357, -0.0581, -0.1260],\n",
      "        [-0.0005,  0.0343, -0.0481, -0.0950]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1883, 1.1876]) - torch.Size([2])\n",
      "Q-values tensor([[0.3185, 0.2265],\n",
      "        [0.3168, 0.2250]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3185],\n",
      "        [0.3168]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0048,  0.0357, -0.0581, -0.1260]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.0055083   0.23158818 -0.0605727  -0.43637773] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1968, 0.1060],\n",
      "        [0.1974, 0.1055]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1968, 0.1974]) - torch.Size([2])\n",
      "P2 tensor([0.1966, 0.1972]) - torch.Size([2])\n",
      "P3 tensor([0.1966, 0.1972]) - torch.Size([2])\n",
      "P4 tensor([1.1966, 1.1972]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0005,  0.0343, -0.0481, -0.0950],\n",
      "        [ 0.0048,  0.0357, -0.0581, -0.1260]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 1.9335e-04,  2.3006e-01, -5.0004e-02, -4.0247e-01],\n",
      "        [ 5.5083e-03,  2.3159e-01, -6.0573e-02, -4.3638e-01]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1966, 1.1972]) - torch.Size([2])\n",
      "Q-values tensor([[0.2991, 0.2116],\n",
      "        [0.3020, 0.2121]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2116],\n",
      "        [0.2121]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0055,  0.2316, -0.0606, -0.4364]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01014007  0.03737364 -0.06930026 -0.16338767] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1885, 0.1055],\n",
      "        [0.1974, 0.1055]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1885, 0.1974]) - torch.Size([2])\n",
      "P2 tensor([0.1883, 0.1972]) - torch.Size([2])\n",
      "P3 tensor([0.1883, 0.1972]) - torch.Size([2])\n",
      "P4 tensor([1.1883, 1.1972]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 1.9335e-04,  2.3006e-01, -5.0004e-02, -4.0247e-01],\n",
      "        [ 4.7946e-03,  3.5685e-02, -5.8054e-02, -1.2596e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0048,  0.0357, -0.0581, -0.1260],\n",
      "        [ 0.0055,  0.2316, -0.0606, -0.4364]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1883, 1.1972]) - torch.Size([2])\n",
      "Q-values tensor([[0.3260, 0.2327],\n",
      "        [0.3021, 0.2176]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3260],\n",
      "        [0.2176]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0101,  0.0374, -0.0693, -0.1634]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01088754 -0.15669134 -0.07256801  0.10665311] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1725, 0.1187],\n",
      "        [0.1974, 0.1055]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1725, 0.1974]) - torch.Size([2])\n",
      "P2 tensor([0.1723, 0.1972]) - torch.Size([2])\n",
      "P3 tensor([0.1723, 0.1972]) - torch.Size([2])\n",
      "P4 tensor([1.1723, 1.1972]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0101,  0.0374, -0.0693, -0.1634],\n",
      "        [ 0.0048,  0.0357, -0.0581, -0.1260]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0109, -0.1567, -0.0726,  0.1067],\n",
      "        [ 0.0055,  0.2316, -0.0606, -0.4364]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1723, 1.1972]) - torch.Size([2])\n",
      "Q-values tensor([[0.3099, 0.2227],\n",
      "        [0.3062, 0.2214]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3099],\n",
      "        [0.2214]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0109, -0.1567, -0.0726,  0.1067]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00775371 -0.35070238 -0.07043495  0.37558746] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1899, 0.1038],\n",
      "        [0.1532, 0.1270]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1899, 0.1532]) - torch.Size([2])\n",
      "P2 tensor([0.1897, 0.1530]) - torch.Size([2])\n",
      "P3 tensor([0.1897, 0.1530]) - torch.Size([2])\n",
      "P4 tensor([1.1897, 1.1530]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0055,  0.2316, -0.0606, -0.4364],\n",
      "        [ 0.0109, -0.1567, -0.0726,  0.1067]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0101,  0.0374, -0.0693, -0.1634],\n",
      "        [ 0.0078, -0.3507, -0.0704,  0.3756]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1897, 1.1530]) - torch.Size([2])\n",
      "Q-values tensor([[0.3388, 0.2421],\n",
      "        [0.2783, 0.2325]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3388],\n",
      "        [0.2783]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0078, -0.3507, -0.0704,  0.3756]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00073967 -0.5447569  -0.0629232   0.64525676] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1398, 0.1361],\n",
      "        [0.1532, 0.1270]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1398, 0.1532]) - torch.Size([2])\n",
      "P2 tensor([0.1397, 0.1530]) - torch.Size([2])\n",
      "P3 tensor([0.1397, 0.1530]) - torch.Size([2])\n",
      "P4 tensor([1.1397, 1.1530]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0078, -0.3507, -0.0704,  0.3756],\n",
      "        [ 0.0109, -0.1567, -0.0726,  0.1067]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0007, -0.5448, -0.0629,  0.6453],\n",
      "        [ 0.0078, -0.3507, -0.0704,  0.3756]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1397, 1.1530]) - torch.Size([2])\n",
      "Q-values tensor([[0.2577, 0.2382],\n",
      "        [0.2837, 0.2328]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2577],\n",
      "        [0.2837]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0007, -0.5448, -0.0629,  0.6453]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01015547 -0.34881714 -0.05001806  0.33344156] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1532, 0.1270],\n",
      "        [0.1398, 0.1361]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1532, 0.1398]) - torch.Size([2])\n",
      "P2 tensor([0.1530, 0.1397]) - torch.Size([2])\n",
      "P3 tensor([0.1530, 0.1397]) - torch.Size([2])\n",
      "P4 tensor([1.1530, 1.1397]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0109, -0.1567, -0.0726,  0.1067],\n",
      "        [ 0.0078, -0.3507, -0.0704,  0.3756]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0078, -0.3507, -0.0704,  0.3756],\n",
      "        [ 0.0007, -0.5448, -0.0629,  0.6453]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1530, 1.1397]) - torch.Size([2])\n",
      "Q-values tensor([[0.2891, 0.2332],\n",
      "        [0.2632, 0.2387]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2891],\n",
      "        [0.2632]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [00:00<02:42,  6.10 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States tensor([[-0.0102, -0.3488, -0.0500,  0.3334]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01713181 -0.15302029 -0.04334923  0.0254144 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1552, 0.1278],\n",
      "        [0.1398, 0.1361]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1552, 0.1398]) - torch.Size([2])\n",
      "P2 tensor([0.1550, 0.1397]) - torch.Size([2])\n",
      "P3 tensor([0.1550, 0.1397]) - torch.Size([2])\n",
      "P4 tensor([1.1550, 1.1397]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0007, -0.5448, -0.0629,  0.6453],\n",
      "        [ 0.0078, -0.3507, -0.0704,  0.3756]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0102, -0.3488, -0.0500,  0.3334],\n",
      "        [ 0.0007, -0.5448, -0.0629,  0.6453]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1550, 1.1397]) - torch.Size([2])\n",
      "Q-values tensor([[0.2499, 0.2463],\n",
      "        [0.2686, 0.2392]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2463],\n",
      "        [0.2686]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0171, -0.1530, -0.0433,  0.0254]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02019222  0.04269565 -0.04284095 -0.28062442] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1799, 0.1182],\n",
      "        [0.1991, 0.1033]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1799, 0.1991]) - torch.Size([2])\n",
      "P2 tensor([0.1797, 0.1989]) - torch.Size([2])\n",
      "P3 tensor([0.1797, 0.1989]) - torch.Size([2])\n",
      "P4 tensor([1.1797, 1.1989]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0102, -0.3488, -0.0500,  0.3334],\n",
      "        [-0.0171, -0.1530, -0.0433,  0.0254]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0171, -0.1530, -0.0433,  0.0254],\n",
      "        [-0.0202,  0.0427, -0.0428, -0.2806]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1797, 1.1989]) - torch.Size([2])\n",
      "Q-values tensor([[0.2767, 0.2455],\n",
      "        [0.3106, 0.2374]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2455],\n",
      "        [0.2374]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0202,  0.0427, -0.0428, -0.2806]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01933831  0.2384017  -0.04845344 -0.58650553] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.1799, 0.1182],\n",
      "        [0.2081, 0.1045]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.1799, 0.2081]) - torch.Size([2])\n",
      "P2 tensor([0.1797, 0.2079]) - torch.Size([2])\n",
      "P3 tensor([0.1797, 0.2079]) - torch.Size([2])\n",
      "P4 tensor([1.1797, 1.2079]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0102, -0.3488, -0.0500,  0.3334],\n",
      "        [-0.0202,  0.0427, -0.0428, -0.2806]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0171, -0.1530, -0.0433,  0.0254],\n",
      "        [-0.0193,  0.2384, -0.0485, -0.5865]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.1797, 1.2079]) - torch.Size([2])\n",
      "Q-values tensor([[0.2770, 0.2507],\n",
      "        [0.3512, 0.2413]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2507],\n",
      "        [0.2413]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0193,  0.2384, -0.0485, -0.5865]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01457027  0.4341676  -0.06018354 -0.89404947] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2253, 0.1139],\n",
      "        [0.1991, 0.1033]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2253, 0.1991]) - torch.Size([2])\n",
      "P2 tensor([0.2251, 0.1989]) - torch.Size([2])\n",
      "P3 tensor([0.2251, 0.1989]) - torch.Size([2])\n",
      "P4 tensor([1.2251, 1.1989]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0193,  0.2384, -0.0485, -0.5865],\n",
      "        [-0.0171, -0.1530, -0.0433,  0.0254]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0146,  0.4342, -0.0602, -0.8940],\n",
      "        [-0.0202,  0.0427, -0.0428, -0.2806]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2251, 1.1989]) - torch.Size([2])\n",
      "Q-values tensor([[0.3819, 0.2666],\n",
      "        [0.3109, 0.2481]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2666],\n",
      "        [0.2481]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0146,  0.4342, -0.0602, -0.8940]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00588692  0.63005185 -0.07806453 -1.2050271 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2081, 0.1045],\n",
      "        [0.2253, 0.1139]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2081, 0.2253]) - torch.Size([2])\n",
      "P2 tensor([0.2079, 0.2251]) - torch.Size([2])\n",
      "P3 tensor([0.2079, 0.2251]) - torch.Size([2])\n",
      "P4 tensor([1.2079, 1.2251]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0202,  0.0427, -0.0428, -0.2806],\n",
      "        [-0.0193,  0.2384, -0.0485, -0.5865]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0193,  0.2384, -0.0485, -0.5865],\n",
      "        [-0.0146,  0.4342, -0.0602, -0.8940]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2079, 1.2251]) - torch.Size([2])\n",
      "Q-values tensor([[0.3513, 0.2543],\n",
      "        [0.3821, 0.2745]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2543],\n",
      "        [0.2745]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0059,  0.6301, -0.0781, -1.2050]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00671411  0.43602076 -0.10216507 -0.93779457] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2464, 0.1302],\n",
      "        [0.2253, 0.1139]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2464, 0.2253]) - torch.Size([2])\n",
      "P2 tensor([0.2461, 0.2251]) - torch.Size([2])\n",
      "P3 tensor([0.2461, 0.2251]) - torch.Size([2])\n",
      "P4 tensor([1.2461, 1.2251]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0146,  0.4342, -0.0602, -0.8940],\n",
      "        [-0.0193,  0.2384, -0.0485, -0.5865]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0059,  0.6301, -0.0781, -1.2050],\n",
      "        [-0.0146,  0.4342, -0.0602, -0.8940]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2461, 1.2251]) - torch.Size([2])\n",
      "Q-values tensor([[0.4253, 0.3098],\n",
      "        [0.3824, 0.2824]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3098],\n",
      "        [0.2824]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0067,  0.4360, -0.1022, -0.9378]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01543453  0.6323608  -0.12092096 -1.2607533 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2260, 0.1150],\n",
      "        [0.2489, 0.1344]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2260, 0.2489]) - torch.Size([2])\n",
      "P2 tensor([0.2257, 0.2486]) - torch.Size([2])\n",
      "P3 tensor([0.2257, 0.2486]) - torch.Size([2])\n",
      "P4 tensor([1.2257, 1.2486]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0059,  0.6301, -0.0781, -1.2050],\n",
      "        [ 0.0067,  0.4360, -0.1022, -0.9378]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0067,  0.4360, -0.1022, -0.9378],\n",
      "        [ 0.0154,  0.6324, -0.1209, -1.2608]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2257, 1.2486]) - torch.Size([2])\n",
      "Q-values tensor([[0.4682, 0.3491],\n",
      "        [0.4290, 0.3220]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4682],\n",
      "        [0.3220]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0154,  0.6324, -0.1209, -1.2608]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.02808174  0.438975   -0.14613603 -1.0082569 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2284, 0.1171],\n",
      "        [0.2489, 0.1344]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2284, 0.2489]) - torch.Size([2])\n",
      "P2 tensor([0.2282, 0.2486]) - torch.Size([2])\n",
      "P3 tensor([0.2282, 0.2486]) - torch.Size([2])\n",
      "P4 tensor([1.2282, 1.2486]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0154,  0.6324, -0.1209, -1.2608],\n",
      "        [ 0.0067,  0.4360, -0.1022, -0.9378]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0281,  0.4390, -0.1461, -1.0083],\n",
      "        [ 0.0154,  0.6324, -0.1209, -1.2608]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2282, 1.2486]) - torch.Size([2])\n",
      "Q-values tensor([[0.4824, 0.3623],\n",
      "        [0.4352, 0.3287]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4824],\n",
      "        [0.3287]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0281,  0.4390, -0.1461, -1.0083]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.03686124  0.6357134  -0.16630118 -1.3430302 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5005, 0.3806],\n",
      "        [0.4908, 0.3697]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5005, 0.4908]) - torch.Size([2])\n",
      "P2 tensor([0.5000, 0.4904]) - torch.Size([2])\n",
      "P3 tensor([0.5000, 0.4904]) - torch.Size([2])\n",
      "P4 tensor([1.5000, 1.4904]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0281,  0.4390, -0.1461, -1.0083],\n",
      "        [ 0.0067,  0.4360, -0.1022, -0.9378]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0369,  0.6357, -0.1663, -1.3430],\n",
      "        [ 0.0154,  0.6324, -0.1209, -1.2608]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5000, 1.4904]) - torch.Size([2])\n",
      "Q-values tensor([[0.4496, 0.3414],\n",
      "        [0.4416, 0.3354]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3414],\n",
      "        [0.3354]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0369,  0.6357, -0.1663, -1.3430]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04957552  0.83249074 -0.19316177 -1.6827916 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5560, 0.4317],\n",
      "        [0.5005, 0.3806]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5560, 0.5005]) - torch.Size([2])\n",
      "P2 tensor([0.5555, 0.5000]) - torch.Size([2])\n",
      "P3 tensor([0.5555, 0.5000]) - torch.Size([2])\n",
      "P4 tensor([1.5555, 1.5000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0369,  0.6357, -0.1663, -1.3430],\n",
      "        [ 0.0281,  0.4390, -0.1461, -1.0083]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0496,  0.8325, -0.1932, -1.6828],\n",
      "        [ 0.0369,  0.6357, -0.1663, -1.3430]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5555, 1.5000]) - torch.Size([2])\n",
      "Q-values tensor([[0.5007, 0.3915],\n",
      "        [0.4496, 0.3508]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3915],\n",
      "        [0.3508]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0496,  0.8325, -0.1932, -1.6828]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.06622533  1.0292537  -0.22681761 -2.028884  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5005, 0.3806],\n",
      "        [0.6243, 0.4907]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5005, 0.6243]) - torch.Size([2])\n",
      "P2 tensor([0.5000, 0.6237]) - torch.Size([2])\n",
      "P3 tensor([0.5000, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.5000, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0281,  0.4390, -0.1461, -1.0083],\n",
      "        [ 0.0496,  0.8325, -0.1932, -1.6828]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0369,  0.6357, -0.1663, -1.3430],\n",
      "        [ 0.0662,  1.0293, -0.2268, -2.0289]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5000, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.4496, 0.3602],\n",
      "        [0.5557, 0.4570]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3602],\n",
      "        [0.4570]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0470, 0.0376, 0.0485, 0.0258]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04774257  0.2319552   0.04902184 -0.25115007] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5560, 0.4317],\n",
      "        [0.3524, 0.2747]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5560, 0.3524]) - torch.Size([2])\n",
      "P2 tensor([0.5555, 0.3521]) - torch.Size([2])\n",
      "P3 tensor([0.5555, 0.3521]) - torch.Size([2])\n",
      "P4 tensor([1.5555, 1.3521]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0369,  0.6357, -0.1663, -1.3430],\n",
      "        [ 0.0470,  0.0376,  0.0485,  0.0258]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0496,  0.8325, -0.1932, -1.6828],\n",
      "        [ 0.0477,  0.2320,  0.0490, -0.2512]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5555, 1.3521]) - torch.Size([2])\n",
      "Q-values tensor([[0.4999, 0.4156],\n",
      "        [0.3193, 0.2731]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4156],\n",
      "        [0.2731]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0477,  0.2320,  0.0490, -0.2512]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05238168  0.4263441   0.04399884 -0.5279765 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3807, 0.2970],\n",
      "        [0.6243, 0.4907]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3807, 0.6243]) - torch.Size([2])\n",
      "P2 tensor([0.3803, 0.6237]) - torch.Size([2])\n",
      "P3 tensor([0.3803, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.3803, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0477,  0.2320,  0.0490, -0.2512],\n",
      "        [ 0.0496,  0.8325, -0.1932, -1.6828]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0524,  0.4263,  0.0440, -0.5280],\n",
      "        [ 0.0662,  1.0293, -0.2268, -2.0289]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3803, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.3532, 0.2984],\n",
      "        [0.5552, 0.4843]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2984],\n",
      "        [0.4843]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0524,  0.4263,  0.0440, -0.5280]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06090856  0.23063162  0.03343931 -0.22176039] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3524, 0.2747],\n",
      "        [0.3807, 0.2970]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3524, 0.3807]) - torch.Size([2])\n",
      "P2 tensor([0.3521, 0.3803]) - torch.Size([2])\n",
      "P3 tensor([0.3521, 0.3803]) - torch.Size([2])\n",
      "P4 tensor([1.3521, 1.3803]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0470,  0.0376,  0.0485,  0.0258],\n",
      "        [ 0.0477,  0.2320,  0.0490, -0.2512]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0477,  0.2320,  0.0490, -0.2512],\n",
      "        [ 0.0524,  0.4263,  0.0440, -0.5280]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3521, 1.3803]) - torch.Size([2])\n",
      "Q-values tensor([[0.3197, 0.2834],\n",
      "        [0.3532, 0.3050]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.2834],\n",
      "        [0.3050]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0609,  0.2306,  0.0334, -0.2218]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06552119  0.42526004  0.0290041  -0.5037105 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3481, 0.2724],\n",
      "        [0.3807, 0.2970]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3481, 0.3807]) - torch.Size([2])\n",
      "P2 tensor([0.3478, 0.3803]) - torch.Size([2])\n",
      "P3 tensor([0.3478, 0.3803]) - torch.Size([2])\n",
      "P4 tensor([1.3478, 1.3803]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0524,  0.4263,  0.0440, -0.5280],\n",
      "        [ 0.0477,  0.2320,  0.0490, -0.2512]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0609,  0.2306,  0.0334, -0.2218],\n",
      "        [ 0.0524,  0.4263,  0.0440, -0.5280]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3478, 1.3803]) - torch.Size([2])\n",
      "Q-values tensor([[0.3808, 0.3398],\n",
      "        [0.3531, 0.3116]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3808],\n",
      "        [0.3116]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0655,  0.4253,  0.0290, -0.5037]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.07402639  0.61996144  0.01892989 -0.7871138 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3760, 0.2952],\n",
      "        [0.4121, 0.3246]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3760, 0.4121]) - torch.Size([2])\n",
      "P2 tensor([0.3756, 0.4117]) - torch.Size([2])\n",
      "P3 tensor([0.3756, 0.4117]) - torch.Size([2])\n",
      "P4 tensor([1.3756, 1.4117]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0609,  0.2306,  0.0334, -0.2218],\n",
      "        [ 0.0655,  0.4253,  0.0290, -0.5037]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0655,  0.4253,  0.0290, -0.5037],\n",
      "        [ 0.0740,  0.6200,  0.0189, -0.7871]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3756, 1.4117]) - torch.Size([2])\n",
      "Q-values tensor([[0.3539, 0.3130],\n",
      "        [0.3822, 0.3419]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3130],\n",
      "        [0.3419]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0740,  0.6200,  0.0189, -0.7871]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.08642562  0.8148183   0.00318761 -1.0737816 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4121, 0.3246],\n",
      "        [0.3760, 0.2952]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4121, 0.3760]) - torch.Size([2])\n",
      "P2 tensor([0.4117, 0.3756]) - torch.Size([2])\n",
      "P3 tensor([0.4117, 0.3756]) - torch.Size([2])\n",
      "P4 tensor([1.4117, 1.3756]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0655,  0.4253,  0.0290, -0.5037],\n",
      "        [ 0.0609,  0.2306,  0.0334, -0.2218]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0740,  0.6200,  0.0189, -0.7871],\n",
      "        [ 0.0655,  0.4253,  0.0290, -0.5037]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4117, 1.3756]) - torch.Size([2])\n",
      "Q-values tensor([[0.3820, 0.3496],\n",
      "        [0.3538, 0.3196]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3496],\n",
      "        [0.3196]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0864,  0.8148,  0.0032, -1.0738]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.10272199  0.6196544  -0.01828802 -0.7801001 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4518, 0.3601],\n",
      "        [0.4121, 0.3246]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4518, 0.4121]) - torch.Size([2])\n",
      "P2 tensor([0.4514, 0.4117]) - torch.Size([2])\n",
      "P3 tensor([0.4514, 0.4117]) - torch.Size([2])\n",
      "P4 tensor([1.4514, 1.4117]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0740,  0.6200,  0.0189, -0.7871],\n",
      "        [ 0.0655,  0.4253,  0.0290, -0.5037]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0864,  0.8148,  0.0032, -1.0738],\n",
      "        [ 0.0740,  0.6200,  0.0189, -0.7871]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4514, 1.4117]) - torch.Size([2])\n",
      "Q-values tensor([[0.4197, 0.3960],\n",
      "        [0.3817, 0.3573]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3960],\n",
      "        [0.3573]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1027,  0.6197, -0.0183, -0.7801]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.11511508  0.81502295 -0.03389002 -1.0784802 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4488, 0.3615],\n",
      "        [0.4073, 0.3244]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4488, 0.4073]) - torch.Size([2])\n",
      "P2 tensor([0.4484, 0.4069]) - torch.Size([2])\n",
      "P3 tensor([0.4484, 0.4069]) - torch.Size([2])\n",
      "P4 tensor([1.4484, 1.4069]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1027,  0.6197, -0.0183, -0.7801],\n",
      "        [ 0.0864,  0.8148,  0.0032, -1.0738]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1151,  0.8150, -0.0339, -1.0785],\n",
      "        [ 0.1027,  0.6197, -0.0183, -0.7801]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4484, 1.4069]) - torch.Size([2])\n",
      "Q-values tensor([[0.4157, 0.4034],\n",
      "        [0.4626, 0.4507]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4034],\n",
      "        [0.4626]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1151,  0.8150, -0.0339, -1.0785]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.13141553  1.0105757  -0.05545963 -1.3816026 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4073, 0.3244],\n",
      "        [0.5016, 0.4057]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4073, 0.5016]) - torch.Size([2])\n",
      "P2 tensor([0.4069, 0.5011]) - torch.Size([2])\n",
      "P3 tensor([0.4069, 0.5011]) - torch.Size([2])\n",
      "P4 tensor([1.4069, 1.5011]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0864,  0.8148,  0.0032, -1.0738],\n",
      "        [ 0.1151,  0.8150, -0.0339, -1.0785]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1027,  0.6197, -0.0183, -0.7801],\n",
      "        [ 0.1314,  1.0106, -0.0555, -1.3816]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4069, 1.5011]) - torch.Size([2])\n",
      "Q-values tensor([[0.4706, 0.4571],\n",
      "        [0.4680, 0.4581]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4706],\n",
      "        [0.4581]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1314,  1.0106, -0.0555, -1.3816]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.15162705  0.8161879  -0.08309168 -1.1067662 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4501, 0.3669],\n",
      "        [0.4488, 0.3615]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4501, 0.4488]) - torch.Size([2])\n",
      "P2 tensor([0.4497, 0.4484]) - torch.Size([2])\n",
      "P3 tensor([0.4497, 0.4484]) - torch.Size([2])\n",
      "P4 tensor([1.4497, 1.4484]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1314,  1.0106, -0.0555, -1.3816],\n",
      "        [ 0.1027,  0.6197, -0.0183, -0.7801]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1516,  0.8162, -0.0831, -1.1068],\n",
      "        [ 0.1151,  0.8150, -0.0339, -1.0785]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4497, 1.4484]) - torch.Size([2])\n",
      "Q-values tensor([[0.5305, 0.5251],\n",
      "        [0.4275, 0.4164]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5305],\n",
      "        [0.4164]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1516,  0.8162, -0.0831, -1.1068]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.16795081  1.012298   -0.105227   -1.4243172 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4501, 0.3669],\n",
      "        [0.5016, 0.4057]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4501, 0.5016]) - torch.Size([2])\n",
      "P2 tensor([0.4497, 0.5011]) - torch.Size([2])\n",
      "P3 tensor([0.4497, 0.5011]) - torch.Size([2])\n",
      "P4 tensor([1.4497, 1.5011]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1314,  1.0106, -0.0555, -1.3816],\n",
      "        [ 0.1151,  0.8150, -0.0339, -1.0785]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1516,  0.8162, -0.0831, -1.1068],\n",
      "        [ 0.1314,  1.0106, -0.0555, -1.3816]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4497, 1.5011]) - torch.Size([2])\n",
      "Q-values tensor([[0.5414, 0.5315],\n",
      "        [0.4826, 0.4725]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5414],\n",
      "        [0.4725]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1680,  1.0123, -0.1052, -1.4243]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.18819676  0.8186225  -0.13371335 -1.1662906 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4501, 0.3669],\n",
      "        [0.5062, 0.4152]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4501, 0.5062]) - torch.Size([2])\n",
      "P2 tensor([0.4497, 0.5057]) - torch.Size([2])\n",
      "P3 tensor([0.4497, 0.5057]) - torch.Size([2])\n",
      "P4 tensor([1.4497, 1.5057]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1314,  1.0106, -0.0555, -1.3816],\n",
      "        [ 0.1516,  0.8162, -0.0831, -1.1068]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1516,  0.8162, -0.0831, -1.1068],\n",
      "        [ 0.1680,  1.0123, -0.1052, -1.4243]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4497, 1.5057]) - torch.Size([2])\n",
      "Q-values tensor([[0.5504, 0.5400],\n",
      "        [0.4927, 0.4868]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5504],\n",
      "        [0.4868]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1882,  0.8186, -0.1337, -1.1663]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.20456922  1.0152068  -0.15703917 -1.4977295 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5062, 0.4152],\n",
      "        [0.4591, 0.3785]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5062, 0.4591]) - torch.Size([2])\n",
      "P2 tensor([0.5057, 0.4587]) - torch.Size([2])\n",
      "P3 tensor([0.5057, 0.4587]) - torch.Size([2])\n",
      "P4 tensor([1.5057, 1.4587]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1516,  0.8162, -0.0831, -1.1068],\n",
      "        [ 0.1680,  1.0123, -0.1052, -1.4243]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1680,  1.0123, -0.1052, -1.4243],\n",
      "        [ 0.1882,  0.8186, -0.1337, -1.1663]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5057, 1.4587]) - torch.Size([2])\n",
      "Q-values tensor([[0.5001, 0.4947],\n",
      "        [0.5647, 0.5602]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4947],\n",
      "        [0.5647]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:01<02:34,  6.42 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States tensor([[ 0.2046,  1.0152, -0.1570, -1.4977]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.22487335  1.2118498  -0.18699375 -1.8350445 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4591, 0.3785],\n",
      "        [0.5804, 0.4843]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4591, 0.5804]) - torch.Size([2])\n",
      "P2 tensor([0.4587, 0.5799]) - torch.Size([2])\n",
      "P3 tensor([0.4587, 0.5799]) - torch.Size([2])\n",
      "P4 tensor([1.4587, 1.5799]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1680,  1.0123, -0.1052, -1.4243],\n",
      "        [ 0.2046,  1.0152, -0.1570, -1.4977]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1882,  0.8186, -0.1337, -1.1663],\n",
      "        [ 0.2249,  1.2118, -0.1870, -1.8350]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4587, 1.5799]) - torch.Size([2])\n",
      "Q-values tensor([[0.5741, 0.5690],\n",
      "        [0.5859, 0.5849]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5741],\n",
      "        [0.5849]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.2249,  1.2118, -0.1870, -1.8350]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.24911036  1.0192239  -0.22369464 -1.6057988 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5345, 0.4471],\n",
      "        [0.5804, 0.4843]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5345, 0.5804]) - torch.Size([2])\n",
      "P2 tensor([0.5340, 0.5799]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.5799]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.5799]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.2249,  1.2118, -0.1870, -1.8350],\n",
      "        [ 0.2046,  1.0152, -0.1570, -1.4977]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.2491,  1.0192, -0.2237, -1.6058],\n",
      "        [ 0.2249,  1.2118, -0.1870, -1.8350]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.5799]) - torch.Size([2])\n",
      "Q-values tensor([[0.6678, 0.6751],\n",
      "        [0.5937, 0.5960]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6678],\n",
      "        [0.5960]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0143,  0.0052, -0.0202, -0.0014]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01421844  0.20062435 -0.02026252 -0.30040863] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3596, 0.2774],\n",
      "        [0.5804, 0.4843]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3596, 0.5804]) - torch.Size([2])\n",
      "P2 tensor([0.3593, 0.5799]) - torch.Size([2])\n",
      "P3 tensor([0.3593, 0.5799]) - torch.Size([2])\n",
      "P4 tensor([1.3593, 1.5799]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-1.4323e-02,  5.2181e-03, -2.0234e-02, -1.4108e-03],\n",
      "        [ 2.0457e-01,  1.0152e+00, -1.5704e-01, -1.4977e+00]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0142,  0.2006, -0.0203, -0.3004],\n",
      "        [ 0.2249,  1.2118, -0.1870, -1.8350]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3593, 1.5799]) - torch.Size([2])\n",
      "Q-values tensor([[0.3511, 0.3365],\n",
      "        [0.5988, 0.6090]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3365],\n",
      "        [0.6090]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0142,  0.2006, -0.0203, -0.3004]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01020596  0.00579698 -0.02627069 -0.01418434] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5345, 0.4471],\n",
      "        [0.3246, 0.2608]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5345, 0.3246]) - torch.Size([2])\n",
      "P2 tensor([0.5340, 0.3243]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.3243]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.3243]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.2249,  1.2118, -0.1870, -1.8350],\n",
      "        [-0.0142,  0.2006, -0.0203, -0.3004]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.2491,  1.0192, -0.2237, -1.6058],\n",
      "        [-0.0102,  0.0058, -0.0263, -0.0142]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.3243]) - torch.Size([2])\n",
      "Q-values tensor([[0.6748, 0.7047],\n",
      "        [0.3931, 0.3829]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6748],\n",
      "        [0.3931]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0102,  0.0058, -0.0263, -0.0142]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01009002  0.20128565 -0.02655438 -0.31503883] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3596, 0.2774],\n",
      "        [0.3246, 0.2608]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3596, 0.3246]) - torch.Size([2])\n",
      "P2 tensor([0.3593, 0.3243]) - torch.Size([2])\n",
      "P3 tensor([0.3593, 0.3243]) - torch.Size([2])\n",
      "P4 tensor([1.3593, 1.3243]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0143,  0.0052, -0.0202, -0.0014],\n",
      "        [-0.0142,  0.2006, -0.0203, -0.3004]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0142,  0.2006, -0.0203, -0.3004],\n",
      "        [-0.0102,  0.0058, -0.0263, -0.0142]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3593, 1.3243]) - torch.Size([2])\n",
      "Q-values tensor([[0.3567, 0.3420],\n",
      "        [0.4002, 0.3827]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3420],\n",
      "        [0.4002]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0101,  0.2013, -0.0266, -0.3150]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0060643   0.39677557 -0.03285515 -0.61597645] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3951, 0.3055],\n",
      "        [0.3607, 0.2786]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3951, 0.3607]) - torch.Size([2])\n",
      "P2 tensor([0.3947, 0.3603]) - torch.Size([2])\n",
      "P3 tensor([0.3947, 0.3603]) - torch.Size([2])\n",
      "P4 tensor([1.3947, 1.3603]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0101,  0.2013, -0.0266, -0.3150],\n",
      "        [-0.0102,  0.0058, -0.0263, -0.0142]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0061,  0.3968, -0.0329, -0.6160],\n",
      "        [-0.0101,  0.2013, -0.0266, -0.3150]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3947, 1.3603]) - torch.Size([2])\n",
      "Q-values tensor([[0.4071, 0.3887],\n",
      "        [0.3621, 0.3465]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3887],\n",
      "        [0.3465]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0061,  0.3968, -0.0329, -0.6160]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00187121  0.59234077 -0.04517468 -0.9188236 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3951, 0.3055],\n",
      "        [0.3607, 0.2786]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3951, 0.3607]) - torch.Size([2])\n",
      "P2 tensor([0.3947, 0.3603]) - torch.Size([2])\n",
      "P3 tensor([0.3947, 0.3603]) - torch.Size([2])\n",
      "P4 tensor([1.3947, 1.3603]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0101,  0.2013, -0.0266, -0.3150],\n",
      "        [-0.0102,  0.0058, -0.0263, -0.0142]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0061,  0.3968, -0.0329, -0.6160],\n",
      "        [-0.0101,  0.2013, -0.0266, -0.3150]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3947, 1.3603]) - torch.Size([2])\n",
      "Q-values tensor([[0.4069, 0.3957],\n",
      "        [0.3620, 0.3522]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.3957],\n",
      "        [0.3522]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0019,  0.5923, -0.0452, -0.9188]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01371802  0.39785767 -0.06355115 -0.6406734 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3966, 0.3074],\n",
      "        [0.3951, 0.3055]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3966, 0.3951]) - torch.Size([2])\n",
      "P2 tensor([0.3962, 0.3947]) - torch.Size([2])\n",
      "P3 tensor([0.3962, 0.3947]) - torch.Size([2])\n",
      "P4 tensor([1.3962, 1.3947]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0019,  0.5923, -0.0452, -0.9188],\n",
      "        [-0.0101,  0.2013, -0.0266, -0.3150]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0137,  0.3979, -0.0636, -0.6407],\n",
      "        [-0.0061,  0.3968, -0.0329, -0.6160]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3962, 1.3947]) - torch.Size([2])\n",
      "Q-values tensor([[0.5107, 0.5047],\n",
      "        [0.4066, 0.4026]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5107],\n",
      "        [0.4026]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0137,  0.3979, -0.0636, -0.6407]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.02167518  0.20367654 -0.07636462 -0.36866117] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4376, 0.3353],\n",
      "        [0.3966, 0.3074]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4376, 0.3966]) - torch.Size([2])\n",
      "P2 tensor([0.4371, 0.3962]) - torch.Size([2])\n",
      "P3 tensor([0.4371, 0.3962]) - torch.Size([2])\n",
      "P4 tensor([1.4371, 1.3962]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0061,  0.3968, -0.0329, -0.6160],\n",
      "        [ 0.0019,  0.5923, -0.0452, -0.9188]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0019,  0.5923, -0.0452, -0.9188],\n",
      "        [ 0.0137,  0.3979, -0.0636, -0.6407]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4371, 1.3962]) - torch.Size([2])\n",
      "Q-values tensor([[0.4604, 0.4551],\n",
      "        [0.5190, 0.5095]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4551],\n",
      "        [0.5190]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0217,  0.2037, -0.0764, -0.3687]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.02574871  0.39979577 -0.08373784 -0.68441206] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3621, 0.2844],\n",
      "        [0.4023, 0.3110]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3621, 0.4023]) - torch.Size([2])\n",
      "P2 tensor([0.3617, 0.4019]) - torch.Size([2])\n",
      "P3 tensor([0.3617, 0.4019]) - torch.Size([2])\n",
      "P4 tensor([1.3617, 1.4019]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0137,  0.3979, -0.0636, -0.6407],\n",
      "        [ 0.0217,  0.2037, -0.0764, -0.3687]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0217,  0.2037, -0.0764, -0.3687],\n",
      "        [ 0.0257,  0.3998, -0.0837, -0.6844]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3617, 1.4019]) - torch.Size([2])\n",
      "Q-values tensor([[0.4693, 0.4641],\n",
      "        [0.4194, 0.4215]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4693],\n",
      "        [0.4215]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0257,  0.3998, -0.0837, -0.6844]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.03374462  0.5959744  -0.09742609 -1.0022391 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3621, 0.2844],\n",
      "        [0.4461, 0.3445]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3621, 0.4461]) - torch.Size([2])\n",
      "P2 tensor([0.3617, 0.4457]) - torch.Size([2])\n",
      "P3 tensor([0.3617, 0.4457]) - torch.Size([2])\n",
      "P4 tensor([1.3617, 1.4457]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0137,  0.3979, -0.0636, -0.6407],\n",
      "        [ 0.0257,  0.3998, -0.0837, -0.6844]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0217,  0.2037, -0.0764, -0.3687],\n",
      "        [ 0.0337,  0.5960, -0.0974, -1.0022]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3617, 1.4457]) - torch.Size([2])\n",
      "Q-values tensor([[0.4756, 0.4692],\n",
      "        [0.4835, 0.4766]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4756],\n",
      "        [0.4766]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0337,  0.5960, -0.0974, -1.0022]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04566411  0.40227965 -0.11747087 -0.7416727 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4023, 0.3110],\n",
      "        [0.4461, 0.3445]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4023, 0.4461]) - torch.Size([2])\n",
      "P2 tensor([0.4019, 0.4457]) - torch.Size([2])\n",
      "P3 tensor([0.4019, 0.4457]) - torch.Size([2])\n",
      "P4 tensor([1.4019, 1.4457]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0217,  0.2037, -0.0764, -0.3687],\n",
      "        [ 0.0257,  0.3998, -0.0837, -0.6844]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0257,  0.3998, -0.0837, -0.6844],\n",
      "        [ 0.0337,  0.5960, -0.0974, -1.0022]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4019, 1.4457]) - torch.Size([2])\n",
      "Q-values tensor([[0.4288, 0.4317],\n",
      "        [0.4889, 0.4831]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4317],\n",
      "        [0.4831]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0457,  0.4023, -0.1175, -0.7417]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.0537097   0.20895834 -0.13230433 -0.4881463 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4461, 0.3445],\n",
      "        [0.4077, 0.3161]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4461, 0.4077]) - torch.Size([2])\n",
      "P2 tensor([0.4457, 0.4073]) - torch.Size([2])\n",
      "P3 tensor([0.4457, 0.4073]) - torch.Size([2])\n",
      "P4 tensor([1.4457, 1.4073]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0257,  0.3998, -0.0837, -0.6844],\n",
      "        [ 0.0337,  0.5960, -0.0974, -1.0022]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0337,  0.5960, -0.0974, -1.0022],\n",
      "        [ 0.0457,  0.4023, -0.1175, -0.7417]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4457, 1.4073]) - torch.Size([2])\n",
      "Q-values tensor([[0.4889, 0.4914],\n",
      "        [0.5531, 0.5546]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4914],\n",
      "        [0.5531]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0537,  0.2090, -0.1323, -0.4881]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05788887  0.0159269  -0.14206725 -0.23991369] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3443, 0.2732],\n",
      "        [0.3739, 0.2943]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3443, 0.3739]) - torch.Size([2])\n",
      "P2 tensor([0.3440, 0.3735]) - torch.Size([2])\n",
      "P3 tensor([0.3440, 0.3735]) - torch.Size([2])\n",
      "P4 tensor([1.3440, 1.3735]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0537,  0.2090, -0.1323, -0.4881],\n",
      "        [ 0.0457,  0.4023, -0.1175, -0.7417]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0579,  0.0159, -0.1421, -0.2399],\n",
      "        [ 0.0537,  0.2090, -0.1323, -0.4881]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3440, 1.3735]) - torch.Size([2])\n",
      "Q-values tensor([[0.4522, 0.4646],\n",
      "        [0.5047, 0.5067]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4522],\n",
      "        [0.5047]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0579,  0.0159, -0.1421, -0.2399]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05820741  0.21276231 -0.14686552 -0.57381964] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3843, 0.3005],\n",
      "        [0.3443, 0.2732]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3843, 0.3443]) - torch.Size([2])\n",
      "P2 tensor([0.3840, 0.3440]) - torch.Size([2])\n",
      "P3 tensor([0.3840, 0.3440]) - torch.Size([2])\n",
      "P4 tensor([1.3840, 1.3440]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0579,  0.0159, -0.1421, -0.2399],\n",
      "        [ 0.0537,  0.2090, -0.1323, -0.4881]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0582,  0.2128, -0.1469, -0.5738],\n",
      "        [ 0.0579,  0.0159, -0.1421, -0.2399]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3840, 1.3440]) - torch.Size([2])\n",
      "Q-values tensor([[0.4113, 0.4210],\n",
      "        [0.4601, 0.4646]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4210],\n",
      "        [0.4601]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0582,  0.2128, -0.1469, -0.5738]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06246265  0.01997155 -0.15834191 -0.33077013] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3443, 0.2732],\n",
      "        [0.3843, 0.3005]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3443, 0.3843]) - torch.Size([2])\n",
      "P2 tensor([0.3440, 0.3840]) - torch.Size([2])\n",
      "P3 tensor([0.3440, 0.3840]) - torch.Size([2])\n",
      "P4 tensor([1.3440, 1.3840]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0537,  0.2090, -0.1323, -0.4881],\n",
      "        [ 0.0579,  0.0159, -0.1421, -0.2399]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0579,  0.0159, -0.1421, -0.2399],\n",
      "        [ 0.0582,  0.2128, -0.1469, -0.5738]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3440, 1.3840]) - torch.Size([2])\n",
      "Q-values tensor([[0.4658, 0.4698],\n",
      "        [0.4157, 0.4261]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4658],\n",
      "        [0.4261]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0625,  0.0200, -0.1583, -0.3308]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06286208 -0.17258383 -0.16495731 -0.09190857] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3542, 0.2810],\n",
      "        [0.3843, 0.3005]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [00:01<02:46,  5.96 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 tensor([0.3542, 0.3843]) - torch.Size([2])\n",
      "P2 tensor([0.3538, 0.3840]) - torch.Size([2])\n",
      "P3 tensor([0.3538, 0.3840]) - torch.Size([2])\n",
      "P4 tensor([1.3538, 1.3840]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0582,  0.2128, -0.1469, -0.5738],\n",
      "        [ 0.0579,  0.0159, -0.1421, -0.2399]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0625,  0.0200, -0.1583, -0.3308],\n",
      "        [ 0.0582,  0.2128, -0.1469, -0.5738]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3538, 1.3840]) - torch.Size([2])\n",
      "Q-values tensor([[0.4878, 0.4874],\n",
      "        [0.4201, 0.4312]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4878],\n",
      "        [0.4312]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0629, -0.1726, -0.1650, -0.0919]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05941041  0.02447101 -0.16679549 -0.43175623] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3231, 0.2678],\n",
      "        [0.3666, 0.2877]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3231, 0.3666]) - torch.Size([2])\n",
      "P2 tensor([0.3228, 0.3663]) - torch.Size([2])\n",
      "P3 tensor([0.3228, 0.3663]) - torch.Size([2])\n",
      "P4 tensor([1.3228, 1.3663]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0625,  0.0200, -0.1583, -0.3308],\n",
      "        [ 0.0629, -0.1726, -0.1650, -0.0919]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0629, -0.1726, -0.1650, -0.0919],\n",
      "        [ 0.0594,  0.0245, -0.1668, -0.4318]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3228, 1.3663]) - torch.Size([2])\n",
      "Q-values tensor([[0.4417, 0.4520],\n",
      "        [0.3941, 0.4147]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4417],\n",
      "        [0.4147]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0594,  0.0245, -0.1668, -0.4318]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05989983 -0.16794509 -0.17543061 -0.19594845] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3666, 0.2877],\n",
      "        [0.3351, 0.2722]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3666, 0.3351]) - torch.Size([2])\n",
      "P2 tensor([0.3663, 0.3348]) - torch.Size([2])\n",
      "P3 tensor([0.3663, 0.3348]) - torch.Size([2])\n",
      "P4 tensor([1.3663, 1.3348]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0629, -0.1726, -0.1650, -0.0919],\n",
      "        [ 0.0594,  0.0245, -0.1668, -0.4318]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0594,  0.0245, -0.1668, -0.4318],\n",
      "        [ 0.0599, -0.1679, -0.1754, -0.1959]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3663, 1.3348]) - torch.Size([2])\n",
      "Q-values tensor([[0.3982, 0.4199],\n",
      "        [0.4677, 0.4735]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4199],\n",
      "        [0.4677]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0599, -0.1679, -0.1754, -0.1959]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05654093  0.02919555 -0.17934959 -0.53843313] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3790, 0.2942],\n",
      "        [0.3666, 0.2877]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3790, 0.3666]) - torch.Size([2])\n",
      "P2 tensor([0.3786, 0.3663]) - torch.Size([2])\n",
      "P3 tensor([0.3786, 0.3663]) - torch.Size([2])\n",
      "P4 tensor([1.3786, 1.3663]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0599, -0.1679, -0.1754, -0.1959],\n",
      "        [ 0.0629, -0.1726, -0.1650, -0.0919]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0565,  0.0292, -0.1793, -0.5384],\n",
      "        [ 0.0594,  0.0245, -0.1668, -0.4318]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3786, 1.3663]) - torch.Size([2])\n",
      "Q-values tensor([[0.4193, 0.4410],\n",
      "        [0.4023, 0.4251]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4410],\n",
      "        [0.4251]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0565,  0.0292, -0.1793, -0.5384]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05712484 -0.16301197 -0.19011824 -0.30719292] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3490, 0.2790],\n",
      "        [0.3351, 0.2722]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3490, 0.3351]) - torch.Size([2])\n",
      "P2 tensor([0.3486, 0.3348]) - torch.Size([2])\n",
      "P3 tensor([0.3486, 0.3348]) - torch.Size([2])\n",
      "P4 tensor([1.3486, 1.3348]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0565,  0.0292, -0.1793, -0.5384],\n",
      "        [ 0.0594,  0.0245, -0.1668, -0.4318]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0571, -0.1630, -0.1901, -0.3072],\n",
      "        [ 0.0599, -0.1679, -0.1754, -0.1959]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3486, 1.3348]) - torch.Size([2])\n",
      "Q-values tensor([[0.4940, 0.5026],\n",
      "        [0.4732, 0.4859]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4940],\n",
      "        [0.4732]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0571, -0.1630, -0.1901, -0.3072]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.0538646  0.0342385 -0.1962621 -0.6532962] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3943, 0.3001],\n",
      "        [0.3490, 0.2790]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3943, 0.3490]) - torch.Size([2])\n",
      "P2 tensor([0.3939, 0.3486]) - torch.Size([2])\n",
      "P3 tensor([0.3939, 0.3486]) - torch.Size([2])\n",
      "P4 tensor([1.3939, 1.3486]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0571, -0.1630, -0.1901, -0.3072],\n",
      "        [ 0.0565,  0.0292, -0.1793, -0.5384]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0539,  0.0342, -0.1963, -0.6533],\n",
      "        [ 0.0571, -0.1630, -0.1901, -0.3072]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3939, 1.3486]) - torch.Size([2])\n",
      "Q-values tensor([[0.4494, 0.4662],\n",
      "        [0.5025, 0.5024]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4662],\n",
      "        [0.5025]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0539,  0.0342, -0.1963, -0.6533]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05454937  0.23147342 -0.20932803 -1.0008003 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4457, 0.3351],\n",
      "        [0.3490, 0.2790]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4457, 0.3490]) - torch.Size([2])\n",
      "P2 tensor([0.4453, 0.3486]) - torch.Size([2])\n",
      "P3 tensor([0.4453, 0.3486]) - torch.Size([2])\n",
      "P4 tensor([1.4453, 1.3486]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0539,  0.0342, -0.1963, -0.6533],\n",
      "        [ 0.0565,  0.0292, -0.1793, -0.5384]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0545,  0.2315, -0.2093, -1.0008],\n",
      "        [ 0.0571, -0.1630, -0.1901, -0.3072]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4453, 1.3486]) - torch.Size([2])\n",
      "Q-values tensor([[0.5333, 0.5245],\n",
      "        [0.5083, 0.5082]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5245],\n",
      "        [0.5083]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0545,  0.2315, -0.2093, -1.0008]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.05917884  0.03966958 -0.22934403 -0.7804688 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3943, 0.3001],\n",
      "        [0.4457, 0.3351]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3943, 0.4457]) - torch.Size([2])\n",
      "P2 tensor([0.3939, 0.4453]) - torch.Size([2])\n",
      "P3 tensor([0.3939, 0.4453]) - torch.Size([2])\n",
      "P4 tensor([1.3939, 1.4453]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0571, -0.1630, -0.1901, -0.3072],\n",
      "        [ 0.0539,  0.0342, -0.1963, -0.6533]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0539,  0.0342, -0.1963, -0.6533],\n",
      "        [ 0.0545,  0.2315, -0.2093, -1.0008]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3939, 1.4453]) - torch.Size([2])\n",
      "Q-values tensor([[0.4587, 0.4780],\n",
      "        [0.5380, 0.5319]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4780],\n",
      "        [0.5319]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0386, 0.0397, 0.0233, 0.0256]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.03944116 -0.15572792  0.02382827  0.32555217] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2813, 0.2652],\n",
      "        [0.4457, 0.3351]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2813, 0.4457]) - torch.Size([2])\n",
      "P2 tensor([0.2810, 0.4453]) - torch.Size([2])\n",
      "P3 tensor([0.2810, 0.4453]) - torch.Size([2])\n",
      "P4 tensor([1.2810, 1.4453]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0386,  0.0397,  0.0233,  0.0256],\n",
      "        [ 0.0539,  0.0342, -0.1963, -0.6533]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0394, -0.1557,  0.0238,  0.3256],\n",
      "        [ 0.0545,  0.2315, -0.2093, -1.0008]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2810, 1.4453]) - torch.Size([2])\n",
      "Q-values tensor([[0.4104, 0.4247],\n",
      "        [0.5375, 0.5409]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4104],\n",
      "        [0.5409]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0394, -0.1557,  0.0238,  0.3256]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.03632661 0.03904681 0.03033932 0.04047787] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2813, 0.2652],\n",
      "        [0.3164, 0.2589]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2813, 0.3164]) - torch.Size([2])\n",
      "P2 tensor([0.2810, 0.3160]) - torch.Size([2])\n",
      "P3 tensor([0.2810, 0.3160]) - torch.Size([2])\n",
      "P4 tensor([1.2810, 1.3160]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0386,  0.0397,  0.0233,  0.0256],\n",
      "        [ 0.0394, -0.1557,  0.0238,  0.3256]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0394, -0.1557,  0.0238,  0.3256],\n",
      "        [ 0.0363,  0.0390,  0.0303,  0.0405]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2810, 1.3160]) - torch.Size([2])\n",
      "Q-values tensor([[0.4143, 0.4290],\n",
      "        [0.3678, 0.4074]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4143],\n",
      "        [0.4074]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0363, 0.0390, 0.0303, 0.0405]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.03710754  0.23372085  0.03114887 -0.24248043] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3164, 0.2589],\n",
      "        [0.3517, 0.2737]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3164, 0.3517]) - torch.Size([2])\n",
      "P2 tensor([0.3160, 0.3514]) - torch.Size([2])\n",
      "P3 tensor([0.3160, 0.3514]) - torch.Size([2])\n",
      "P4 tensor([1.3160, 1.3514]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0394, -0.1557,  0.0238,  0.3256],\n",
      "        [ 0.0363,  0.0390,  0.0303,  0.0405]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0363,  0.0390,  0.0303,  0.0405],\n",
      "        [ 0.0371,  0.2337,  0.0311, -0.2425]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3160, 1.3514]) - torch.Size([2])\n",
      "Q-values tensor([[0.3719, 0.4118],\n",
      "        [0.4170, 0.4311]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4118],\n",
      "        [0.4311]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0371,  0.2337,  0.0311, -0.2425]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.04178196 0.03816814 0.02629927 0.05986265] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3133, 0.2584],\n",
      "        [0.3164, 0.2589]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3133, 0.3164]) - torch.Size([2])\n",
      "P2 tensor([0.3130, 0.3160]) - torch.Size([2])\n",
      "P3 tensor([0.3130, 0.3160]) - torch.Size([2])\n",
      "P4 tensor([1.3130, 1.3160]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0371,  0.2337,  0.0311, -0.2425],\n",
      "        [ 0.0394, -0.1557,  0.0238,  0.3256]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[0.0418, 0.0382, 0.0263, 0.0599],\n",
      "        [0.0363, 0.0390, 0.0303, 0.0405]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3130, 1.3160]) - torch.Size([2])\n",
      "Q-values tensor([[0.4732, 0.4840],\n",
      "        [0.3722, 0.4176]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4732],\n",
      "        [0.4176]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0418, 0.0382, 0.0263, 0.0599]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04254532  0.23290333  0.02749652 -0.22440805] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3495, 0.2724],\n",
      "        [0.3133, 0.2584]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3495, 0.3133]) - torch.Size([2])\n",
      "P2 tensor([0.3492, 0.3130]) - torch.Size([2])\n",
      "P3 tensor([0.3492, 0.3130]) - torch.Size([2])\n",
      "P4 tensor([1.3492, 1.3130]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0418,  0.0382,  0.0263,  0.0599],\n",
      "        [ 0.0371,  0.2337,  0.0311, -0.2425]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0425,  0.2329,  0.0275, -0.2244],\n",
      "        [ 0.0418,  0.0382,  0.0263,  0.0599]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3492, 1.3130]) - torch.Size([2])\n",
      "Q-values tensor([[0.4180, 0.4379],\n",
      "        [0.4791, 0.4870]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4379],\n",
      "        [0.4791]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0425,  0.2329,  0.0275, -0.2244]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.04720339 0.0373994  0.02300836 0.07681996] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3133, 0.2584],\n",
      "        [0.3495, 0.2724]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3133, 0.3495]) - torch.Size([2])\n",
      "P2 tensor([0.3130, 0.3492]) - torch.Size([2])\n",
      "P3 tensor([0.3130, 0.3492]) - torch.Size([2])\n",
      "P4 tensor([1.3130, 1.3492]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0371,  0.2337,  0.0311, -0.2425],\n",
      "        [ 0.0418,  0.0382,  0.0263,  0.0599]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0418,  0.0382,  0.0263,  0.0599],\n",
      "        [ 0.0425,  0.2329,  0.0275, -0.2244]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3130, 1.3492]) - torch.Size([2])\n",
      "Q-values tensor([[0.4844, 0.4912],\n",
      "        [0.4220, 0.4424]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4844],\n",
      "        [0.4424]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0472, 0.0374, 0.0230, 0.0768]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04795137 -0.15804471  0.02454476  0.3766724 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3495, 0.2724],\n",
      "        [0.3105, 0.2580]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3495, 0.3105]) - torch.Size([2])\n",
      "P2 tensor([0.3492, 0.3102]) - torch.Size([2])\n",
      "P3 tensor([0.3492, 0.3102]) - torch.Size([2])\n",
      "P4 tensor([1.3492, 1.3102]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0418,  0.0382,  0.0263,  0.0599],\n",
      "        [ 0.0425,  0.2329,  0.0275, -0.2244]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0425,  0.2329,  0.0275, -0.2244],\n",
      "        [ 0.0472,  0.0374,  0.0230,  0.0768]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3492, 1.3102]) - torch.Size([2])\n",
      "Q-values tensor([[0.4261, 0.4468],\n",
      "        [0.4859, 0.4920]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4468],\n",
      "        [0.4859]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0480, -0.1580,  0.0245,  0.3767]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.04479048 0.03672019 0.0320782  0.09182847] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.2760, 0.2633],\n",
      "        [0.3090, 0.2582]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.2760, 0.3090]) - torch.Size([2])\n",
      "P2 tensor([0.2757, 0.3087]) - torch.Size([2])\n",
      "P3 tensor([0.2757, 0.3087]) - torch.Size([2])\n",
      "P4 tensor([1.2757, 1.3087]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0472,  0.0374,  0.0230,  0.0768],\n",
      "        [ 0.0480, -0.1580,  0.0245,  0.3767]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0480, -0.1580,  0.0245,  0.3767],\n",
      "        [ 0.0448,  0.0367,  0.0321,  0.0918]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.2757, 1.3087]) - torch.Size([2])\n",
      "Q-values tensor([[0.4270, 0.4492],\n",
      "        [0.3805, 0.4293]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4270],\n",
      "        [0.4293]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0448, 0.0367, 0.0321, 0.0918]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04552488  0.23136802  0.03391477 -0.1905637 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3090, 0.2582],\n",
      "        [0.2760, 0.2633]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3090, 0.2760]) - torch.Size([2])\n",
      "P2 tensor([0.3087, 0.2757]) - torch.Size([2])\n",
      "P3 tensor([0.3087, 0.2757]) - torch.Size([2])\n",
      "P4 tensor([1.3087, 1.2757]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0480, -0.1580,  0.0245,  0.3767],\n",
      "        [ 0.0472,  0.0374,  0.0230,  0.0768]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0448,  0.0367,  0.0321,  0.0918],\n",
      "        [ 0.0480, -0.1580,  0.0245,  0.3767]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3087, 1.2757]) - torch.Size([2])\n",
      "Q-values tensor([[0.3847, 0.4339],\n",
      "        [0.4319, 0.4526]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4339],\n",
      "        [0.4319]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0455,  0.2314,  0.0339, -0.1906]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05015225  0.4259888   0.0301035  -0.47235793] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3736, 0.2932],\n",
      "        [0.3090, 0.2582]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.3736, 0.3090]) - torch.Size([2])\n",
      "P2 tensor([0.3732, 0.3087]) - torch.Size([2])\n",
      "P3 tensor([0.3732, 0.3087]) - torch.Size([2])\n",
      "P4 tensor([1.3732, 1.3087]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0455,  0.2314,  0.0339, -0.1906],\n",
      "        [ 0.0480, -0.1580,  0.0245,  0.3767]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0502,  0.4260,  0.0301, -0.4724],\n",
      "        [ 0.0448,  0.0367,  0.0321,  0.0918]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.3732, 1.3087]) - torch.Size([2])\n",
      "Q-values tensor([[0.4961, 0.4962],\n",
      "        [0.3888, 0.4384]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4962],\n",
      "        [0.4384]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0502,  0.4260,  0.0301, -0.4724]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05867202  0.23045488  0.02065634 -0.17034088] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5534, 0.5554],\n",
      "        [0.4910, 0.4995]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5554, 0.4995]) - torch.Size([2])\n",
      "P2 tensor([0.5548, 0.4990]) - torch.Size([2])\n",
      "P3 tensor([0.5548, 0.4990]) - torch.Size([2])\n",
      "P4 tensor([1.5548, 1.4990]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0455,  0.2314,  0.0339, -0.1906],\n",
      "        [ 0.0502,  0.4260,  0.0301, -0.4724]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0502,  0.4260,  0.0301, -0.4724],\n",
      "        [ 0.0587,  0.2305,  0.0207, -0.1703]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5548, 1.4990]) - torch.Size([2])\n",
      "Q-values tensor([[0.4957, 0.5030],\n",
      "        [0.5534, 0.5554]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5030],\n",
      "        [0.5534]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0587,  0.2305,  0.0207, -0.1703]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.06328112 0.03504346 0.01724952 0.12878627] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5534, 0.5554],\n",
      "        [0.4910, 0.4995]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5554, 0.4995]) - torch.Size([2])\n",
      "P2 tensor([0.5548, 0.4990]) - torch.Size([2])\n",
      "P3 tensor([0.5548, 0.4990]) - torch.Size([2])\n",
      "P4 tensor([1.5548, 1.4990]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0455,  0.2314,  0.0339, -0.1906],\n",
      "        [ 0.0502,  0.4260,  0.0301, -0.4724]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0502,  0.4260,  0.0301, -0.4724],\n",
      "        [ 0.0587,  0.2305,  0.0207, -0.1703]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5548, 1.4990]) - torch.Size([2])\n",
      "Q-values tensor([[0.5000, 0.5081],\n",
      "        [0.5595, 0.5609]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5081],\n",
      "        [0.5595]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0633, 0.0350, 0.0172, 0.1288]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06398199  0.22991411  0.01982525 -0.15840513] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4910, 0.4995],\n",
      "        [0.4886, 0.4975]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4995, 0.4975]) - torch.Size([2])\n",
      "P2 tensor([0.4990, 0.4970]) - torch.Size([2])\n",
      "P3 tensor([0.4990, 0.4970]) - torch.Size([2])\n",
      "P4 tensor([1.4990, 1.4970]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0502,  0.4260,  0.0301, -0.4724],\n",
      "        [ 0.0633,  0.0350,  0.0172,  0.1288]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0587,  0.2305,  0.0207, -0.1703],\n",
      "        [ 0.0640,  0.2299,  0.0198, -0.1584]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4990, 1.4970]) - torch.Size([2])\n",
      "Q-values tensor([[0.5655, 0.5664],\n",
      "        [0.4342, 0.4646]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5655],\n",
      "        [0.4646]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0640,  0.2299,  0.0198, -0.1584]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.06858027 0.03451402 0.01665715 0.14046566] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4270, 0.4557],\n",
      "        [0.4886, 0.4975]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4557, 0.4975]) - torch.Size([2])\n",
      "P2 tensor([0.4553, 0.4970]) - torch.Size([2])\n",
      "P3 tensor([0.4553, 0.4970]) - torch.Size([2])\n",
      "P4 tensor([1.4553, 1.4970]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0587,  0.2305,  0.0207, -0.1703],\n",
      "        [ 0.0633,  0.0350,  0.0172,  0.1288]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0633,  0.0350,  0.0172,  0.1288],\n",
      "        [ 0.0640,  0.2299,  0.0198, -0.1584]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4553, 1.4970]) - torch.Size([2])\n",
      "Q-values tensor([[0.5050, 0.5132],\n",
      "        [0.4383, 0.4688]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5050],\n",
      "        [0.4688]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0686, 0.0345, 0.0167, 0.1405]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06927055  0.22939348  0.01946646 -0.14691602] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4886, 0.4975],\n",
      "        [0.4249, 0.4544]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4975, 0.4544]) - torch.Size([2])\n",
      "P2 tensor([0.4970, 0.4539]) - torch.Size([2])\n",
      "P3 tensor([0.4970, 0.4539]) - torch.Size([2])\n",
      "P4 tensor([1.4970, 1.4539]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0633,  0.0350,  0.0172,  0.1288],\n",
      "        [ 0.0640,  0.2299,  0.0198, -0.1584]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0640,  0.2299,  0.0198, -0.1584],\n",
      "        [ 0.0686,  0.0345,  0.0167,  0.1405]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4970, 1.4539]) - torch.Size([2])\n",
      "Q-values tensor([[0.4427, 0.4729],\n",
      "        [0.5078, 0.5149]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4729],\n",
      "        [0.5078]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0693,  0.2294,  0.0195, -0.1469]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.07385842  0.42423135  0.01652814 -0.43339464] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5448, 0.5488],\n",
      "        [0.4249, 0.4544]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5488, 0.4544]) - torch.Size([2])\n",
      "P2 tensor([0.5483, 0.4539]) - torch.Size([2])\n",
      "P3 tensor([0.5483, 0.4539]) - torch.Size([2])\n",
      "P4 tensor([1.5483, 1.4539]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0693,  0.2294,  0.0195, -0.1469],\n",
      "        [ 0.0640,  0.2299,  0.0198, -0.1584]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0739,  0.4242,  0.0165, -0.4334],\n",
      "        [ 0.0686,  0.0345,  0.0167,  0.1405]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5483, 1.4539]) - torch.Size([2])\n",
      "Q-values tensor([[0.5105, 0.5170],\n",
      "        [0.5130, 0.5189]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5170],\n",
      "        [0.5130]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0739,  0.4242,  0.0165, -0.4334]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.08234305  0.22887933  0.00786025 -0.1355476 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5448, 0.5488],\n",
      "        [0.4832, 0.4936]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5488, 0.4936]) - torch.Size([2])\n",
      "P2 tensor([0.5483, 0.4931]) - torch.Size([2])\n",
      "P3 tensor([0.5483, 0.4931]) - torch.Size([2])\n",
      "P4 tensor([1.5483, 1.4931]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0693,  0.2294,  0.0195, -0.1469],\n",
      "        [ 0.0739,  0.4242,  0.0165, -0.4334]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0739,  0.4242,  0.0165, -0.4334],\n",
      "        [ 0.0823,  0.2289,  0.0079, -0.1355]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5483, 1.4931]) - torch.Size([2])\n",
      "Q-values tensor([[0.5149, 0.5222],\n",
      "        [0.5802, 0.5772]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5222],\n",
      "        [0.5802]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0823,  0.2289,  0.0079, -0.1355]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.08692063 0.03364568 0.00514929 0.15960471] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5448, 0.5488],\n",
      "        [0.4208, 0.4514]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5488, 0.4514]) - torch.Size([2])\n",
      "P2 tensor([0.5483, 0.4509]) - torch.Size([2])\n",
      "P3 tensor([0.5483, 0.4509]) - torch.Size([2])\n",
      "P4 tensor([1.5483, 1.4509]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0693,  0.2294,  0.0195, -0.1469],\n",
      "        [ 0.0823,  0.2289,  0.0079, -0.1355]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0739,  0.4242,  0.0165, -0.4334],\n",
      "        [ 0.0869,  0.0336,  0.0051,  0.1596]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5483, 1.4509]) - torch.Size([2])\n",
      "Q-values tensor([[0.5193, 0.5272],\n",
      "        [0.5161, 0.5252]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5272],\n",
      "        [0.5161]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0869, 0.0336, 0.0051, 0.1596]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.08759355 -0.16154961  0.00834139  0.45390767] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4832, 0.4936],\n",
      "        [0.4208, 0.4514]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4936, 0.4514]) - torch.Size([2])\n",
      "P2 tensor([0.4931, 0.4509]) - torch.Size([2])\n",
      "P3 tensor([0.4931, 0.4509]) - torch.Size([2])\n",
      "P4 tensor([1.4931, 1.4509]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0739,  0.4242,  0.0165, -0.4334],\n",
      "        [ 0.0823,  0.2289,  0.0079, -0.1355]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0823,  0.2289,  0.0079, -0.1355],\n",
      "        [ 0.0869,  0.0336,  0.0051,  0.1596]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4931, 1.4509]) - torch.Size([2])\n",
      "Q-values tensor([[0.5912, 0.5882],\n",
      "        [0.5205, 0.5304]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5912],\n",
      "        [0.5205]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0876, -0.1615,  0.0083,  0.4539]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.08436255 -0.35678852  0.01741954  0.74920815] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3583, 0.4339],\n",
      "        [0.3810, 0.4373]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4339, 0.4373]) - torch.Size([2])\n",
      "P2 tensor([0.4335, 0.4368]) - torch.Size([2])\n",
      "P3 tensor([0.4335, 0.4368]) - torch.Size([2])\n",
      "P4 tensor([1.4335, 1.4368]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0876, -0.1615,  0.0083,  0.4539],\n",
      "        [ 0.0869,  0.0336,  0.0051,  0.1596]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0844, -0.3568,  0.0174,  0.7492],\n",
      "        [ 0.0876, -0.1615,  0.0083,  0.4539]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4335, 1.4368]) - torch.Size([2])\n",
      "Q-values tensor([[0.4142, 0.4700],\n",
      "        [0.4578, 0.4863]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4142],\n",
      "        [0.4578]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0844, -0.3568,  0.0174,  0.7492]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.07722678 -0.5521464   0.0324037   1.0473216 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3810, 0.4373],\n",
      "        [0.3542, 0.4448]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4373, 0.4448]) - torch.Size([2])\n",
      "P2 tensor([0.4368, 0.4443]) - torch.Size([2])\n",
      "P3 tensor([0.4368, 0.4443]) - torch.Size([2])\n",
      "P4 tensor([1.4368, 1.4443]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0869,  0.0336,  0.0051,  0.1596],\n",
      "        [ 0.0844, -0.3568,  0.0174,  0.7492]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0876, -0.1615,  0.0083,  0.4539],\n",
      "        [ 0.0772, -0.5521,  0.0324,  1.0473]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4368, 1.4443]) - torch.Size([2])\n",
      "Q-values tensor([[0.4641, 0.4865],\n",
      "        [0.3958, 0.4669]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4641],\n",
      "        [0.3958]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0772, -0.5521,  0.0324,  1.0473]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06618386 -0.747683    0.05335014  1.3499976 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3542, 0.4448],\n",
      "        [0.3669, 0.4668]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4448, 0.4668]) - torch.Size([2])\n",
      "P2 tensor([0.4443, 0.4663]) - torch.Size([2])\n",
      "P3 tensor([0.4443, 0.4663]) - torch.Size([2])\n",
      "P4 tensor([1.4443, 1.4663]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0844, -0.3568,  0.0174,  0.7492],\n",
      "        [ 0.0772, -0.5521,  0.0324,  1.0473]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0772, -0.5521,  0.0324,  1.0473],\n",
      "        [ 0.0662, -0.7477,  0.0534,  1.3500]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4443, 1.4663]) - torch.Size([2])\n",
      "Q-values tensor([[0.4024, 0.4684],\n",
      "        [0.3982, 0.4801]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4024],\n",
      "        [0.3982]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0662, -0.7477,  0.0534,  1.3500]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.0512302  -0.9434331   0.08035009  1.6588825 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3669, 0.4668],\n",
      "        [0.3836, 0.4971]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4668, 0.4971]) - torch.Size([2])\n",
      "P2 tensor([0.4663, 0.4967]) - torch.Size([2])\n",
      "P3 tensor([0.4663, 0.4967]) - torch.Size([2])\n",
      "P4 tensor([1.4663, 1.4967]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0772, -0.5521,  0.0324,  1.0473],\n",
      "        [ 0.0662, -0.7477,  0.0534,  1.3500]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0662, -0.7477,  0.0534,  1.3500],\n",
      "        [ 0.0512, -0.9434,  0.0804,  1.6589]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4663, 1.4967]) - torch.Size([2])\n",
      "Q-values tensor([[0.4055, 0.4817],\n",
      "        [0.4224, 0.5061]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4055],\n",
      "        [0.4224]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0512, -0.9434,  0.0804,  1.6589]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.03236153 -0.7493345   0.11352774  1.3922715 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3677, 0.4733],\n",
      "        [0.3836, 0.4971]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4733, 0.4971]) - torch.Size([2])\n",
      "P2 tensor([0.4728, 0.4967]) - torch.Size([2])\n",
      "P3 tensor([0.4728, 0.4967]) - torch.Size([2])\n",
      "P4 tensor([1.4728, 1.4967]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0512, -0.9434,  0.0804,  1.6589],\n",
      "        [ 0.0662, -0.7477,  0.0534,  1.3500]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0324, -0.7493,  0.1135,  1.3923],\n",
      "        [ 0.0512, -0.9434,  0.0804,  1.6589]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4728, 1.4967]) - torch.Size([2])\n",
      "Q-values tensor([[0.4514, 0.5397],\n",
      "        [0.4312, 0.5079]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5397],\n",
      "        [0.4312]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0324, -0.7493,  0.1135,  1.3923]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01737484 -0.5557943   0.14137317  1.1371354 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3544, 0.4588],\n",
      "        [0.3677, 0.4733]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4588, 0.4733]) - torch.Size([2])\n",
      "P2 tensor([0.4583, 0.4728]) - torch.Size([2])\n",
      "P3 tensor([0.4583, 0.4728]) - torch.Size([2])\n",
      "P4 tensor([1.4583, 1.4728]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0324, -0.7493,  0.1135,  1.3923],\n",
      "        [ 0.0512, -0.9434,  0.0804,  1.6589]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0174, -0.5558,  0.1414,  1.1371],\n",
      "        [ 0.0324, -0.7493,  0.1135,  1.3923]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4583, 1.4728]) - torch.Size([2])\n",
      "Q-values tensor([[0.4396, 0.5213],\n",
      "        [0.4595, 0.5471]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5213],\n",
      "        [0.5471]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0174, -0.5558,  0.1414,  1.1371]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00625896 -0.36277556  0.16411588  0.8919212 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3677, 0.4733],\n",
      "        [0.3544, 0.4588]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4733, 0.4588]) - torch.Size([2])\n",
      "P2 tensor([0.4728, 0.4583]) - torch.Size([2])\n",
      "P3 tensor([0.4728, 0.4583]) - torch.Size([2])\n",
      "P4 tensor([1.4728, 1.4583]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0512, -0.9434,  0.0804,  1.6589],\n",
      "        [ 0.0324, -0.7493,  0.1135,  1.3923]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0324, -0.7493,  0.1135,  1.3923],\n",
      "        [ 0.0174, -0.5558,  0.1414,  1.1371]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4728, 1.4583]) - torch.Size([2])\n",
      "Q-values tensor([[0.4612, 0.5566],\n",
      "        [0.4411, 0.5299]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5566],\n",
      "        [0.5299]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0063, -0.3628,  0.1641,  0.8919]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00099655 -0.17021455  0.1819543   0.65499455] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3635, 0.4428],\n",
      "        [0.3514, 0.4483]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4428, 0.4483]) - torch.Size([2])\n",
      "P2 tensor([0.4424, 0.4478]) - torch.Size([2])\n",
      "P3 tensor([0.4424, 0.4478]) - torch.Size([2])\n",
      "P4 tensor([1.4424, 1.4478]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0063, -0.3628,  0.1641,  0.8919],\n",
      "        [ 0.0174, -0.5558,  0.1414,  1.1371]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0010, -0.1702,  0.1820,  0.6550],\n",
      "        [ 0.0063, -0.3628,  0.1641,  0.8919]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4424, 1.4478]) - torch.Size([2])\n",
      "Q-values tensor([[0.4165, 0.5051],\n",
      "        [0.4239, 0.5190]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5051],\n",
      "        [0.5190]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0010, -0.1702,  0.1820,  0.6550]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00440084  0.02197042  0.19505419  0.4246759 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3635, 0.4428],\n",
      "        [0.3969, 0.4491]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4428, 0.4491]) - torch.Size([2])\n",
      "P2 tensor([0.4424, 0.4486]) - torch.Size([2])\n",
      "P3 tensor([0.4424, 0.4486]) - torch.Size([2])\n",
      "P4 tensor([1.4424, 1.4486]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0063, -0.3628,  0.1641,  0.8919],\n",
      "        [-0.0010, -0.1702,  0.1820,  0.6550]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0010, -0.1702,  0.1820,  0.6550],\n",
      "        [-0.0044,  0.0220,  0.1951,  0.4247]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4424, 1.4486]) - torch.Size([2])\n",
      "Q-values tensor([[0.4177, 0.5116],\n",
      "        [0.4285, 0.5024]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5116],\n",
      "        [0.5024]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0044,  0.0220,  0.1951,  0.4247]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00396144  0.21387205  0.20354772  0.19926497] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3969, 0.4491],\n",
      "        [0.3635, 0.4428]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4491, 0.4428]) - torch.Size([2])\n",
      "P2 tensor([0.4486, 0.4424]) - torch.Size([2])\n",
      "P3 tensor([0.4486, 0.4424]) - torch.Size([2])\n",
      "P4 tensor([1.4486, 1.4424]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0010, -0.1702,  0.1820,  0.6550],\n",
      "        [ 0.0063, -0.3628,  0.1641,  0.8919]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0044,  0.0220,  0.1951,  0.4247],\n",
      "        [-0.0010, -0.1702,  0.1820,  0.6550]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4486, 1.4424]) - torch.Size([2])\n",
      "Q-values tensor([[0.4294, 0.5087],\n",
      "        [0.4189, 0.5182]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5087],\n",
      "        [0.5182]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0040,  0.2139,  0.2035,  0.1993]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 3.1600505e-04  4.0558940e-01  2.0753300e-01 -2.2943571e-02] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4406, 0.4626],\n",
      "        [0.3969, 0.4491]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4626, 0.4491]) - torch.Size([2])\n",
      "P2 tensor([0.4622, 0.4486]) - torch.Size([2])\n",
      "P3 tensor([0.4622, 0.4486]) - torch.Size([2])\n",
      "P4 tensor([1.4622, 1.4486]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0044,  0.0220,  0.1951,  0.4247],\n",
      "        [-0.0010, -0.1702,  0.1820,  0.6550]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0040,  0.2139,  0.2035,  0.1993],\n",
      "        [-0.0044,  0.0220,  0.1951,  0.4247]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4622, 1.4486]) - torch.Size([2])\n",
      "Q-values tensor([[0.4635, 0.5169],\n",
      "        [0.4303, 0.5150]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5169],\n",
      "        [0.5150]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 3.1601e-04,  4.0559e-01,  2.0753e-01, -2.2944e-02]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00842779  0.59722453  0.20707414 -0.24365053] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4406, 0.4626],\n",
      "        [0.4851, 0.4963]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4626, 0.4963]) - torch.Size([2])\n",
      "P2 tensor([0.4622, 0.4958]) - torch.Size([2])\n",
      "P3 tensor([0.4622, 0.4958]) - torch.Size([2])\n",
      "P4 tensor([1.4622, 1.4958]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0044,  0.0220,  0.1951,  0.4247],\n",
      "        [-0.0040,  0.2139,  0.2035,  0.1993]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-3.9614e-03,  2.1387e-01,  2.0355e-01,  1.9926e-01],\n",
      "        [ 3.1601e-04,  4.0559e-01,  2.0753e-01, -2.2944e-02]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4622, 1.4958]) - torch.Size([2])\n",
      "Q-values tensor([[0.4640, 0.5230],\n",
      "        [0.5108, 0.5347]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5230],\n",
      "        [0.5347]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0084,  0.5972,  0.2071, -0.2437]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.02037228 0.39983943 0.20220113 0.10654759] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4631, 0.4823],\n",
      "        [0.4851, 0.4963]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4823, 0.4963]) - torch.Size([2])\n",
      "P2 tensor([0.4818, 0.4958]) - torch.Size([2])\n",
      "P3 tensor([0.4818, 0.4958]) - torch.Size([2])\n",
      "P4 tensor([1.4818, 1.4958]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0084,  0.5972,  0.2071, -0.2437],\n",
      "        [-0.0040,  0.2139,  0.2035,  0.1993]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 2.0372e-02,  3.9984e-01,  2.0220e-01,  1.0655e-01],\n",
      "        [ 3.1601e-04,  4.0559e-01,  2.0753e-01, -2.2944e-02]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4818, 1.4958]) - torch.Size([2])\n",
      "Q-values tensor([[0.6102, 0.6143],\n",
      "        [0.5110, 0.5412]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6102],\n",
      "        [0.5412]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0204, 0.3998, 0.2022, 0.1065]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [0.02836907 0.20248064 0.20433208 0.4555974 ] -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [00:01<03:30,  4.70 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "P0 tensor([[0.4631, 0.4823],\n",
      "        [0.4066, 0.4455]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4823, 0.4455]) - torch.Size([2])\n",
      "P2 tensor([0.4818, 0.4451]) - torch.Size([2])\n",
      "P3 tensor([0.4818, 0.4451]) - torch.Size([2])\n",
      "P4 tensor([1.4818, 1.4451]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0084,  0.5972,  0.2071, -0.2437],\n",
      "        [ 0.0204,  0.3998,  0.2022,  0.1065]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[0.0204, 0.3998, 0.2022, 0.1065],\n",
      "        [0.0284, 0.2025, 0.2043, 0.4556]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4818, 1.4451]) - torch.Size([2])\n",
      "Q-values tensor([[0.6168, 0.6187],\n",
      "        [0.5420, 0.5634]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6168],\n",
      "        [0.5420]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0284, 0.2025, 0.2043, 0.4556]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [0.03241868 0.00514477 0.21344402 0.80509984] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4066, 0.4455],\n",
      "        [0.4631, 0.4823]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4455, 0.4823]) - torch.Size([2])\n",
      "P2 tensor([0.4451, 0.4818]) - torch.Size([2])\n",
      "P3 tensor([0.4451, 0.4818]) - torch.Size([2])\n",
      "P4 tensor([1.4451, 1.4818]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0204,  0.3998,  0.2022,  0.1065],\n",
      "        [ 0.0084,  0.5972,  0.2071, -0.2437]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[0.0284, 0.2025, 0.2043, 0.4556],\n",
      "        [0.0204, 0.3998, 0.2022, 0.1065]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4451, 1.4818]) - torch.Size([2])\n",
      "Q-values tensor([[0.5488, 0.5637],\n",
      "        [0.6248, 0.6193]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5488],\n",
      "        [0.6248]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0199,  0.0226, -0.0135, -0.0198]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01940899  0.21794318 -0.01393641 -0.31675342] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.3651, 0.4342],\n",
      "        [0.5199, 0.5246]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4342, 0.5246]) - torch.Size([2])\n",
      "P2 tensor([0.4338, 0.5241]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.5241]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.5241]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0284,  0.2025,  0.2043,  0.4556],\n",
      "        [-0.0199,  0.0226, -0.0135, -0.0198]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0324,  0.0051,  0.2134,  0.8051],\n",
      "        [-0.0194,  0.2179, -0.0139, -0.3168]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.5241]) - torch.Size([2])\n",
      "Q-values tensor([[0.4920, 0.5301],\n",
      "        [0.5417, 0.5528]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.4920],\n",
      "        [0.5528]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0194,  0.2179, -0.0139, -0.3168]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01505013  0.41326085 -0.02027148 -0.6137986 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5871, 0.5820],\n",
      "        [0.3651, 0.4342]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5871, 0.4342]) - torch.Size([2])\n",
      "P2 tensor([0.5865, 0.4338]) - torch.Size([2])\n",
      "P3 tensor([0.5865, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.5865, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0194,  0.2179, -0.0139, -0.3168],\n",
      "        [ 0.0284,  0.2025,  0.2043,  0.4556]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0151,  0.4133, -0.0203, -0.6138],\n",
      "        [ 0.0324,  0.0051,  0.2134,  0.8051]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5865, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.6184, 0.6137],\n",
      "        [0.4961, 0.5352]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6137],\n",
      "        [0.4961]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0151,  0.4133, -0.0203, -0.6138]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00678491  0.6086601  -0.03254745 -0.9127967 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6683, 0.6499],\n",
      "        [0.5871, 0.5820]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6683, 0.5871]) - torch.Size([2])\n",
      "P2 tensor([0.6676, 0.5865]) - torch.Size([2])\n",
      "P3 tensor([0.6676, 0.5865]) - torch.Size([2])\n",
      "P4 tensor([1.6676, 1.5865]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0151,  0.4133, -0.0203, -0.6138],\n",
      "        [-0.0194,  0.2179, -0.0139, -0.3168]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0068,  0.6087, -0.0325, -0.9128],\n",
      "        [-0.0151,  0.4133, -0.0203, -0.6138]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6676, 1.5865]) - torch.Size([2])\n",
      "Q-values tensor([[0.7048, 0.6833],\n",
      "        [0.6213, 0.6208]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6833],\n",
      "        [0.6208]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0068,  0.6087, -0.0325, -0.9128]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00538829  0.8042069  -0.05080338 -1.2155288 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6683, 0.6499],\n",
      "        [0.7496, 0.7258]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6683, 0.7496]) - torch.Size([2])\n",
      "P2 tensor([0.6676, 0.7489]) - torch.Size([2])\n",
      "P3 tensor([0.6676, 0.7489]) - torch.Size([2])\n",
      "P4 tensor([1.6676, 1.7489]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0151,  0.4133, -0.0203, -0.6138],\n",
      "        [-0.0068,  0.6087, -0.0325, -0.9128]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0068,  0.6087, -0.0325, -0.9128],\n",
      "        [ 0.0054,  0.8042, -0.0508, -1.2155]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6676, 1.7489]) - torch.Size([2])\n",
      "Q-values tensor([[0.7052, 0.6917],\n",
      "        [0.8021, 0.7694]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6917],\n",
      "        [0.7694]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0054,  0.8042, -0.0508, -1.2155]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.02147243  0.99994606 -0.07511396 -1.5236884 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7496, 0.7258],\n",
      "        [0.6683, 0.6499]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7496, 0.6683]) - torch.Size([2])\n",
      "P2 tensor([0.7489, 0.6676]) - torch.Size([2])\n",
      "P3 tensor([0.7489, 0.6676]) - torch.Size([2])\n",
      "P4 tensor([1.7489, 1.6676]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0068,  0.6087, -0.0325, -0.9128],\n",
      "        [-0.0151,  0.4133, -0.0203, -0.6138]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0054,  0.8042, -0.0508, -1.2155],\n",
      "        [-0.0068,  0.6087, -0.0325, -0.9128]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7489, 1.6676]) - torch.Size([2])\n",
      "Q-values tensor([[0.8028, 0.7799],\n",
      "        [0.7056, 0.7002]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7799],\n",
      "        [0.7002]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0215,  0.9999, -0.0751, -1.5237]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04147135  0.80580735 -0.10558773 -1.2553643 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8472, 0.8155],\n",
      "        [0.7566, 0.7390]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8472, 0.7566]) - torch.Size([2])\n",
      "P2 tensor([0.8463, 0.7558]) - torch.Size([2])\n",
      "P3 tensor([0.8463, 0.7558]) - torch.Size([2])\n",
      "P4 tensor([1.8463, 1.7558]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0054,  0.8042, -0.0508, -1.2155],\n",
      "        [ 0.0215,  0.9999, -0.0751, -1.5237]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0215,  0.9999, -0.0751, -1.5237],\n",
      "        [ 0.0415,  0.8058, -0.1056, -1.2554]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8463, 1.7558]) - torch.Size([2])\n",
      "Q-values tensor([[0.9024, 0.8798],\n",
      "        [1.0181, 0.9863]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8798],\n",
      "        [1.0181]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0415,  0.8058, -0.1056, -1.2554]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.0575875   1.0021107  -0.13069502 -1.5791646 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7566, 0.7390],\n",
      "        [0.8599, 0.8353]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7566, 0.8599]) - torch.Size([2])\n",
      "P2 tensor([0.7558, 0.8590]) - torch.Size([2])\n",
      "P3 tensor([0.7558, 0.8590]) - torch.Size([2])\n",
      "P4 tensor([1.7558, 1.8590]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0215,  0.9999, -0.0751, -1.5237],\n",
      "        [ 0.0415,  0.8058, -0.1056, -1.2554]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0415,  0.8058, -0.1056, -1.2554],\n",
      "        [ 0.0576,  1.0021, -0.1307, -1.5792]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7558, 1.8590]) - torch.Size([2])\n",
      "Q-values tensor([[1.0291, 0.9978],\n",
      "        [0.9201, 0.9046]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0291],\n",
      "        [0.9046]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0576,  1.0021, -0.1307, -1.5792]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.07762972  0.8087651  -0.16227831 -1.3299346 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8599, 0.8353],\n",
      "        [0.7566, 0.7390]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8599, 0.7566]) - torch.Size([2])\n",
      "P2 tensor([0.8590, 0.7558]) - torch.Size([2])\n",
      "P3 tensor([0.8590, 0.7558]) - torch.Size([2])\n",
      "P4 tensor([1.8590, 1.7558]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0415,  0.8058, -0.1056, -1.2554],\n",
      "        [ 0.0215,  0.9999, -0.0751, -1.5237]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0576,  1.0021, -0.1307, -1.5792],\n",
      "        [ 0.0415,  0.8058, -0.1056, -1.2554]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8590, 1.7558]) - torch.Size([2])\n",
      "Q-values tensor([[0.9293, 0.9148],\n",
      "        [1.0400, 1.0096]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9148],\n",
      "        [1.0400]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0776,  0.8088, -0.1623, -1.3299]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.09380502  1.0055192  -0.188877   -1.6686888 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8599, 0.8353],\n",
      "        [0.8817, 0.8647]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8599, 0.8817]) - torch.Size([2])\n",
      "P2 tensor([0.8590, 0.8808]) - torch.Size([2])\n",
      "P3 tensor([0.8590, 0.8808]) - torch.Size([2])\n",
      "P4 tensor([1.8590, 1.8808]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0415,  0.8058, -0.1056, -1.2554],\n",
      "        [ 0.0776,  0.8088, -0.1623, -1.3299]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0576,  1.0021, -0.1307, -1.5792],\n",
      "        [ 0.0938,  1.0055, -0.1889, -1.6687]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8590, 1.8808]) - torch.Size([2])\n",
      "Q-values tensor([[0.9386, 0.9251],\n",
      "        [0.9624, 0.9534]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9251],\n",
      "        [0.9534]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0938,  1.0055, -0.1889, -1.6687]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.1139154   1.2022684  -0.22225077 -2.0137644 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9894, 0.9731],\n",
      "        [0.7761, 0.7622]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9894, 0.7761]) - torch.Size([2])\n",
      "P2 tensor([0.9885, 0.7754]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.7754]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.7754]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0938,  1.0055, -0.1889, -1.6687],\n",
      "        [ 0.0576,  1.0021, -0.1307, -1.5792]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1139,  1.2023, -0.2223, -2.0138],\n",
      "        [ 0.0776,  0.8088, -0.1623, -1.3299]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.7754]) - torch.Size([2])\n",
      "Q-values tensor([[1.0946, 1.0972],\n",
      "        [1.0675, 1.0605]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0972],\n",
      "        [1.0675]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0324,  0.0028,  0.0217, -0.0284]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03236097  0.19760826  0.02109017 -0.31420276] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9894, 0.9731],\n",
      "        [0.8817, 0.8647]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9894, 0.8817]) - torch.Size([2])\n",
      "P2 tensor([0.9885, 0.8808]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.8808]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.8808]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0938,  1.0055, -0.1889, -1.6687],\n",
      "        [ 0.0776,  0.8088, -0.1623, -1.3299]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1139,  1.2023, -0.2223, -2.0138],\n",
      "        [ 0.0938,  1.0055, -0.1889, -1.6687]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.8808]) - torch.Size([2])\n",
      "Q-values tensor([[1.1113, 1.0933],\n",
      "        [0.9773, 0.9637]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0933],\n",
      "        [0.9637]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0324,  0.1976,  0.0211, -0.3142]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0284088   0.00219233  0.01480612 -0.01494398] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9894, 0.9731],\n",
      "        [0.4552, 0.4740]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9894, 0.4740]) - torch.Size([2])\n",
      "P2 tensor([0.9885, 0.4735]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.4735]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.4735]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0938,  1.0055, -0.1889, -1.6687],\n",
      "        [-0.0324,  0.1976,  0.0211, -0.3142]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1139,  1.2023, -0.2223, -2.0138],\n",
      "        [-0.0284,  0.0022,  0.0148, -0.0149]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.4735]) - torch.Size([2])\n",
      "Q-values tensor([[1.1122, 1.1093],\n",
      "        [0.6438, 0.6712]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1093],\n",
      "        [0.6438]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0284,  0.0022,  0.0148, -0.0149]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02836496  0.19709885  0.01450724 -0.30291888] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4552, 0.4740],\n",
      "        [0.5209, 0.5245]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.4740, 0.5245]) - torch.Size([2])\n",
      "P2 tensor([0.4735, 0.5240]) - torch.Size([2])\n",
      "P3 tensor([0.4735, 0.5240]) - torch.Size([2])\n",
      "P4 tensor([1.4735, 1.5240]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0324,  0.1976,  0.0211, -0.3142],\n",
      "        [-0.0324,  0.0028,  0.0217, -0.0284]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0284,  0.0022,  0.0148, -0.0149],\n",
      "        [-0.0324,  0.1976,  0.0211, -0.3142]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.4735, 1.5240]) - torch.Size([2])\n",
      "Q-values tensor([[0.6513, 0.6686],\n",
      "        [0.5753, 0.6108]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6513],\n",
      "        [0.6108]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0284,  0.1971,  0.0145, -0.3029]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02442298  0.39201108  0.00844886 -0.59099144] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5182, 0.5221],\n",
      "        [0.5832, 0.5781]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5221, 0.5832]) - torch.Size([2])\n",
      "P2 tensor([0.5216, 0.5827]) - torch.Size([2])\n",
      "P3 tensor([0.5216, 0.5827]) - torch.Size([2])\n",
      "P4 tensor([1.5216, 1.5827]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0284,  0.0022,  0.0148, -0.0149],\n",
      "        [-0.0284,  0.1971,  0.0145, -0.3029]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0284,  0.1971,  0.0145, -0.3029],\n",
      "        [-0.0244,  0.3920,  0.0084, -0.5910]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5216, 1.5827]) - torch.Size([2])\n",
      "Q-values tensor([[0.5759, 0.6129],\n",
      "        [0.6537, 0.6709]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6129],\n",
      "        [0.6709]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0244,  0.3920,  0.0084, -0.5910]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01658276  0.5870137  -0.00337097 -0.88100106] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5832, 0.5781],\n",
      "        [0.6615, 0.6424]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5832, 0.6615]) - torch.Size([2])\n",
      "P2 tensor([0.5827, 0.6608]) - torch.Size([2])\n",
      "P3 tensor([0.5827, 0.6608]) - torch.Size([2])\n",
      "P4 tensor([1.5827, 1.6608]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0284,  0.1971,  0.0145, -0.3029],\n",
      "        [-0.0244,  0.3920,  0.0084, -0.5910]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0244,  0.3920,  0.0084, -0.5910],\n",
      "        [-0.0166,  0.5870, -0.0034, -0.8810]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5827, 1.6608]) - torch.Size([2])\n",
      "Q-values tensor([[0.6539, 0.6785],\n",
      "        [0.7426, 0.7487]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6785],\n",
      "        [0.7487]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0166,  0.5870, -0.0034, -0.8810]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00484248  0.7821813  -0.02099099 -1.1747419 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6615, 0.6424],\n",
      "        [0.5832, 0.5781]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6615, 0.5832]) - torch.Size([2])\n",
      "P2 tensor([0.6608, 0.5827]) - torch.Size([2])\n",
      "P3 tensor([0.6608, 0.5827]) - torch.Size([2])\n",
      "P4 tensor([1.6608, 1.5827]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0244,  0.3920,  0.0084, -0.5910],\n",
      "        [-0.0284,  0.1971,  0.0145, -0.3029]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0166,  0.5870, -0.0034, -0.8810],\n",
      "        [-0.0244,  0.3920,  0.0084, -0.5910]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6608, 1.5827]) - torch.Size([2])\n",
      "Q-values tensor([[0.7430, 0.7574],\n",
      "        [0.6540, 0.6862]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7574],\n",
      "        [0.6862]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0048,  0.7822, -0.0210, -1.1747]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01080114  0.97756964 -0.04448583 -1.4739307 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6615, 0.6424],\n",
      "        [0.7424, 0.7153]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6615, 0.7424]) - torch.Size([2])\n",
      "P2 tensor([0.6608, 0.7417]) - torch.Size([2])\n",
      "P3 tensor([0.6608, 0.7417]) - torch.Size([2])\n",
      "P4 tensor([1.6608, 1.7417]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0244,  0.3920,  0.0084, -0.5910],\n",
      "        [-0.0166,  0.5870, -0.0034, -0.8810]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0166,  0.5870, -0.0034, -0.8810],\n",
      "        [-0.0048,  0.7822, -0.0210, -1.1747]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6608, 1.7417]) - torch.Size([2])\n",
      "Q-values tensor([[0.7435, 0.7661],\n",
      "        [0.8467, 0.8540]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7661],\n",
      "        [0.8540]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0108,  0.9776, -0.0445, -1.4739]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.03035253  0.78301877 -0.07396445 -1.1954676 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7424, 0.7153],\n",
      "        [0.7401, 0.7217]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7424, 0.7401]) - torch.Size([2])\n",
      "P2 tensor([0.7417, 0.7394]) - torch.Size([2])\n",
      "P3 tensor([0.7417, 0.7394]) - torch.Size([2])\n",
      "P4 tensor([1.7417, 1.7394]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0166,  0.5870, -0.0034, -0.8810],\n",
      "        [ 0.0108,  0.9776, -0.0445, -1.4739]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0048,  0.7822, -0.0210, -1.1747],\n",
      "        [ 0.0304,  0.7830, -0.0740, -1.1955]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7417, 1.7394]) - torch.Size([2])\n",
      "Q-values tensor([[0.8471, 0.8643],\n",
      "        [1.0746, 1.0767]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8643],\n",
      "        [1.0746]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0304,  0.7830, -0.0740, -1.1955]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04601291  0.5889282  -0.09787379 -0.9268535 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8327, 0.7996],\n",
      "        [0.6651, 0.6527]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8327, 0.6651]) - torch.Size([2])\n",
      "P2 tensor([0.8318, 0.6644]) - torch.Size([2])\n",
      "P3 tensor([0.8318, 0.6644]) - torch.Size([2])\n",
      "P4 tensor([1.8318, 1.6644]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0048,  0.7822, -0.0210, -1.1747],\n",
      "        [ 0.0304,  0.7830, -0.0740, -1.1955]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0108,  0.9776, -0.0445, -1.4739],\n",
      "        [ 0.0460,  0.5889, -0.0979, -0.9269]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8318, 1.6644]) - torch.Size([2])\n",
      "Q-values tensor([[0.9644, 0.9723],\n",
      "        [0.9655, 0.9796]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9723],\n",
      "        [0.9655]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0460,  0.5889, -0.0979, -0.9269]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05779147  0.39525437 -0.11641087 -0.66646206] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5941, 0.5916],\n",
      "        [0.7401, 0.7217]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5941, 0.7401]) - torch.Size([2])\n",
      "P2 tensor([0.5935, 0.7394]) - torch.Size([2])\n",
      "P3 tensor([0.5935, 0.7394]) - torch.Size([2])\n",
      "P4 tensor([1.5935, 1.7394]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0460,  0.5889, -0.0979, -0.9269],\n",
      "        [ 0.0108,  0.9776, -0.0445, -1.4739]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0578,  0.3953, -0.1164, -0.6665],\n",
      "        [ 0.0304,  0.7830, -0.0740, -1.1955]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5935, 1.7394]) - torch.Size([2])\n",
      "Q-values tensor([[0.8693, 0.8950],\n",
      "        [1.0961, 1.0984]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8693],\n",
      "        [1.0961]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0578,  0.3953, -0.1164, -0.6665]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06569656  0.2019274  -0.1297401  -0.4125818 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5941, 0.5916],\n",
      "        [0.6651, 0.6527]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5941, 0.6651]) - torch.Size([2])\n",
      "P2 tensor([0.5935, 0.6644]) - torch.Size([2])\n",
      "P3 tensor([0.5935, 0.6644]) - torch.Size([2])\n",
      "P4 tensor([1.5935, 1.6644]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0460,  0.5889, -0.0979, -0.9269],\n",
      "        [ 0.0304,  0.7830, -0.0740, -1.1955]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0578,  0.3953, -0.1164, -0.6665],\n",
      "        [ 0.0460,  0.5889, -0.0979, -0.9269]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5935, 1.6644]) - torch.Size([2])\n",
      "Q-values tensor([[0.8805, 0.8955],\n",
      "        [0.9876, 0.9908]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8805],\n",
      "        [0.9876]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0657,  0.2019, -0.1297, -0.4126]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06973511  0.3986268  -0.13799174 -0.74318784] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5941, 0.5916],\n",
      "        [0.5306, 0.5461]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.5941, 0.5461]) - torch.Size([2])\n",
      "P2 tensor([0.5935, 0.5455]) - torch.Size([2])\n",
      "P3 tensor([0.5935, 0.5455]) - torch.Size([2])\n",
      "P4 tensor([1.5935, 1.5455]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0460,  0.5889, -0.0979, -0.9269],\n",
      "        [ 0.0578,  0.3953, -0.1164, -0.6665]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0578,  0.3953, -0.1164, -0.6665],\n",
      "        [ 0.0657,  0.2019, -0.1297, -0.4126]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.5935, 1.5455]) - torch.Size([2])\n",
      "Q-values tensor([[0.8920, 0.8959],\n",
      "        [0.7889, 0.8076]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8920],\n",
      "        [0.7889]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0697,  0.3986, -0.1380, -0.7432]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.07770765  0.5953562  -0.1528555  -1.0759178 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6132, 0.6083],\n",
      "        [0.5306, 0.5461]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6132, 0.5461]) - torch.Size([2])\n",
      "P2 tensor([0.6126, 0.5455]) - torch.Size([2])\n",
      "P3 tensor([0.6126, 0.5455]) - torch.Size([2])\n",
      "P4 tensor([1.6126, 1.5455]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0657,  0.2019, -0.1297, -0.4126],\n",
      "        [ 0.0578,  0.3953, -0.1164, -0.6665]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0697,  0.3986, -0.1380, -0.7432],\n",
      "        [ 0.0657,  0.2019, -0.1297, -0.4126]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6126, 1.5455]) - torch.Size([2])\n",
      "Q-values tensor([[0.7072, 0.7425],\n",
      "        [0.7981, 0.8080]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7425],\n",
      "        [0.7981]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [00:01<02:40,  6.17 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States tensor([[ 0.0777,  0.5954, -0.1529, -1.0759]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.08961477  0.4025476  -0.17437385 -0.8348417 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6338, 0.6303],\n",
      "        [0.6132, 0.6083]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6338, 0.6132]) - torch.Size([2])\n",
      "P2 tensor([0.6332, 0.6126]) - torch.Size([2])\n",
      "P3 tensor([0.6332, 0.6126]) - torch.Size([2])\n",
      "P4 tensor([1.6332, 1.6126]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0777,  0.5954, -0.1529, -1.0759],\n",
      "        [ 0.0657,  0.2019, -0.1297, -0.4126]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0896,  0.4025, -0.1744, -0.8348],\n",
      "        [ 0.0697,  0.3986, -0.1380, -0.7432]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6332, 1.6126]) - torch.Size([2])\n",
      "Q-values tensor([[0.9650, 0.9562],\n",
      "        [0.7127, 0.7489]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9650],\n",
      "        [0.7489]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0896,  0.4025, -0.1744, -0.8348]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.09766572  0.59956807 -0.19107069 -1.1768973 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7013, 0.6916],\n",
      "        [0.6338, 0.6303]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7013, 0.6338]) - torch.Size([2])\n",
      "P2 tensor([0.7006, 0.6332]) - torch.Size([2])\n",
      "P3 tensor([0.7006, 0.6332]) - torch.Size([2])\n",
      "P4 tensor([1.7006, 1.6332]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0697,  0.3986, -0.1380, -0.7432],\n",
      "        [ 0.0777,  0.5954, -0.1529, -1.0759]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0777,  0.5954, -0.1529, -1.0759],\n",
      "        [ 0.0896,  0.4025, -0.1744, -0.8348]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7006, 1.6332]) - torch.Size([2])\n",
      "Q-values tensor([[0.8391, 0.8464],\n",
      "        [0.9750, 0.9635]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8464],\n",
      "        [0.9750]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0977,  0.5996, -0.1911, -1.1769]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.10965708  0.4073707  -0.21460864 -0.94968146] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6338, 0.6303],\n",
      "        [0.7298, 0.7194]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6338, 0.7298]) - torch.Size([2])\n",
      "P2 tensor([0.6332, 0.7291]) - torch.Size([2])\n",
      "P3 tensor([0.6332, 0.7291]) - torch.Size([2])\n",
      "P4 tensor([1.6332, 1.7291]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0777,  0.5954, -0.1529, -1.0759],\n",
      "        [ 0.0896,  0.4025, -0.1744, -0.8348]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0896,  0.4025, -0.1744, -0.8348],\n",
      "        [ 0.0977,  0.5996, -0.1911, -1.1769]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6332, 1.7291]) - torch.Size([2])\n",
      "Q-values tensor([[0.9840, 0.9722],\n",
      "        [0.8798, 0.8858]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9840],\n",
      "        [0.8858]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0215, -0.0344,  0.0164,  0.0497]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02217251 -0.22970484  0.01742879  0.34755778] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9301, 0.9333],\n",
      "        [0.5370, 0.6329]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9333, 0.6329]) - torch.Size([2])\n",
      "P2 tensor([0.9323, 0.6323]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.6323]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.6323]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0977,  0.5996, -0.1911, -1.1769],\n",
      "        [-0.0215, -0.0344,  0.0164,  0.0497]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1097,  0.4074, -0.2146, -0.9497],\n",
      "        [-0.0222, -0.2297,  0.0174,  0.3476]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.6323]) - torch.Size([2])\n",
      "Q-values tensor([[1.0340, 1.0206],\n",
      "        [0.6001, 0.6604]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0340],\n",
      "        [0.6001]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0222, -0.2297,  0.0174,  0.3476]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02676661 -0.4250703   0.02437994  0.64568526] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9301, 0.9333],\n",
      "        [0.5009, 0.6219]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9333, 0.6219]) - torch.Size([2])\n",
      "P2 tensor([0.9323, 0.6213]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.6213]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.6213]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0977,  0.5996, -0.1911, -1.1769],\n",
      "        [-0.0222, -0.2297,  0.0174,  0.3476]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1097,  0.4074, -0.2146, -0.9497],\n",
      "        [-0.0268, -0.4251,  0.0244,  0.6457]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.6213]) - torch.Size([2])\n",
      "Q-values tensor([[1.0385, 1.0218],\n",
      "        [0.5434, 0.6335]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0385],\n",
      "        [0.5434]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0268, -0.4251,  0.0244,  0.6457]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03526801 -0.2302964   0.03729365  0.36077824] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5360, 0.6332],\n",
      "        [0.5370, 0.6329]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6332, 0.6329]) - torch.Size([2])\n",
      "P2 tensor([0.6326, 0.6323]) - torch.Size([2])\n",
      "P3 tensor([0.6326, 0.6323]) - torch.Size([2])\n",
      "P4 tensor([1.6326, 1.6323]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0268, -0.4251,  0.0244,  0.6457],\n",
      "        [-0.0215, -0.0344,  0.0164,  0.0497]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0353, -0.2303,  0.0373,  0.3608],\n",
      "        [-0.0222, -0.2297,  0.0174,  0.3476]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6326, 1.6323]) - torch.Size([2])\n",
      "Q-values tensor([[0.5132, 0.6238],\n",
      "        [0.6132, 0.6614]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6238],\n",
      "        [0.6132]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0353, -0.2303,  0.0373,  0.3608]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03987394 -0.03572386  0.04450921  0.08008416] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5360, 0.6332],\n",
      "        [0.5009, 0.6219]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6332, 0.6219]) - torch.Size([2])\n",
      "P2 tensor([0.6326, 0.6213]) - torch.Size([2])\n",
      "P3 tensor([0.6326, 0.6213]) - torch.Size([2])\n",
      "P4 tensor([1.6326, 1.6213]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0268, -0.4251,  0.0244,  0.6457],\n",
      "        [-0.0222, -0.2297,  0.0174,  0.3476]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0353, -0.2303,  0.0373,  0.3608],\n",
      "        [-0.0268, -0.4251,  0.0244,  0.6457]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6326, 1.6213]) - torch.Size([2])\n",
      "Q-values tensor([[0.5178, 0.6295],\n",
      "        [0.5549, 0.6385]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6295],\n",
      "        [0.5549]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0399, -0.0357,  0.0445,  0.0801]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04058842 -0.23145467  0.0461109   0.38647097] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5321, 0.6307],\n",
      "        [0.5360, 0.6332]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6307, 0.6332]) - torch.Size([2])\n",
      "P2 tensor([0.6301, 0.6326]) - torch.Size([2])\n",
      "P3 tensor([0.6301, 0.6326]) - torch.Size([2])\n",
      "P4 tensor([1.6301, 1.6326]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0399, -0.0357,  0.0445,  0.0801],\n",
      "        [-0.0268, -0.4251,  0.0244,  0.6457]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0406, -0.2315,  0.0461,  0.3865],\n",
      "        [-0.0353, -0.2303,  0.0373,  0.3608]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6301, 1.6326]) - torch.Size([2])\n",
      "Q-values tensor([[0.6200, 0.6666],\n",
      "        [0.5229, 0.6351]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6200],\n",
      "        [0.6351]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0406, -0.2315,  0.0461,  0.3865]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04521751 -0.42719984  0.05384032  0.69332844] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5321, 0.6307],\n",
      "        [0.4972, 0.6214]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6307, 0.6214]) - torch.Size([2])\n",
      "P2 tensor([0.6301, 0.6207]) - torch.Size([2])\n",
      "P3 tensor([0.6301, 0.6207]) - torch.Size([2])\n",
      "P4 tensor([1.6301, 1.6207]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0399, -0.0357,  0.0445,  0.0801],\n",
      "        [-0.0406, -0.2315,  0.0461,  0.3865]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0406, -0.2315,  0.0461,  0.3865],\n",
      "        [-0.0452, -0.4272,  0.0538,  0.6933]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6301, 1.6207]) - torch.Size([2])\n",
      "Q-values tensor([[0.6256, 0.6708],\n",
      "        [0.5605, 0.6456]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6256],\n",
      "        [0.5605]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0452, -0.4272,  0.0538,  0.6933]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.05376151 -0.6230257   0.06770688  1.002463  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5014, 0.6438],\n",
      "        [0.4972, 0.6214]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6438, 0.6214]) - torch.Size([2])\n",
      "P2 tensor([0.6431, 0.6207]) - torch.Size([2])\n",
      "P3 tensor([0.6431, 0.6207]) - torch.Size([2])\n",
      "P4 tensor([1.6431, 1.6207]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0452, -0.4272,  0.0538,  0.6933],\n",
      "        [-0.0406, -0.2315,  0.0461,  0.3865]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0538, -0.6230,  0.0677,  1.0025],\n",
      "        [-0.0452, -0.4272,  0.0538,  0.6933]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6431, 1.6207]) - torch.Size([2])\n",
      "Q-values tensor([[0.5306, 0.6416],\n",
      "        [0.5672, 0.6463]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5306],\n",
      "        [0.5672]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0538, -0.6230,  0.0677,  1.0025]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06622202 -0.81898373  0.08775615  1.3156172 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5014, 0.6438],\n",
      "        [0.5292, 0.6797]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6438, 0.6797]) - torch.Size([2])\n",
      "P2 tensor([0.6431, 0.6790]) - torch.Size([2])\n",
      "P3 tensor([0.6431, 0.6790]) - torch.Size([2])\n",
      "P4 tensor([1.6431, 1.6790]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0452, -0.4272,  0.0538,  0.6933],\n",
      "        [-0.0538, -0.6230,  0.0677,  1.0025]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0538, -0.6230,  0.0677,  1.0025],\n",
      "        [-0.0662, -0.8190,  0.0878,  1.3156]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6431, 1.6790]) - torch.Size([2])\n",
      "Q-values tensor([[0.5377, 0.6431],\n",
      "        [0.5435, 0.6690]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5377],\n",
      "        [0.5435]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0662, -0.8190,  0.0878,  1.3156]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0826017  -1.0150995   0.11406849  1.6344261 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5292, 0.6797],\n",
      "        [0.5609, 0.7177]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6797, 0.7177]) - torch.Size([2])\n",
      "P2 tensor([0.6790, 0.7170]) - torch.Size([2])\n",
      "P3 tensor([0.6790, 0.7170]) - torch.Size([2])\n",
      "P4 tensor([1.6790, 1.7170]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0538, -0.6230,  0.0677,  1.0025],\n",
      "        [-0.0662, -0.8190,  0.0878,  1.3156]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0662, -0.8190,  0.0878,  1.3156],\n",
      "        [-0.0826, -1.0151,  0.1141,  1.6344]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6790, 1.7170]) - torch.Size([2])\n",
      "Q-values tensor([[0.5513, 0.6712],\n",
      "        [0.5840, 0.7111]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5513],\n",
      "        [0.5840]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0826, -1.0151,  0.1141,  1.6344]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.10290369 -0.8214863   0.14675702  1.379357  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5609, 0.7177],\n",
      "        [0.5292, 0.6797]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7177, 0.6797]) - torch.Size([2])\n",
      "P2 tensor([0.7170, 0.6790]) - torch.Size([2])\n",
      "P3 tensor([0.7170, 0.6790]) - torch.Size([2])\n",
      "P4 tensor([1.7170, 1.6790]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0662, -0.8190,  0.0878,  1.3156],\n",
      "        [-0.0538, -0.6230,  0.0677,  1.0025]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0826, -1.0151,  0.1141,  1.6344],\n",
      "        [-0.0662, -0.8190,  0.0878,  1.3156]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7170, 1.6790]) - torch.Size([2])\n",
      "Q-values tensor([[0.5930, 0.7137],\n",
      "        [0.5591, 0.6734]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5930],\n",
      "        [0.5591]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.1029, -0.8215,  0.1468,  1.3794]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.11933342 -0.6284695   0.17434415  1.135936  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5070, 0.6630],\n",
      "        [0.5340, 0.6893]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6630, 0.6893]) - torch.Size([2])\n",
      "P2 tensor([0.6623, 0.6887]) - torch.Size([2])\n",
      "P3 tensor([0.6623, 0.6887]) - torch.Size([2])\n",
      "P4 tensor([1.6623, 1.6887]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.1029, -0.8215,  0.1468,  1.3794],\n",
      "        [-0.0826, -1.0151,  0.1141,  1.6344]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1193, -0.6285,  0.1743,  1.1359],\n",
      "        [-0.1029, -0.8215,  0.1468,  1.3794]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6623, 1.6887]) - torch.Size([2])\n",
      "Q-values tensor([[0.6080, 0.7264],\n",
      "        [0.6412, 0.7587]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7264],\n",
      "        [0.7587]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.1193, -0.6285,  0.1743,  1.1359]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.1319028  -0.4360029   0.19706288  0.90261054] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4960, 0.6441],\n",
      "        [0.5070, 0.6630]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6441, 0.6630]) - torch.Size([2])\n",
      "P2 tensor([0.6434, 0.6623]) - torch.Size([2])\n",
      "P3 tensor([0.6434, 0.6623]) - torch.Size([2])\n",
      "P4 tensor([1.6434, 1.6623]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.1193, -0.6285,  0.1743,  1.1359],\n",
      "        [-0.1029, -0.8215,  0.1468,  1.3794]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1319, -0.4360,  0.1971,  0.9026],\n",
      "        [-0.1193, -0.6285,  0.1743,  1.1359]]) - torch.Size([2, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1000 [00:01<02:27,  6.71 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next values tensor([1.6434, 1.6623]) - torch.Size([2])\n",
      "Q-values tensor([[0.5775, 0.7036],\n",
      "        [0.6102, 0.7355]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7036],\n",
      "        [0.7355]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.1319, -0.4360,  0.1971,  0.9026]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [-0.14062287 -0.6331698   0.21511509  1.2501991 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5114, 0.6708],\n",
      "        [0.4960, 0.6441]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6708, 0.6441]) - torch.Size([2])\n",
      "P2 tensor([0.6702, 0.6434]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.6434]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.6434]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.1319, -0.4360,  0.1971,  0.9026],\n",
      "        [-0.1193, -0.6285,  0.1743,  1.1359]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1406, -0.6332,  0.2151,  1.2502],\n",
      "        [-0.1319, -0.4360,  0.1971,  0.9026]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.6434]) - torch.Size([2])\n",
      "Q-values tensor([[0.5633, 0.6863],\n",
      "        [0.5793, 0.7118]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.5633],\n",
      "        [0.7118]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0171, -0.0424,  0.0012,  0.0224]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01795344 -0.23755676  0.00166832  0.31547725] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4960, 0.6441],\n",
      "        [0.5413, 0.6362]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6441, 0.6362]) - torch.Size([2])\n",
      "P2 tensor([0.6434, 0.6356]) - torch.Size([2])\n",
      "P3 tensor([0.6434, 0.6356]) - torch.Size([2])\n",
      "P4 tensor([1.6434, 1.6356]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.1193, -0.6285,  0.1743,  1.1359],\n",
      "        [-0.0171, -0.0424,  0.0012,  0.0224]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1319, -0.4360,  0.1971,  0.9026],\n",
      "        [-0.0180, -0.2376,  0.0017,  0.3155]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6434, 1.6356]) - torch.Size([2])\n",
      "Q-values tensor([[0.5845, 0.7196],\n",
      "        [0.6698, 0.6954]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7196],\n",
      "        [0.6698]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0180, -0.2376,  0.0017,  0.3155]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02270457 -0.43270242  0.00797787  0.60868585] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5413, 0.6362],\n",
      "        [0.5114, 0.6708]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6362, 0.6708]) - torch.Size([2])\n",
      "P2 tensor([0.6356, 0.6702]) - torch.Size([2])\n",
      "P3 tensor([0.6356, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.6356, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0171, -0.0424,  0.0012,  0.0224],\n",
      "        [-0.1319, -0.4360,  0.1971,  0.9026]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0180, -0.2376,  0.0017,  0.3155],\n",
      "        [-0.1406, -0.6332,  0.2151,  1.2502]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6356, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.6758, 0.6990],\n",
      "        [0.5732, 0.6989]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6758],\n",
      "        [0.5732]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0227, -0.4327,  0.0080,  0.6087]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03135862 -0.23769291  0.02015158  0.31852636] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5422, 0.6378],\n",
      "        [0.5035, 0.6236]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6378, 0.6236]) - torch.Size([2])\n",
      "P2 tensor([0.6372, 0.6230]) - torch.Size([2])\n",
      "P3 tensor([0.6372, 0.6230]) - torch.Size([2])\n",
      "P4 tensor([1.6372, 1.6230]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0227, -0.4327,  0.0080,  0.6087],\n",
      "        [-0.0180, -0.2376,  0.0017,  0.3155]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0314, -0.2377,  0.0202,  0.3185],\n",
      "        [-0.0227, -0.4327,  0.0080,  0.6087]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6372, 1.6230]) - torch.Size([2])\n",
      "Q-values tensor([[0.5826, 0.6734],\n",
      "        [0.6172, 0.6747]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6734],\n",
      "        [0.6172]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0314, -0.2377,  0.0202,  0.3185]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03611248 -0.04286367  0.02652211  0.03226601] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5422, 0.6378],\n",
      "        [0.6053, 0.6655]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6378, 0.6655]) - torch.Size([2])\n",
      "P2 tensor([0.6372, 0.6649]) - torch.Size([2])\n",
      "P3 tensor([0.6372, 0.6649]) - torch.Size([2])\n",
      "P4 tensor([1.6372, 1.6649]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0227, -0.4327,  0.0080,  0.6087],\n",
      "        [-0.0314, -0.2377,  0.0202,  0.3185]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0314, -0.2377,  0.0202,  0.3185],\n",
      "        [-0.0361, -0.0429,  0.0265,  0.0323]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6372, 1.6649]) - torch.Size([2])\n",
      "Q-values tensor([[0.5878, 0.6792],\n",
      "        [0.6237, 0.6812]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6792],\n",
      "        [0.6812]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0361, -0.0429,  0.0265,  0.0323]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03696975  0.15186809  0.02716743 -0.25193232] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5422, 0.6378],\n",
      "        [0.6053, 0.6655]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6378, 0.6655]) - torch.Size([2])\n",
      "P2 tensor([0.6372, 0.6649]) - torch.Size([2])\n",
      "P3 tensor([0.6372, 0.6649]) - torch.Size([2])\n",
      "P4 tensor([1.6372, 1.6649]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0227, -0.4327,  0.0080,  0.6087],\n",
      "        [-0.0314, -0.2377,  0.0202,  0.3185]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0314, -0.2377,  0.0202,  0.3185],\n",
      "        [-0.0361, -0.0429,  0.0265,  0.0323]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6372, 1.6649]) - torch.Size([2])\n",
      "Q-values tensor([[0.5889, 0.6861],\n",
      "        [0.6243, 0.6878]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6861],\n",
      "        [0.6878]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0370,  0.1519,  0.0272, -0.2519]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03393239  0.3465918   0.02212879 -0.53592384] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6904, 0.7259],\n",
      "        [0.7854, 0.8009]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7259, 0.8009]) - torch.Size([2])\n",
      "P2 tensor([0.7252, 0.8001]) - torch.Size([2])\n",
      "P3 tensor([0.7252, 0.8001]) - torch.Size([2])\n",
      "P4 tensor([1.7252, 1.8001]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0361, -0.0429,  0.0265,  0.0323],\n",
      "        [-0.0370,  0.1519,  0.0272, -0.2519]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0370,  0.1519,  0.0272, -0.2519],\n",
      "        [-0.0339,  0.3466,  0.0221, -0.5359]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7252, 1.8001]) - torch.Size([2])\n",
      "Q-values tensor([[0.6903, 0.7180],\n",
      "        [0.7758, 0.7764]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7180],\n",
      "        [0.7764]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0339,  0.3466,  0.0221, -0.5359]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02700055  0.15116578  0.01141031 -0.23635127] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6850, 0.7213],\n",
      "        [0.7854, 0.8009]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7213, 0.8009]) - torch.Size([2])\n",
      "P2 tensor([0.7206, 0.8001]) - torch.Size([2])\n",
      "P3 tensor([0.7206, 0.8001]) - torch.Size([2])\n",
      "P4 tensor([1.7206, 1.8001]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0339,  0.3466,  0.0221, -0.5359],\n",
      "        [-0.0370,  0.1519,  0.0272, -0.2519]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0270,  0.1512,  0.0114, -0.2364],\n",
      "        [-0.0339,  0.3466,  0.0221, -0.5359]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7206, 1.8001]) - torch.Size([2])\n",
      "Q-values tensor([[0.8741, 0.8604],\n",
      "        [0.7761, 0.7841]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8741],\n",
      "        [0.7841]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0270,  0.1512,  0.0114, -0.2364]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02397724 -0.04411731  0.00668328  0.05990886] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5965, 0.6579],\n",
      "        [0.7854, 0.8009]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6579, 0.8009]) - torch.Size([2])\n",
      "P2 tensor([0.6573, 0.8001]) - torch.Size([2])\n",
      "P3 tensor([0.6573, 0.8001]) - torch.Size([2])\n",
      "P4 tensor([1.6573, 1.8001]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0270,  0.1512,  0.0114, -0.2364],\n",
      "        [-0.0370,  0.1519,  0.0272, -0.2519]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0240, -0.0441,  0.0067,  0.0599],\n",
      "        [-0.0339,  0.3466,  0.0221, -0.5359]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6573, 1.8001]) - torch.Size([2])\n",
      "Q-values tensor([[0.7756, 0.7855],\n",
      "        [0.7816, 0.7904]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7756],\n",
      "        [0.7904]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0240, -0.0441,  0.0067,  0.0599]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02485958 -0.23933445  0.00788146  0.3546929 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5351, 0.6318],\n",
      "        [0.5965, 0.6579]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6318, 0.6579]) - torch.Size([2])\n",
      "P2 tensor([0.6311, 0.6573]) - torch.Size([2])\n",
      "P3 tensor([0.6311, 0.6573]) - torch.Size([2])\n",
      "P4 tensor([1.6311, 1.6573]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0240, -0.0441,  0.0067,  0.0599],\n",
      "        [-0.0270,  0.1512,  0.0114, -0.2364]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0249, -0.2393,  0.0079,  0.3547],\n",
      "        [-0.0240, -0.0441,  0.0067,  0.0599]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6311, 1.6573]) - torch.Size([2])\n",
      "Q-values tensor([[0.6904, 0.7277],\n",
      "        [0.7811, 0.7918]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6904],\n",
      "        [0.7811]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0249, -0.2393,  0.0079,  0.3547]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02964627 -0.04432544  0.01497532  0.06450559] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5965, 0.6579],\n",
      "        [0.5351, 0.6318]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6579, 0.6318]) - torch.Size([2])\n",
      "P2 tensor([0.6573, 0.6311]) - torch.Size([2])\n",
      "P3 tensor([0.6573, 0.6311]) - torch.Size([2])\n",
      "P4 tensor([1.6573, 1.6311]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0270,  0.1512,  0.0114, -0.2364],\n",
      "        [-0.0240, -0.0441,  0.0067,  0.0599]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0240, -0.0441,  0.0067,  0.0599],\n",
      "        [-0.0249, -0.2393,  0.0079,  0.3547]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6573, 1.6311]) - torch.Size([2])\n",
      "Q-values tensor([[0.7889, 0.7921],\n",
      "        [0.6973, 0.7281]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7889],\n",
      "        [0.6973]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0296, -0.0443,  0.0150,  0.0645]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03053278 -0.23965886  0.01626543  0.36187544] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5962, 0.6580],\n",
      "        [0.5343, 0.6316]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6580, 0.6316]) - torch.Size([2])\n",
      "P2 tensor([0.6573, 0.6310]) - torch.Size([2])\n",
      "P3 tensor([0.6573, 0.6310]) - torch.Size([2])\n",
      "P4 tensor([1.6573, 1.6310]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0249, -0.2393,  0.0079,  0.3547],\n",
      "        [-0.0296, -0.0443,  0.0150,  0.0645]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0296, -0.0443,  0.0150,  0.0645],\n",
      "        [-0.0305, -0.2397,  0.0163,  0.3619]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6573, 1.6310]) - torch.Size([2])\n",
      "Q-values tensor([[0.6387, 0.7057],\n",
      "        [0.7042, 0.7287]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7057],\n",
      "        [0.7042]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0305, -0.2397,  0.0163,  0.3619]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03532596 -0.4350082   0.02350294  0.6596425 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5962, 0.6580],\n",
      "        [0.5343, 0.6316]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6580, 0.6316]) - torch.Size([2])\n",
      "P2 tensor([0.6573, 0.6310]) - torch.Size([2])\n",
      "P3 tensor([0.6573, 0.6310]) - torch.Size([2])\n",
      "P4 tensor([1.6573, 1.6310]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0249, -0.2393,  0.0079,  0.3547],\n",
      "        [-0.0296, -0.0443,  0.0150,  0.0645]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0296, -0.0443,  0.0150,  0.0645],\n",
      "        [-0.0305, -0.2397,  0.0163,  0.3619]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6573, 1.6310]) - torch.Size([2])\n",
      "Q-values tensor([[0.6438, 0.7110],\n",
      "        [0.7098, 0.7336]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7110],\n",
      "        [0.7098]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0353, -0.4350,  0.0235,  0.6596]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04402613 -0.6304492   0.03669579  0.9596323 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5004, 0.6397],\n",
      "        [0.5343, 0.6316]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6397, 0.6316]) - torch.Size([2])\n",
      "P2 tensor([0.6390, 0.6310]) - torch.Size([2])\n",
      "P3 tensor([0.6390, 0.6310]) - torch.Size([2])\n",
      "P4 tensor([1.6390, 1.6310]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0353, -0.4350,  0.0235,  0.6596],\n",
      "        [-0.0296, -0.0443,  0.0150,  0.0645]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0440, -0.6304,  0.0367,  0.9596],\n",
      "        [-0.0305, -0.2397,  0.0163,  0.3619]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6390, 1.6310]) - torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|郊         | 13/1000 [00:02<02:35,  6.34 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values tensor([[0.6157, 0.7204],\n",
      "        [0.7153, 0.7385]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6157],\n",
      "        [0.7153]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0440, -0.6304,  0.0367,  0.9596]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.05663511 -0.82604474  0.05588843  1.2636142 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5274, 0.6756],\n",
      "        [0.5004, 0.6397]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6756, 0.6397]) - torch.Size([2])\n",
      "P2 tensor([0.6749, 0.6390]) - torch.Size([2])\n",
      "P3 tensor([0.6749, 0.6390]) - torch.Size([2])\n",
      "P4 tensor([1.6749, 1.6390]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0440, -0.6304,  0.0367,  0.9596],\n",
      "        [-0.0353, -0.4350,  0.0235,  0.6596]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0566, -0.8260,  0.0559,  1.2636],\n",
      "        [-0.0440, -0.6304,  0.0367,  0.9596]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6749, 1.6390]) - torch.Size([2])\n",
      "Q-values tensor([[0.6328, 0.7533],\n",
      "        [0.6230, 0.7218]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6328],\n",
      "        [0.6230]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0566, -0.8260,  0.0559,  1.2636]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.073156   -0.63168     0.08116072  0.98894495] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5021, 0.6475],\n",
      "        [0.5274, 0.6756]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6475, 0.6756]) - torch.Size([2])\n",
      "P2 tensor([0.6468, 0.6749]) - torch.Size([2])\n",
      "P3 tensor([0.6468, 0.6749]) - torch.Size([2])\n",
      "P4 tensor([1.6468, 1.6749]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0566, -0.8260,  0.0559,  1.2636],\n",
      "        [-0.0440, -0.6304,  0.0367,  0.9596]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0732, -0.6317,  0.0812,  0.9889],\n",
      "        [-0.0566, -0.8260,  0.0559,  1.2636]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6468, 1.6749]) - torch.Size([2])\n",
      "Q-values tensor([[0.6835, 0.8053],\n",
      "        [0.6409, 0.7554]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8053],\n",
      "        [0.6409]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0732, -0.6317,  0.0812,  0.9889]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.08578961 -0.43773282  0.10093962  0.7228169 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4967, 0.6280],\n",
      "        [0.5021, 0.6475]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6280, 0.6475]) - torch.Size([2])\n",
      "P2 tensor([0.6274, 0.6468]) - torch.Size([2])\n",
      "P3 tensor([0.6274, 0.6468]) - torch.Size([2])\n",
      "P4 tensor([1.6274, 1.6468]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0732, -0.6317,  0.0812,  0.9889],\n",
      "        [-0.0566, -0.8260,  0.0559,  1.2636]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0858, -0.4377,  0.1009,  0.7228],\n",
      "        [-0.0732, -0.6317,  0.0812,  0.9889]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6274, 1.6468]) - torch.Size([2])\n",
      "Q-values tensor([[0.6517, 0.7711],\n",
      "        [0.6911, 0.8128]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7711],\n",
      "        [0.8128]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0858, -0.4377,  0.1009,  0.7228]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.09454426 -0.24414115  0.11539596  0.46353334] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4967, 0.6280],\n",
      "        [0.5252, 0.6322]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6280, 0.6322]) - torch.Size([2])\n",
      "P2 tensor([0.6274, 0.6316]) - torch.Size([2])\n",
      "P3 tensor([0.6274, 0.6316]) - torch.Size([2])\n",
      "P4 tensor([1.6274, 1.6316]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0732, -0.6317,  0.0812,  0.9889],\n",
      "        [-0.0858, -0.4377,  0.1009,  0.7228]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0858, -0.4377,  0.1009,  0.7228],\n",
      "        [-0.0945, -0.2441,  0.1154,  0.4635]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6274, 1.6316]) - torch.Size([2])\n",
      "Q-values tensor([[0.6534, 0.7791],\n",
      "        [0.6369, 0.7445]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7791],\n",
      "        [0.7445]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0945, -0.2441,  0.1154,  0.4635]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.09942708 -0.4406886   0.12466662  0.79024494] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4967, 0.6280],\n",
      "        [0.4957, 0.6322]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6280, 0.6322]) - torch.Size([2])\n",
      "P2 tensor([0.6274, 0.6316]) - torch.Size([2])\n",
      "P3 tensor([0.6274, 0.6316]) - torch.Size([2])\n",
      "P4 tensor([1.6274, 1.6316]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0732, -0.6317,  0.0812,  0.9889],\n",
      "        [-0.0945, -0.2441,  0.1154,  0.4635]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0858, -0.4377,  0.1009,  0.7228],\n",
      "        [-0.0994, -0.4407,  0.1247,  0.7902]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6274, 1.6316]) - torch.Size([2])\n",
      "Q-values tensor([[0.6552, 0.7871],\n",
      "        [0.6630, 0.7390]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7871],\n",
      "        [0.6630]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0994, -0.4407,  0.1247,  0.7902]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.10824085 -0.63728184  0.14047152  1.1194032 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.4957, 0.6322],\n",
      "        [0.5252, 0.6322]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6322, 0.6322]) - torch.Size([2])\n",
      "P2 tensor([0.6316, 0.6316]) - torch.Size([2])\n",
      "P3 tensor([0.6316, 0.6316]) - torch.Size([2])\n",
      "P4 tensor([1.6316, 1.6316]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0945, -0.2441,  0.1154,  0.4635],\n",
      "        [-0.0858, -0.4377,  0.1009,  0.7228]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0994, -0.4407,  0.1247,  0.7902],\n",
      "        [-0.0945, -0.2441,  0.1154,  0.4635]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6316, 1.6316]) - torch.Size([2])\n",
      "Q-values tensor([[0.6690, 0.7436],\n",
      "        [0.6439, 0.7572]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6690],\n",
      "        [0.7572]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.1082, -0.6373,  0.1405,  1.1194]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.12098649 -0.8339387   0.16285959  1.452646  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5381, 0.6957],\n",
      "        [0.4957, 0.6322]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6957, 0.6322]) - torch.Size([2])\n",
      "P2 tensor([0.6950, 0.6316]) - torch.Size([2])\n",
      "P3 tensor([0.6950, 0.6316]) - torch.Size([2])\n",
      "P4 tensor([1.6950, 1.6316]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.1082, -0.6373,  0.1405,  1.1194],\n",
      "        [-0.0945, -0.2441,  0.1154,  0.4635]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1210, -0.8339,  0.1629,  1.4526],\n",
      "        [-0.0994, -0.4407,  0.1247,  0.7902]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6950, 1.6316]) - torch.Size([2])\n",
      "Q-values tensor([[0.6768, 0.8189],\n",
      "        [0.6749, 0.7485]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.6768],\n",
      "        [0.6749]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.1210, -0.8339,  0.1629,  1.4526]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.13766527 -0.6411484   0.1919125   1.2149564 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5101, 0.6689],\n",
      "        [0.5381, 0.6957]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6689, 0.6957]) - torch.Size([2])\n",
      "P2 tensor([0.6682, 0.6950]) - torch.Size([2])\n",
      "P3 tensor([0.6682, 0.6950]) - torch.Size([2])\n",
      "P4 tensor([1.6682, 1.6950]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.1210, -0.8339,  0.1629,  1.4526],\n",
      "        [-0.1082, -0.6373,  0.1405,  1.1194]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1377, -0.6411,  0.1919,  1.2150],\n",
      "        [-0.1210, -0.8339,  0.1629,  1.4526]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6682, 1.6950]) - torch.Size([2])\n",
      "Q-values tensor([[0.7389, 0.8788],\n",
      "        [0.6854, 0.8211]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8788],\n",
      "        [0.6854]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.1377, -0.6411,  0.1919,  1.2150]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [-0.15048823 -0.8381564   0.21621163  1.5611198 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5430, 0.7047],\n",
      "        [0.5101, 0.6689]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7047, 0.6689]) - torch.Size([2])\n",
      "P2 tensor([0.7040, 0.6682]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.6682]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.6682]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.1377, -0.6411,  0.1919,  1.2150],\n",
      "        [-0.1210, -0.8339,  0.1629,  1.4526]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1505, -0.8382,  0.2162,  1.5611],\n",
      "        [-0.1377, -0.6411,  0.1919,  1.2150]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.6682]) - torch.Size([2])\n",
      "Q-values tensor([[0.7024, 0.8432],\n",
      "        [0.7469, 0.8874]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7024],\n",
      "        [0.8874]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0415, -0.0098, -0.0100, -0.0469]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04167691 -0.2048001  -0.01096531  0.2425848 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5101, 0.6689],\n",
      "        [0.5430, 0.7047]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6689, 0.7047]) - torch.Size([2])\n",
      "P2 tensor([0.6682, 0.7040]) - torch.Size([2])\n",
      "P3 tensor([0.6682, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.6682, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.1210, -0.8339,  0.1629,  1.4526],\n",
      "        [-0.1377, -0.6411,  0.1919,  1.2150]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1377, -0.6411,  0.1919,  1.2150],\n",
      "        [-0.1505, -0.8382,  0.2162,  1.5611]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6682, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.7531, 0.8972],\n",
      "        [0.7085, 0.8512]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8972],\n",
      "        [0.7085]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0417, -0.2048, -0.0110,  0.2426]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04577291 -0.00952324 -0.00611362 -0.05353661] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.5430, 0.7047],\n",
      "        [0.5526, 0.6408]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7047, 0.6408]) - torch.Size([2])\n",
      "P2 tensor([0.7040, 0.6402]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 0.6402]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 1.6402]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.1377, -0.6411,  0.1919,  1.2150],\n",
      "        [-0.0415, -0.0098, -0.0100, -0.0469]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1505, -0.8382,  0.2162,  1.5611],\n",
      "        [-0.0417, -0.2048, -0.0110,  0.2426]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 1.6402]) - torch.Size([2])\n",
      "Q-values tensor([[0.7145, 0.8593],\n",
      "        [0.7959, 0.7939]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7145],\n",
      "        [0.7959]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0458, -0.0095, -0.0061, -0.0535]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04596338  0.18568583 -0.00718435 -0.34814215] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7168, 0.7493],\n",
      "        [0.5526, 0.6408]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7493, 0.6408]) - torch.Size([2])\n",
      "P2 tensor([0.7486, 0.6402]) - torch.Size([2])\n",
      "P3 tensor([0.7486, 0.6402]) - torch.Size([2])\n",
      "P4 tensor([1.7486, 1.6402]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0458, -0.0095, -0.0061, -0.0535],\n",
      "        [-0.0415, -0.0098, -0.0100, -0.0469]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0460,  0.1857, -0.0072, -0.3481],\n",
      "        [-0.0417, -0.2048, -0.0110,  0.2426]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7486, 1.6402]) - torch.Size([2])\n",
      "Q-values tensor([[0.8060, 0.7961],\n",
      "        [0.8033, 0.7942]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7961],\n",
      "        [0.8033]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0460,  0.1857, -0.0072, -0.3481]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04224966  0.38090923 -0.01414719 -0.64308184] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6273, 0.6770],\n",
      "        [0.7168, 0.7493]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6770, 0.7493]) - torch.Size([2])\n",
      "P2 tensor([0.6763, 0.7486]) - torch.Size([2])\n",
      "P3 tensor([0.6763, 0.7486]) - torch.Size([2])\n",
      "P4 tensor([1.6763, 1.7486]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0417, -0.2048, -0.0110,  0.2426],\n",
      "        [-0.0458, -0.0095, -0.0061, -0.0535]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0458, -0.0095, -0.0061, -0.0535],\n",
      "        [-0.0460,  0.1857, -0.0072, -0.3481]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6763, 1.7486]) - torch.Size([2])\n",
      "Q-values tensor([[0.7283, 0.7710],\n",
      "        [0.8114, 0.8022]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.7710],\n",
      "        [0.8022]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0422,  0.3809, -0.0141, -0.6431]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03463147  0.1859873  -0.02700883 -0.35488734] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7168, 0.7493],\n",
      "        [0.8234, 0.8325]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7493, 0.8325]) - torch.Size([2])\n",
      "P2 tensor([0.7486, 0.8317]) - torch.Size([2])\n",
      "P3 tensor([0.7486, 0.8317]) - torch.Size([2])\n",
      "P4 tensor([1.7486, 1.8317]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0458, -0.0095, -0.0061, -0.0535],\n",
      "        [-0.0460,  0.1857, -0.0072, -0.3481]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0460,  0.1857, -0.0072, -0.3481],\n",
      "        [-0.0422,  0.3809, -0.0141, -0.6431]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7486, 1.8317]) - torch.Size([2])\n",
      "Q-values tensor([[0.8122, 0.8095],\n",
      "        [0.9091, 0.8817]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8095],\n",
      "        [0.8817]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0346,  0.1860, -0.0270, -0.3549]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03091173 -0.00874043 -0.03410657 -0.07084192] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8234, 0.8325],\n",
      "        [0.7172, 0.7509]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8325, 0.7509]) - torch.Size([2])\n",
      "P2 tensor([0.8317, 0.7501]) - torch.Size([2])\n",
      "P3 tensor([0.8317, 0.7501]) - torch.Size([2])\n",
      "P4 tensor([1.8317, 1.7501]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0460,  0.1857, -0.0072, -0.3481],\n",
      "        [-0.0422,  0.3809, -0.0141, -0.6431]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0422,  0.3809, -0.0141, -0.6431],\n",
      "        [-0.0346,  0.1860, -0.0270, -0.3549]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8317, 1.7501]) - torch.Size([2])\n",
      "Q-values tensor([[0.9101, 0.8902],\n",
      "        [1.0289, 0.9795]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8902],\n",
      "        [1.0289]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0309, -0.0087, -0.0341, -0.0708]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03108654  0.18685347 -0.03552341 -0.37408754] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6287, 0.6791],\n",
      "        [0.7223, 0.7560]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6791, 0.7560]) - torch.Size([2])\n",
      "P2 tensor([0.6785, 0.7552]) - torch.Size([2])\n",
      "P3 tensor([0.6785, 0.7552]) - torch.Size([2])\n",
      "P4 tensor([1.6785, 1.7552]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0346,  0.1860, -0.0270, -0.3549],\n",
      "        [-0.0309, -0.0087, -0.0341, -0.0708]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0309, -0.0087, -0.0341, -0.0708],\n",
      "        [-0.0311,  0.1869, -0.0355, -0.3741]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6785, 1.7552]) - torch.Size([2])\n",
      "Q-values tensor([[0.9160, 0.8985],\n",
      "        [0.8191, 0.8249]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9160],\n",
      "        [0.8249]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0311,  0.1869, -0.0355, -0.3741]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02734947  0.38246155 -0.04300516 -0.67775625] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6287, 0.6791],\n",
      "        [0.7223, 0.7560]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.6791, 0.7560]) - torch.Size([2])\n",
      "P2 tensor([0.6785, 0.7552]) - torch.Size([2])\n",
      "P3 tensor([0.6785, 0.7552]) - torch.Size([2])\n",
      "P4 tensor([1.6785, 1.7552]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0346,  0.1860, -0.0270, -0.3549],\n",
      "        [-0.0309, -0.0087, -0.0341, -0.0708]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0309, -0.0087, -0.0341, -0.0708],\n",
      "        [-0.0311,  0.1869, -0.0355, -0.3741]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.6785, 1.7552]) - torch.Size([2])\n",
      "Q-values tensor([[0.9224, 0.9050],\n",
      "        [0.8244, 0.8311]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9224],\n",
      "        [0.8311]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0273,  0.3825, -0.0430, -0.6778]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01970024  0.18796264 -0.05656029 -0.39891723] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8340, 0.8424],\n",
      "        [0.7223, 0.7560]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8424, 0.7560]) - torch.Size([2])\n",
      "P2 tensor([0.8416, 0.7552]) - torch.Size([2])\n",
      "P3 tensor([0.8416, 0.7552]) - torch.Size([2])\n",
      "P4 tensor([1.8416, 1.7552]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0311,  0.1869, -0.0355, -0.3741],\n",
      "        [-0.0309, -0.0087, -0.0341, -0.0708]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0273,  0.3825, -0.0430, -0.6778],\n",
      "        [-0.0311,  0.1869, -0.0355, -0.3741]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8416, 1.7552]) - torch.Size([2])\n",
      "Q-values tensor([[0.9346, 0.9171],\n",
      "        [0.8297, 0.8373]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9171],\n",
      "        [0.8373]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0197,  0.1880, -0.0566, -0.3989]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01594098  0.38383943 -0.06453864 -0.70888245] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7286, 0.7626],\n",
      "        [0.8441, 0.8517]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7626, 0.8517]) - torch.Size([2])\n",
      "P2 tensor([0.7618, 0.8509]) - torch.Size([2])\n",
      "P3 tensor([0.7618, 0.8509]) - torch.Size([2])\n",
      "P4 tensor([1.7618, 1.8509]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0273,  0.3825, -0.0430, -0.6778],\n",
      "        [-0.0197,  0.1880, -0.0566, -0.3989]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0197,  0.1880, -0.0566, -0.3989],\n",
      "        [-0.0159,  0.3838, -0.0645, -0.7089]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7618, 1.8509]) - torch.Size([2])\n",
      "Q-values tensor([[1.0651, 1.0213],\n",
      "        [0.9429, 0.9325]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0651],\n",
      "        [0.9325]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0159,  0.3838, -0.0645, -0.7089]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0082642   0.18966797 -0.07871628 -0.4371928 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7394, 0.7728],\n",
      "        [0.7286, 0.7626]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7728, 0.7626]) - torch.Size([2])\n",
      "P2 tensor([0.7721, 0.7618]) - torch.Size([2])\n",
      "P3 tensor([0.7721, 0.7618]) - torch.Size([2])\n",
      "P4 tensor([1.7721, 1.7618]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0159,  0.3838, -0.0645, -0.7089],\n",
      "        [-0.0273,  0.3825, -0.0430, -0.6778]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0083,  0.1897, -0.0787, -0.4372],\n",
      "        [-0.0197,  0.1880, -0.0566, -0.3989]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7721, 1.7618]) - torch.Size([2])\n",
      "Q-values tensor([[1.0846, 1.0397],\n",
      "        [1.0728, 1.0293]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0846],\n",
      "        [1.0728]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0083,  0.1897, -0.0787, -0.4372]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00447084 -0.00425666 -0.08746014 -0.1703253 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.6523, 0.7028],\n",
      "        [0.7394, 0.7728]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7028, 0.7728]) - torch.Size([2])\n",
      "P2 tensor([0.7021, 0.7721]) - torch.Size([2])\n",
      "P3 tensor([0.7021, 0.7721]) - torch.Size([2])\n",
      "P4 tensor([1.7021, 1.7721]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0083,  0.1897, -0.0787, -0.4372],\n",
      "        [-0.0159,  0.3838, -0.0645, -0.7089]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0045, -0.0043, -0.0875, -0.1703],\n",
      "        [-0.0083,  0.1897, -0.0787, -0.4372]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7021, 1.7721]) - torch.Size([2])\n",
      "Q-values tensor([[0.9704, 0.9515],\n",
      "        [1.0948, 1.0405]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9704],\n",
      "        [1.0948]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0045, -0.0043, -0.0875, -0.1703]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00455597  0.19200118 -0.09086664 -0.48926735] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7394, 0.7728],\n",
      "        [0.7567, 0.7869]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7728, 0.7869]) - torch.Size([2])\n",
      "P2 tensor([0.7721, 0.7861]) - torch.Size([2])\n",
      "P3 tensor([0.7721, 0.7861]) - torch.Size([2])\n",
      "P4 tensor([1.7721, 1.7861]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0159,  0.3838, -0.0645, -0.7089],\n",
      "        [-0.0045, -0.0043, -0.0875, -0.1703]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0083,  0.1897, -0.0787, -0.4372],\n",
      "        [-0.0046,  0.1920, -0.0909, -0.4893]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7721, 1.7861]) - torch.Size([2])\n",
      "Q-values tensor([[1.1051, 1.0417],\n",
      "        [0.8768, 0.8765]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1051],\n",
      "        [0.8765]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0046,  0.1920, -0.0909, -0.4893]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-7.1594556e-04  3.8827968e-01 -1.0065199e-01 -8.0914849e-01] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7567, 0.7869],\n",
      "        [0.6523, 0.7028]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.7869, 0.7028]) - torch.Size([2])\n",
      "P2 tensor([0.7861, 0.7021]) - torch.Size([2])\n",
      "P3 tensor([0.7861, 0.7021]) - torch.Size([2])\n",
      "P4 tensor([1.7861, 1.7021]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0045, -0.0043, -0.0875, -0.1703],\n",
      "        [-0.0083,  0.1897, -0.0787, -0.4372]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0046,  0.1920, -0.0909, -0.4893],\n",
      "        [-0.0045, -0.0043, -0.0875, -0.1703]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.7861, 1.7021]) - torch.Size([2])\n",
      "Q-values tensor([[0.8820, 0.8833],\n",
      "        [0.9859, 0.9593]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8833],\n",
      "        [0.9859]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-7.1595e-04,  3.8828e-01, -1.0065e-01, -8.0915e-01]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00704965  0.19467035 -0.11683496 -0.5497461 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8814, 0.8830],\n",
      "        [0.7766, 0.8035]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8830, 0.8035]) - torch.Size([2])\n",
      "P2 tensor([0.8821, 0.8027]) - torch.Size([2])\n",
      "P3 tensor([0.8821, 0.8027]) - torch.Size([2])\n",
      "P4 tensor([1.8821, 1.8027]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-4.5560e-03,  1.9200e-01, -9.0867e-02, -4.8927e-01],\n",
      "        [-7.1595e-04,  3.8828e-01, -1.0065e-01, -8.0915e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-7.1595e-04,  3.8828e-01, -1.0065e-01, -8.0915e-01],\n",
      "        [ 7.0496e-03,  1.9467e-01, -1.1683e-01, -5.4975e-01]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8821, 1.8027]) - torch.Size([2])\n",
      "Q-values tensor([[1.0137, 0.9823],\n",
      "        [1.1658, 1.0935]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9823],\n",
      "        [1.1658]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0070,  0.1947, -0.1168, -0.5497]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01094306  0.0013666  -0.12782988 -0.2960386 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.0449, 1.0094],\n",
      "        [0.9335, 0.9351]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0449, 0.9351]) - torch.Size([2])\n",
      "P2 tensor([1.0438, 0.9342]) - torch.Size([2])\n",
      "P3 tensor([1.0438, 0.9342]) - torch.Size([2])\n",
      "P4 tensor([2.0438, 1.9342]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-7.1595e-04,  3.8828e-01, -1.0065e-01, -8.0915e-01],\n",
      "        [ 7.0496e-03,  1.9467e-01, -1.1683e-01, -5.4975e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0070,  0.1947, -0.1168, -0.5497],\n",
      "        [ 0.0109,  0.0014, -0.1278, -0.2960]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0438, 1.9342]) - torch.Size([2])\n",
      "Q-values tensor([[1.1743, 1.1021],\n",
      "        [1.0449, 1.0094]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1743],\n",
      "        [1.0449]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0109,  0.0014, -0.1278, -0.2960]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01097039 -0.19172327 -0.13375065 -0.04624667] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8403, 0.8777],\n",
      "        [1.0449, 1.0094]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8777, 1.0449]) - torch.Size([2])\n",
      "P2 tensor([0.8768, 1.0438]) - torch.Size([2])\n",
      "P3 tensor([0.8768, 1.0438]) - torch.Size([2])\n",
      "P4 tensor([1.8768, 2.0438]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 1.0943e-02,  1.3666e-03, -1.2783e-01, -2.9604e-01],\n",
      "        [-7.1595e-04,  3.8828e-01, -1.0065e-01, -8.0915e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0110, -0.1917, -0.1338, -0.0462],\n",
      "        [ 0.0070,  0.1947, -0.1168, -0.5497]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8768, 2.0438]) - torch.Size([2])\n",
      "Q-values tensor([[0.9419, 0.9359],\n",
      "        [1.1854, 1.1034]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9419],\n",
      "        [1.1854]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0110, -0.1917, -0.1338, -0.0462]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00713592  0.00503776 -0.13467559 -0.3779578 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9665, 0.9626],\n",
      "        [0.9335, 0.9351]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9665, 0.9351]) - torch.Size([2])\n",
      "P2 tensor([0.9655, 0.9342]) - torch.Size([2])\n",
      "P3 tensor([0.9655, 0.9342]) - torch.Size([2])\n",
      "P4 tensor([1.9655, 1.9342]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0110, -0.1917, -0.1338, -0.0462],\n",
      "        [ 0.0070,  0.1947, -0.1168, -0.5497]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0071,  0.0050, -0.1347, -0.3780],\n",
      "        [ 0.0109,  0.0014, -0.1278, -0.2960]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.9655, 1.9342]) - torch.Size([2])\n",
      "Q-values tensor([[0.8554, 0.8789],\n",
      "        [1.0641, 1.0110]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8789],\n",
      "        [1.0641]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0071,  0.0050, -0.1347, -0.3780]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00723668  0.20178963 -0.14223474 -0.7098882 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8403, 0.8777],\n",
      "        [1.1167, 1.0650]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8777, 1.1167]) - torch.Size([2])\n",
      "P2 tensor([0.8768, 1.1155]) - torch.Size([2])\n",
      "P3 tensor([0.8768, 1.1155]) - torch.Size([2])\n",
      "P4 tensor([1.8768, 2.1155]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0109,  0.0014, -0.1278, -0.2960],\n",
      "        [ 0.0071,  0.0050, -0.1347, -0.3780]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0110, -0.1917, -0.1338, -0.0462],\n",
      "        [ 0.0072,  0.2018, -0.1422, -0.7099]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8768, 2.1155]) - torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|郊         | 14/1000 [00:02<02:41,  6.11 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values tensor([[0.9567, 0.9431],\n",
      "        [0.9906, 0.9709]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9567],\n",
      "        [0.9709]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0072,  0.2018, -0.1422, -0.7099]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01127247  0.00889406 -0.15643251 -0.46514294] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9665, 0.9626],\n",
      "        [1.0013, 0.9906]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9665, 1.0013]) - torch.Size([2])\n",
      "P2 tensor([0.9655, 1.0003]) - torch.Size([2])\n",
      "P3 tensor([0.9655, 1.0003]) - torch.Size([2])\n",
      "P4 tensor([1.9655, 2.0003]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0110, -0.1917, -0.1338, -0.0462],\n",
      "        [ 0.0072,  0.2018, -0.1422, -0.7099]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0071,  0.0050, -0.1347, -0.3780],\n",
      "        [ 0.0113,  0.0089, -0.1564, -0.4651]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.9655, 2.0003]) - torch.Size([2])\n",
      "Q-values tensor([[0.8661, 0.8921],\n",
      "        [1.1537, 1.0826]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8921],\n",
      "        [1.1537]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0113,  0.0089, -0.1564, -0.4651]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01145035  0.2058404  -0.16573536 -0.802759  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.1603, 1.0999],\n",
      "        [1.1167, 1.0650]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.1603, 1.1167]) - torch.Size([2])\n",
      "P2 tensor([1.1592, 1.1155]) - torch.Size([2])\n",
      "P3 tensor([1.1592, 1.1155]) - torch.Size([2])\n",
      "P4 tensor([2.1592, 2.1155]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0113,  0.0089, -0.1564, -0.4651],\n",
      "        [ 0.0071,  0.0050, -0.1347, -0.3780]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0115,  0.2058, -0.1657, -0.8028],\n",
      "        [ 0.0072,  0.2018, -0.1422, -0.7099]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.1592, 2.1155]) - torch.Size([2])\n",
      "Q-values tensor([[1.0401, 1.0145],\n",
      "        [1.0033, 0.9854]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0145],\n",
      "        [0.9854]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0115,  0.2058, -0.1657, -0.8028]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01556716  0.40279996 -0.18179055 -1.1426535 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.1603, 1.0999],\n",
      "        [1.3398, 1.2394]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.1603, 1.3398]) - torch.Size([2])\n",
      "P2 tensor([1.1592, 1.3385]) - torch.Size([2])\n",
      "P3 tensor([1.1592, 1.3385]) - torch.Size([2])\n",
      "P4 tensor([2.1592, 2.3385]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0113,  0.0089, -0.1564, -0.4651],\n",
      "        [ 0.0115,  0.2058, -0.1657, -0.8028]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0115,  0.2058, -0.1657, -0.8028],\n",
      "        [ 0.0156,  0.4028, -0.1818, -1.1427]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.1592, 2.3385]) - torch.Size([2])\n",
      "Q-values tensor([[1.0413, 1.0242],\n",
      "        [1.2100, 1.1368]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0242],\n",
      "        [1.1368]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0156,  0.4028, -0.1818, -1.1427]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.02362316  0.21045782 -0.20464362 -0.912047  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.3398, 1.2394],\n",
      "        [1.2120, 1.1401]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.3398, 1.2120]) - torch.Size([2])\n",
      "P2 tensor([1.3385, 1.2108]) - torch.Size([2])\n",
      "P3 tensor([1.3385, 1.2108]) - torch.Size([2])\n",
      "P4 tensor([2.3385, 2.2108]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0115,  0.2058, -0.1657, -0.8028],\n",
      "        [ 0.0156,  0.4028, -0.1818, -1.1427]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0156,  0.4028, -0.1818, -1.1427],\n",
      "        [ 0.0236,  0.2105, -0.2046, -0.9120]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.3385, 2.2108]) - torch.Size([2])\n",
      "Q-values tensor([[1.2116, 1.1481],\n",
      "        [1.4009, 1.2937]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1481],\n",
      "        [1.4009]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0236,  0.2105, -0.2046, -0.9120]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.02783231  0.01860432 -0.22288455 -0.6900179 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.2120, 1.1401],\n",
      "        [1.0992, 1.0608]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.2120, 1.0992]) - torch.Size([2])\n",
      "P2 tensor([1.2108, 1.0981]) - torch.Size([2])\n",
      "P3 tensor([1.2108, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([2.2108, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0156,  0.4028, -0.1818, -1.1427],\n",
      "        [ 0.0236,  0.2105, -0.2046, -0.9120]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0236,  0.2105, -0.2046, -0.9120],\n",
      "        [ 0.0278,  0.0186, -0.2229, -0.6900]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.2108, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[1.4124, 1.3036],\n",
      "        [1.2756, 1.2002]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.4124],\n",
      "        [1.2756]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0086, -0.0181,  0.0379,  0.0123]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00892774  0.17646565  0.03814759 -0.2682155 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.0992, 1.0608],\n",
      "        [1.2120, 1.1401]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0992, 1.2120]) - torch.Size([2])\n",
      "P2 tensor([1.0981, 1.2108]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 1.2108]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 2.2108]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0236,  0.2105, -0.2046, -0.9120],\n",
      "        [ 0.0156,  0.4028, -0.1818, -1.1427]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0278,  0.0186, -0.2229, -0.6900],\n",
      "        [ 0.0236,  0.2105, -0.2046, -0.9120]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 2.2108]) - torch.Size([2])\n",
      "Q-values tensor([[1.2874, 1.2023],\n",
      "        [1.4268, 1.3053]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.2874],\n",
      "        [1.4268]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0089,  0.1765,  0.0381, -0.2682]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00539843 -0.01917937  0.03278328  0.03625118] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9460, 0.9277],\n",
      "        [1.0992, 1.0608]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9460, 1.0992]) - torch.Size([2])\n",
      "P2 tensor([0.9450, 1.0981]) - torch.Size([2])\n",
      "P3 tensor([0.9450, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.9450, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0086, -0.0181,  0.0379,  0.0123],\n",
      "        [ 0.0236,  0.2105, -0.2046, -0.9120]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0089,  0.1765,  0.0381, -0.2682],\n",
      "        [ 0.0278,  0.0186, -0.2229, -0.6900]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.9450, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[0.8957, 0.9019],\n",
      "        [1.2991, 1.2046]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9019],\n",
      "        [1.2991]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0054, -0.0192,  0.0328,  0.0363]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00578201 -0.21475573  0.0335083   0.33909464] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7621, 0.8270],\n",
      "        [0.9460, 0.9277]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8270, 0.9460]) - torch.Size([2])\n",
      "P2 tensor([0.8262, 0.9450]) - torch.Size([2])\n",
      "P3 tensor([0.8262, 0.9450]) - torch.Size([2])\n",
      "P4 tensor([1.8262, 1.9450]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0054, -0.0192,  0.0328,  0.0363],\n",
      "        [-0.0086, -0.0181,  0.0379,  0.0123]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0058, -0.2148,  0.0335,  0.3391],\n",
      "        [-0.0089,  0.1765,  0.0381, -0.2682]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8262, 1.9450]) - torch.Size([2])\n",
      "Q-values tensor([[0.8839, 0.9020],\n",
      "        [0.8929, 0.9089]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8839],\n",
      "        [0.9089]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0058, -0.2148,  0.0335,  0.3391]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01007713 -0.0201262   0.0402902   0.05716384] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7621, 0.8270],\n",
      "        [0.8377, 0.8532]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8270, 0.8532]) - torch.Size([2])\n",
      "P2 tensor([0.8262, 0.8523]) - torch.Size([2])\n",
      "P3 tensor([0.8262, 0.8523]) - torch.Size([2])\n",
      "P4 tensor([1.8262, 1.8523]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0054, -0.0192,  0.0328,  0.0363],\n",
      "        [-0.0089,  0.1765,  0.0381, -0.2682]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0058, -0.2148,  0.0335,  0.3391],\n",
      "        [-0.0054, -0.0192,  0.0328,  0.0363]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8262, 1.8523]) - torch.Size([2])\n",
      "Q-values tensor([[0.8898, 0.9081],\n",
      "        [1.0071, 0.9883]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8898],\n",
      "        [1.0071]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0101, -0.0201,  0.0403,  0.0572]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01047965  0.17439559  0.04143347 -0.22253995] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7621, 0.8270],\n",
      "        [0.8327, 0.8499]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8270, 0.8499]) - torch.Size([2])\n",
      "P2 tensor([0.8262, 0.8491]) - torch.Size([2])\n",
      "P3 tensor([0.8262, 0.8491]) - torch.Size([2])\n",
      "P4 tensor([1.8262, 1.8491]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0054, -0.0192,  0.0328,  0.0363],\n",
      "        [-0.0058, -0.2148,  0.0335,  0.3391]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0058, -0.2148,  0.0335,  0.3391],\n",
      "        [-0.0101, -0.0201,  0.0403,  0.0572]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8262, 1.8491]) - torch.Size([2])\n",
      "Q-values tensor([[0.8974, 0.9091],\n",
      "        [0.8149, 0.8782]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.8974],\n",
      "        [0.8782]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0105,  0.1744,  0.0414, -0.2225]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00699174 -0.02129333  0.03698267  0.08291947] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8247, 0.8439],\n",
      "        [0.9306, 0.9138]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8439, 0.9306]) - torch.Size([2])\n",
      "P2 tensor([0.8431, 0.9297]) - torch.Size([2])\n",
      "P3 tensor([0.8431, 0.9297]) - torch.Size([2])\n",
      "P4 tensor([1.8431, 1.9297]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0105,  0.1744,  0.0414, -0.2225],\n",
      "        [-0.0101, -0.0201,  0.0403,  0.0572]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0070, -0.0213,  0.0370,  0.0829],\n",
      "        [-0.0105,  0.1744,  0.0414, -0.2225]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8431, 1.9297]) - torch.Size([2])\n",
      "Q-values tensor([[1.0058, 0.9798],\n",
      "        [0.8982, 0.9104]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0058],\n",
      "        [0.9104]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0070, -0.0213,  0.0370,  0.0829]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00741761  0.17327951  0.03864106 -0.1978698 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8247, 0.8439],\n",
      "        [0.9217, 0.9061]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8439, 0.9217]) - torch.Size([2])\n",
      "P2 tensor([0.8431, 0.9208]) - torch.Size([2])\n",
      "P3 tensor([0.8431, 0.9208]) - torch.Size([2])\n",
      "P4 tensor([1.8431, 1.9208]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0105,  0.1744,  0.0414, -0.2225],\n",
      "        [-0.0070, -0.0213,  0.0370,  0.0829]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0070, -0.0213,  0.0370,  0.0829],\n",
      "        [-0.0074,  0.1733,  0.0386, -0.1979]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8431, 1.9208]) - torch.Size([2])\n",
      "Q-values tensor([[1.0125, 0.9860],\n",
      "        [0.8951, 0.9100]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0125],\n",
      "        [0.9100]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0074,  0.1733,  0.0386, -0.1979]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00395202 -0.02237323  0.03468367  0.10674787] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9217, 0.9061],\n",
      "        [0.8179, 0.8389]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9217, 0.8389]) - torch.Size([2])\n",
      "P2 tensor([0.9208, 0.8380]) - torch.Size([2])\n",
      "P3 tensor([0.9208, 0.8380]) - torch.Size([2])\n",
      "P4 tensor([1.9208, 1.8380]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0070, -0.0213,  0.0370,  0.0829],\n",
      "        [-0.0074,  0.1733,  0.0386, -0.1979]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0074,  0.1733,  0.0386, -0.1979],\n",
      "        [-0.0040, -0.0224,  0.0347,  0.1067]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.9208, 1.8380]) - torch.Size([2])\n",
      "Q-values tensor([[0.9007, 0.9161],\n",
      "        [1.0093, 0.9838]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9161],\n",
      "        [1.0093]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0040, -0.0224,  0.0347,  0.1067]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00439948  0.17223497  0.03681862 -0.1747939 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9138, 0.8988],\n",
      "        [0.9217, 0.9061]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9138, 0.9217]) - torch.Size([2])\n",
      "P2 tensor([0.9129, 0.9208]) - torch.Size([2])\n",
      "P3 tensor([0.9129, 0.9208]) - torch.Size([2])\n",
      "P4 tensor([1.9129, 1.9208]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0040, -0.0224,  0.0347,  0.1067],\n",
      "        [-0.0070, -0.0213,  0.0370,  0.0829]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0044,  0.1722,  0.0368, -0.1748],\n",
      "        [-0.0074,  0.1733,  0.0386, -0.1979]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.9129, 1.9208]) - torch.Size([2])\n",
      "Q-values tensor([[0.8987, 0.9166],\n",
      "        [0.9063, 0.9223]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9166],\n",
      "        [0.9223]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0044,  0.1722,  0.0368, -0.1748]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00095478 -0.02339404  0.03332274  0.12927297] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8117, 0.8342],\n",
      "        [0.9138, 0.8988]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8342, 0.9138]) - torch.Size([2])\n",
      "P2 tensor([0.8334, 0.9129]) - torch.Size([2])\n",
      "P3 tensor([0.8334, 0.9129]) - torch.Size([2])\n",
      "P4 tensor([1.8334, 1.9129]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0044,  0.1722,  0.0368, -0.1748],\n",
      "        [-0.0040, -0.0224,  0.0347,  0.1067]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0010, -0.0234,  0.0333,  0.1293],\n",
      "        [-0.0044,  0.1722,  0.0368, -0.1748]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8334, 1.9129]) - torch.Size([2])\n",
      "Q-values tensor([[1.0084, 0.9898],\n",
      "        [0.9001, 0.9241]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0084],\n",
      "        [0.9241]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0010, -0.0234,  0.0333,  0.1293]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00142266 -0.21897711  0.0359082   0.43227977] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9138, 0.8988],\n",
      "        [0.7458, 0.8167]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9138, 0.8167]) - torch.Size([2])\n",
      "P2 tensor([0.9129, 0.8158]) - torch.Size([2])\n",
      "P3 tensor([0.9129, 0.8158]) - torch.Size([2])\n",
      "P4 tensor([1.9129, 1.8158]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0040, -0.0224,  0.0347,  0.1067],\n",
      "        [-0.0010, -0.0234,  0.0333,  0.1293]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0044,  0.1722,  0.0368, -0.1748],\n",
      "        [-0.0014, -0.2190,  0.0359,  0.4323]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.9129, 1.8158]) - torch.Size([2])\n",
      "Q-values tensor([[0.9056, 0.9303],\n",
      "        [0.8986, 0.9251]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9303],\n",
      "        [0.8986]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0014, -0.2190,  0.0359,  0.4323]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0058022  -0.0243815   0.0445538   0.15112926] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8073, 0.8318],\n",
      "        [0.8117, 0.8342]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8318, 0.8342]) - torch.Size([2])\n",
      "P2 tensor([0.8310, 0.8334]) - torch.Size([2])\n",
      "P3 tensor([0.8310, 0.8334]) - torch.Size([2])\n",
      "P4 tensor([1.8310, 1.8334]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0014, -0.2190,  0.0359,  0.4323],\n",
      "        [-0.0044,  0.1722,  0.0368, -0.1748]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0058, -0.0244,  0.0446,  0.1511],\n",
      "        [-0.0010, -0.0234,  0.0333,  0.1293]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8310, 1.8334]) - torch.Size([2])\n",
      "Q-values tensor([[0.8292, 0.9081],\n",
      "        [1.0214, 1.0023]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9081],\n",
      "        [1.0214]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0058, -0.0244,  0.0446,  0.1511]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00628983  0.1700751   0.04757639 -0.1271717 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8993, 0.8851],\n",
      "        [0.8073, 0.8318]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8993, 0.8318]) - torch.Size([2])\n",
      "P2 tensor([0.8984, 0.8310]) - torch.Size([2])\n",
      "P3 tensor([0.8984, 0.8310]) - torch.Size([2])\n",
      "P4 tensor([1.8984, 1.8310]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0058, -0.0244,  0.0446,  0.1511],\n",
      "        [-0.0014, -0.2190,  0.0359,  0.4323]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0063,  0.1701,  0.0476, -0.1272],\n",
      "        [-0.0058, -0.0244,  0.0446,  0.1511]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8984, 1.8310]) - torch.Size([2])\n",
      "Q-values tensor([[0.9056, 0.9339],\n",
      "        [0.8345, 0.9145]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9339],\n",
      "        [0.9145]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0063,  0.1701,  0.0476, -0.1272]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00288833 -0.02569499  0.04503295  0.18013334] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.8073, 0.8318],\n",
      "        [0.8993, 0.8851]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8318, 0.8993]) - torch.Size([2])\n",
      "P2 tensor([0.8310, 0.8984]) - torch.Size([2])\n",
      "P3 tensor([0.8310, 0.8984]) - torch.Size([2])\n",
      "P4 tensor([1.8310, 1.8984]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0014, -0.2190,  0.0359,  0.4323],\n",
      "        [-0.0058, -0.0244,  0.0446,  0.1511]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0058, -0.0244,  0.0446,  0.1511],\n",
      "        [-0.0063,  0.1701,  0.0476, -0.1272]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8310, 1.8984]) - torch.Size([2])\n",
      "Q-values tensor([[0.8363, 0.9218],\n",
      "        [0.9071, 0.9414]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9218],\n",
      "        [0.9414]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0029, -0.0257,  0.0450,  0.1801]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00340223  0.16875459  0.04863562 -0.09801011] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7995, 0.8271],\n",
      "        [0.8908, 0.8773]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8271, 0.8908]) - torch.Size([2])\n",
      "P2 tensor([0.8263, 0.8899]) - torch.Size([2])\n",
      "P3 tensor([0.8263, 0.8899]) - torch.Size([2])\n",
      "P4 tensor([1.8263, 1.8899]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0063,  0.1701,  0.0476, -0.1272],\n",
      "        [-0.0029, -0.0257,  0.0450,  0.1801]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0029, -0.0257,  0.0450,  0.1801],\n",
      "        [-0.0034,  0.1688,  0.0486, -0.0980]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8263, 1.8899]) - torch.Size([2])\n",
      "Q-values tensor([[1.0147, 1.0079],\n",
      "        [0.8999, 0.9427]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0147],\n",
      "        [0.9427]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0034,  0.1688,  0.0486, -0.0980]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-2.7140100e-05 -2.7029455e-02  4.6675418e-02  2.0961192e-01] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7923, 0.8229],\n",
      "        [0.8908, 0.8773]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8229, 0.8908]) - torch.Size([2])\n",
      "P2 tensor([0.8221, 0.8899]) - torch.Size([2])\n",
      "P3 tensor([0.8221, 0.8899]) - torch.Size([2])\n",
      "P4 tensor([1.8221, 1.8899]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0034,  0.1688,  0.0486, -0.0980],\n",
      "        [-0.0029, -0.0257,  0.0450,  0.1801]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-2.7140e-05, -2.7029e-02,  4.6675e-02,  2.0961e-01],\n",
      "        [-3.4022e-03,  1.6875e-01,  4.8636e-02, -9.8010e-02]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8221, 1.8899]) - torch.Size([2])\n",
      "Q-values tensor([[1.0112, 1.0053],\n",
      "        [0.9056, 0.9488]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0112],\n",
      "        [0.9488]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-2.7140e-05, -2.7029e-02,  4.6675e-02,  2.0961e-01]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00056773 -0.22278665  0.05086765  0.5166455 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7923, 0.8229],\n",
      "        [0.8908, 0.8773]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8229, 0.8908]) - torch.Size([2])\n",
      "P2 tensor([0.8221, 0.8899]) - torch.Size([2])\n",
      "P3 tensor([0.8221, 0.8899]) - torch.Size([2])\n",
      "P4 tensor([1.8221, 1.8899]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0034,  0.1688,  0.0486, -0.0980],\n",
      "        [-0.0029, -0.0257,  0.0450,  0.1801]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-2.7140e-05, -2.7029e-02,  4.6675e-02,  2.0961e-01],\n",
      "        [-3.4022e-03,  1.6875e-01,  4.8636e-02, -9.8010e-02]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8221, 1.8899]) - torch.Size([2])\n",
      "Q-values tensor([[1.0178, 1.0114],\n",
      "        [0.9115, 0.9550]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0178],\n",
      "        [0.9550]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0006, -0.2228,  0.0509,  0.5166]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00502346 -0.02841648  0.06120057  0.24041602] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7361, 0.8117],\n",
      "        [0.7866, 0.8209]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8117, 0.8209]) - torch.Size([2])\n",
      "P2 tensor([0.8109, 0.8201]) - torch.Size([2])\n",
      "P3 tensor([0.8109, 0.8201]) - torch.Size([2])\n",
      "P4 tensor([1.8109, 1.8201]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-2.7140e-05, -2.7029e-02,  4.6675e-02,  2.0961e-01],\n",
      "        [-5.6773e-04, -2.2279e-01,  5.0868e-02,  5.1665e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0006, -0.2228,  0.0509,  0.5166],\n",
      "        [-0.0050, -0.0284,  0.0612,  0.2404]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8109, 1.8201]) - torch.Size([2])\n",
      "Q-values tensor([[0.9093, 0.9554],\n",
      "        [0.8424, 0.9414]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9093],\n",
      "        [0.9414]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0050, -0.0284,  0.0612,  0.2404]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00559179  0.16578025  0.06600889 -0.03235199] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7361, 0.8117],\n",
      "        [0.8738, 0.8615]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8117, 0.8738]) - torch.Size([2])\n",
      "P2 tensor([0.8109, 0.8730]) - torch.Size([2])\n",
      "P3 tensor([0.8109, 0.8730]) - torch.Size([2])\n",
      "P4 tensor([1.8109, 1.8730]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-2.7140e-05, -2.7029e-02,  4.6675e-02,  2.0961e-01],\n",
      "        [-5.0235e-03, -2.8416e-02,  6.1201e-02,  2.4042e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0006, -0.2228,  0.0509,  0.5166],\n",
      "        [-0.0056,  0.1658,  0.0660, -0.0324]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8109, 1.8730]) - torch.Size([2])\n",
      "Q-values tensor([[0.9157, 0.9609],\n",
      "        [0.9092, 0.9583]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9157],\n",
      "        [0.9583]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0056,  0.1658,  0.0660, -0.0324]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00227619  0.35989654  0.06536184 -0.3034998 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9701, 0.9339],\n",
      "        [0.8738, 0.8615]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9701, 0.8738]) - torch.Size([2])\n",
      "P2 tensor([0.9691, 0.8730]) - torch.Size([2])\n",
      "P3 tensor([0.9691, 0.8730]) - torch.Size([2])\n",
      "P4 tensor([1.9691, 1.8730]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0056,  0.1658,  0.0660, -0.0324],\n",
      "        [-0.0050, -0.0284,  0.0612,  0.2404]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0023,  0.3599,  0.0654, -0.3035],\n",
      "        [-0.0056,  0.1658,  0.0660, -0.0324]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.9691, 1.8730]) - torch.Size([2])\n",
      "Q-values tensor([[1.0176, 1.0125],\n",
      "        [0.9155, 0.9642]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0125],\n",
      "        [0.9642]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0023,  0.3599,  0.0654, -0.3035]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00492174  0.55402905  0.05929185 -0.57487404] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9701, 0.9339],\n",
      "        [0.8738, 0.8615]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.9701, 0.8738]) - torch.Size([2])\n",
      "P2 tensor([0.9691, 0.8730]) - torch.Size([2])\n",
      "P3 tensor([0.9691, 0.8730]) - torch.Size([2])\n",
      "P4 tensor([1.9691, 1.8730]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0056,  0.1658,  0.0660, -0.0324],\n",
      "        [-0.0050, -0.0284,  0.0612,  0.2404]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0023,  0.3599,  0.0654, -0.3035],\n",
      "        [-0.0056,  0.1658,  0.0660, -0.0324]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.9691, 1.8730]) - torch.Size([2])\n",
      "Q-values tensor([[1.0191, 1.0207],\n",
      "        [0.9171, 0.9716]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0207],\n",
      "        [0.9716]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0049,  0.5540,  0.0593, -0.5749]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01600233  0.7482719   0.04779437 -0.8483047 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.0906, 1.0241],\n",
      "        [1.2240, 1.1358]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0906, 1.2240]) - torch.Size([2])\n",
      "P2 tensor([1.0895, 1.2227]) - torch.Size([2])\n",
      "P3 tensor([1.0895, 1.2227]) - torch.Size([2])\n",
      "P4 tensor([2.0895, 2.2227]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0023,  0.3599,  0.0654, -0.3035],\n",
      "        [ 0.0049,  0.5540,  0.0593, -0.5749]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0049,  0.5540,  0.0593, -0.5749],\n",
      "        [ 0.0160,  0.7483,  0.0478, -0.8483]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0895, 2.2227]) - torch.Size([2])\n",
      "Q-values tensor([[1.1353, 1.1109],\n",
      "        [1.2775, 1.2156]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1109],\n",
      "        [1.2156]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0160,  0.7483,  0.0478, -0.8483]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.03096776  0.94271046  0.03082827 -1.1255834 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.2240, 1.1358],\n",
      "        [1.0906, 1.0241]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.2240, 1.0906]) - torch.Size([2])\n",
      "P2 tensor([1.2227, 1.0895]) - torch.Size([2])\n",
      "P3 tensor([1.2227, 1.0895]) - torch.Size([2])\n",
      "P4 tensor([2.2227, 2.0895]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0049,  0.5540,  0.0593, -0.5749],\n",
      "        [-0.0023,  0.3599,  0.0654, -0.3035]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0160,  0.7483,  0.0478, -0.8483],\n",
      "        [ 0.0049,  0.5540,  0.0593, -0.5749]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.2227, 2.0895]) - torch.Size([2])\n",
      "Q-values tensor([[1.2798, 1.2262],\n",
      "        [1.1370, 1.1201]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.2262],\n",
      "        [1.1201]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0310,  0.9427,  0.0308, -1.1256]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04982197  0.7471984   0.00831661 -0.82339245] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.2067, 1.1248],\n",
      "        [1.3708, 1.2581]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.2067, 1.3708]) - torch.Size([2])\n",
      "P2 tensor([1.2055, 1.3694]) - torch.Size([2])\n",
      "P3 tensor([1.2055, 1.3694]) - torch.Size([2])\n",
      "P4 tensor([2.2055, 2.3694]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0310,  0.9427,  0.0308, -1.1256],\n",
      "        [ 0.0160,  0.7483,  0.0478, -0.8483]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0498,  0.7472,  0.0083, -0.8234],\n",
      "        [ 0.0310,  0.9427,  0.0308, -1.1256]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.2055, 2.3694]) - torch.Size([2])\n",
      "Q-values tensor([[1.6163, 1.5134],\n",
      "        [1.4422, 1.3686]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.6163],\n",
      "        [1.3686]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0498,  0.7472,  0.0083, -0.8234]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06476594  0.9422056  -0.00815124 -1.1134481 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.3708, 1.2581],\n",
      "        [1.2067, 1.1248]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.3708, 1.2067]) - torch.Size([2])\n",
      "P2 tensor([1.3694, 1.2055]) - torch.Size([2])\n",
      "P3 tensor([1.3694, 1.2055]) - torch.Size([2])\n",
      "P4 tensor([2.3694, 2.2055]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0160,  0.7483,  0.0478, -0.8483],\n",
      "        [ 0.0310,  0.9427,  0.0308, -1.1256]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0310,  0.9427,  0.0308, -1.1256],\n",
      "        [ 0.0498,  0.7472,  0.0083, -0.8234]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.3694, 2.2055]) - torch.Size([2])\n",
      "Q-values tensor([[1.4509, 1.3797],\n",
      "        [1.6267, 1.5263]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.3797],\n",
      "        [1.6267]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0648,  0.9422, -0.0082, -1.1134]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.08361005  1.1374336  -0.0304202  -1.4086769 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.3589, 1.2538],\n",
      "        [1.5281, 1.4003]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.3589, 1.5281]) - torch.Size([2])\n",
      "P2 tensor([1.3575, 1.5266]) - torch.Size([2])\n",
      "P3 tensor([1.3575, 1.5266]) - torch.Size([2])\n",
      "P4 tensor([2.3575, 2.5266]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0498,  0.7472,  0.0083, -0.8234],\n",
      "        [ 0.0648,  0.9422, -0.0082, -1.1134]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0648,  0.9422, -0.0082, -1.1134],\n",
      "        [ 0.0836,  1.1374, -0.0304, -1.4087]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.3575, 2.5266]) - torch.Size([2])\n",
      "Q-values tensor([[1.4402, 1.3752],\n",
      "        [1.6239, 1.5315]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.3752],\n",
      "        [1.5315]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0836,  1.1374, -0.0304, -1.4087]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.10635872  0.942702   -0.05859374 -1.1256567 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.3589, 1.2538],\n",
      "        [1.5281, 1.4003]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.3589, 1.5281]) - torch.Size([2])\n",
      "P2 tensor([1.3575, 1.5266]) - torch.Size([2])\n",
      "P3 tensor([1.3575, 1.5266]) - torch.Size([2])\n",
      "P4 tensor([2.3575, 2.5266]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0498,  0.7472,  0.0083, -0.8234],\n",
      "        [ 0.0648,  0.9422, -0.0082, -1.1134]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0648,  0.9422, -0.0082, -1.1134],\n",
      "        [ 0.0836,  1.1374, -0.0304, -1.4087]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.3575, 2.5266]) - torch.Size([2])\n",
      "Q-values tensor([[1.4429, 1.3877],\n",
      "        [1.6271, 1.5465]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.3877],\n",
      "        [1.5465]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1064,  0.9427, -0.0586, -1.1257]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.12521276  1.1385407  -0.08110687 -1.4361275 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.3610, 1.2629],\n",
      "        [1.5395, 1.4193]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.3610, 1.5395]) - torch.Size([2])\n",
      "P2 tensor([1.3596, 1.5380]) - torch.Size([2])\n",
      "P3 tensor([1.3596, 1.5380]) - torch.Size([2])\n",
      "P4 tensor([2.3596, 2.5380]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0836,  1.1374, -0.0304, -1.4087],\n",
      "        [ 0.1064,  0.9427, -0.0586, -1.1257]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1064,  0.9427, -0.0586, -1.1257],\n",
      "        [ 0.1252,  1.1385, -0.0811, -1.4361]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.3596, 2.5380]) - torch.Size([2])\n",
      "Q-values tensor([[1.8342, 1.7420],\n",
      "        [1.6332, 1.5711]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.8342],\n",
      "        [1.5711]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1252,  1.1385, -0.0811, -1.4361]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.14798358  0.9445071  -0.10982943 -1.1698531 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.5395, 1.4193],\n",
      "        [1.3610, 1.2629]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.5395, 1.3610]) - torch.Size([2])\n",
      "P2 tensor([1.5380, 1.3596]) - torch.Size([2])\n",
      "P3 tensor([1.5380, 1.3596]) - torch.Size([2])\n",
      "P4 tensor([2.5380, 2.3596]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1064,  0.9427, -0.0586, -1.1257],\n",
      "        [ 0.0836,  1.1374, -0.0304, -1.4087]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1252,  1.1385, -0.0811, -1.4361],\n",
      "        [ 0.1064,  0.9427, -0.0586, -1.1257]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.5380, 2.3596]) - torch.Size([2])\n",
      "Q-values tensor([[1.6432, 1.5846],\n",
      "        [1.8463, 1.7569]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.5846],\n",
      "        [1.8463]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1480,  0.9445, -0.1098, -1.1699]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.16687372  1.1408726  -0.13322648 -1.4948515 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.5678, 1.4517],\n",
      "        [1.5395, 1.4193]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.5678, 1.5395]) - torch.Size([2])\n",
      "P2 tensor([1.5662, 1.5380]) - torch.Size([2])\n",
      "P3 tensor([1.5662, 1.5380]) - torch.Size([2])\n",
      "P4 tensor([2.5662, 2.5380]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1480,  0.9445, -0.1098, -1.1699],\n",
      "        [ 0.1064,  0.9427, -0.0586, -1.1257]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1669,  1.1409, -0.1332, -1.4949],\n",
      "        [ 0.1252,  1.1385, -0.0811, -1.4361]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.5662, 2.5380]) - torch.Size([2])\n",
      "Q-values tensor([[1.6796, 1.6268],\n",
      "        [1.6531, 1.5981]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.6268],\n",
      "        [1.5981]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|郊         | 15/1000 [00:02<03:06,  5.28 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States tensor([[ 0.1669,  1.1409, -0.1332, -1.4949]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.18969117  0.9475987  -0.16312352 -1.2465626 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.5678, 1.4517],\n",
      "        [1.3813, 1.2878]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.5678, 1.3813]) - torch.Size([2])\n",
      "P2 tensor([1.5662, 1.3799]) - torch.Size([2])\n",
      "P3 tensor([1.5662, 1.3799]) - torch.Size([2])\n",
      "P4 tensor([2.5662, 2.3799]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1480,  0.9445, -0.1098, -1.1699],\n",
      "        [ 0.1252,  1.1385, -0.0811, -1.4361]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1669,  1.1409, -0.1332, -1.4949],\n",
      "        [ 0.1480,  0.9445, -0.1098, -1.1699]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.5662, 2.3799]) - torch.Size([2])\n",
      "Q-values tensor([[1.6825, 1.6421],\n",
      "        [1.8766, 1.8094]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.6421],\n",
      "        [1.8766]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1897,  0.9476, -0.1631, -1.2466]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.20864315  0.7549006  -0.18805477 -1.0090984 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.5678, 1.4517],\n",
      "        [1.2805, 1.2114]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.5678, 1.2805]) - torch.Size([2])\n",
      "P2 tensor([1.5662, 1.2792]) - torch.Size([2])\n",
      "P3 tensor([1.5662, 1.2792]) - torch.Size([2])\n",
      "P4 tensor([2.5662, 2.2792]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1480,  0.9445, -0.1098, -1.1699],\n",
      "        [ 0.1897,  0.9476, -0.1631, -1.2466]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1669,  1.1409, -0.1332, -1.4949],\n",
      "        [ 0.2086,  0.7549, -0.1881, -1.0091]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.5662, 2.2792]) - torch.Size([2])\n",
      "Q-values tensor([[1.6929, 1.6559],\n",
      "        [1.7414, 1.7034]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.6559],\n",
      "        [1.7414]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.2086,  0.7549, -0.1881, -1.0091]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.22374116  0.9519667  -0.20823674 -1.3544489 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.4191, 1.3280],\n",
      "        [1.2805, 1.2114]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.4191, 1.2805]) - torch.Size([2])\n",
      "P2 tensor([1.4177, 1.2792]) - torch.Size([2])\n",
      "P3 tensor([1.4177, 1.2792]) - torch.Size([2])\n",
      "P4 tensor([2.4177, 2.2792]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1669,  1.1409, -0.1332, -1.4949],\n",
      "        [ 0.1897,  0.9476, -0.1631, -1.2466]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1897,  0.9476, -0.1631, -1.2466],\n",
      "        [ 0.2086,  0.7549, -0.1881, -1.0091]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.4177, 2.2792]) - torch.Size([2])\n",
      "Q-values tensor([[1.9373, 1.8789],\n",
      "        [1.7526, 1.7179]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.9373],\n",
      "        [1.7526]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.2237,  0.9520, -0.2082, -1.3544]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.24278049  1.1490033  -0.23532571 -1.704399  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.6733, 1.5553],\n",
      "        [1.2805, 1.2114]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.6733, 1.2805]) - torch.Size([2])\n",
      "P2 tensor([1.6716, 1.2792]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 1.2792]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 2.2792]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.2237,  0.9520, -0.2082, -1.3544],\n",
      "        [ 0.1897,  0.9476, -0.1631, -1.2466]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.2428,  1.1490, -0.2353, -1.7044],\n",
      "        [ 0.2086,  0.7549, -0.1881, -1.0091]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 2.2792]) - torch.Size([2])\n",
      "Q-values tensor([[1.8395, 1.7860],\n",
      "        [1.7691, 1.7205]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.7860],\n",
      "        [1.7691]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0108, -0.0129, -0.0270,  0.0251]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01102804 -0.2076455  -0.02650051  0.3091771 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7584, 0.8200],\n",
      "        [1.6733, 1.5553]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8200, 1.6733]) - torch.Size([2])\n",
      "P2 tensor([0.8192, 1.6716]) - torch.Size([2])\n",
      "P3 tensor([0.8192, 0.0000]) - torch.Size([2])\n",
      "P4 tensor([1.8192, 1.0000]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0108, -0.0129, -0.0270,  0.0251],\n",
      "        [ 0.2237,  0.9520, -0.2082, -1.3544]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 1.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0110, -0.2076, -0.0265,  0.3092],\n",
      "        [ 0.2428,  1.1490, -0.2353, -1.7044]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8192, 1.0000]) - torch.Size([2])\n",
      "Q-values tensor([[1.0198, 1.0798],\n",
      "        [1.8470, 1.7711]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0198],\n",
      "        [1.7711]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0110, -0.2076, -0.0265,  0.3092]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01518095 -0.40238005 -0.02031697  0.5933861 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7584, 0.8200],\n",
      "        [0.7270, 0.8262]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8200, 0.8262]) - torch.Size([2])\n",
      "P2 tensor([0.8192, 0.8254]) - torch.Size([2])\n",
      "P3 tensor([0.8192, 0.8254]) - torch.Size([2])\n",
      "P4 tensor([1.8192, 1.8254]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0108, -0.0129, -0.0270,  0.0251],\n",
      "        [-0.0110, -0.2076, -0.0265,  0.3092]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0110, -0.2076, -0.0265,  0.3092],\n",
      "        [-0.0152, -0.4024, -0.0203,  0.5934]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8192, 1.8254]) - torch.Size([2])\n",
      "Q-values tensor([[1.0254, 1.0749],\n",
      "        [0.9299, 1.0324]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0254],\n",
      "        [0.9299]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0152, -0.4024, -0.0203,  0.5934]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02322855 -0.5972118  -0.00844925  0.87960064] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7270, 0.8262],\n",
      "        [0.7584, 0.8200]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8262, 0.8200]) - torch.Size([2])\n",
      "P2 tensor([0.8254, 0.8192]) - torch.Size([2])\n",
      "P3 tensor([0.8254, 0.8192]) - torch.Size([2])\n",
      "P4 tensor([1.8254, 1.8192]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0110, -0.2076, -0.0265,  0.3092],\n",
      "        [-0.0108, -0.0129, -0.0270,  0.0251]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0152, -0.4024, -0.0203,  0.5934],\n",
      "        [-0.0110, -0.2076, -0.0265,  0.3092]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8254, 1.8192]) - torch.Size([2])\n",
      "Q-values tensor([[0.9376, 1.0341],\n",
      "        [1.0336, 1.0760]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9376],\n",
      "        [1.0336]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0232, -0.5972, -0.0084,  0.8796]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03517279 -0.40197608  0.00914277  0.58427346] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7270, 0.8262],\n",
      "        [0.7351, 0.8608]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8262, 0.8608]) - torch.Size([2])\n",
      "P2 tensor([0.8254, 0.8599]) - torch.Size([2])\n",
      "P3 tensor([0.8254, 0.8599]) - torch.Size([2])\n",
      "P4 tensor([1.8254, 1.8599]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0110, -0.2076, -0.0265,  0.3092],\n",
      "        [-0.0152, -0.4024, -0.0203,  0.5934]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0152, -0.4024, -0.0203,  0.5934],\n",
      "        [-0.0232, -0.5972, -0.0084,  0.8796]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8254, 1.8599]) - torch.Size([2])\n",
      "Q-values tensor([[0.9454, 1.0357],\n",
      "        [0.9043, 1.0348]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9454],\n",
      "        [0.9043]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0352, -0.4020,  0.0091,  0.5843]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04321231 -0.5972249   0.02082823  0.8798224 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7368, 0.8643],\n",
      "        [0.7351, 0.8608]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8643, 0.8608]) - torch.Size([2])\n",
      "P2 tensor([0.8635, 0.8599]) - torch.Size([2])\n",
      "P3 tensor([0.8635, 0.8599]) - torch.Size([2])\n",
      "P4 tensor([1.8635, 1.8599]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0352, -0.4020,  0.0091,  0.5843],\n",
      "        [-0.0152, -0.4024, -0.0203,  0.5934]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0432, -0.5972,  0.0208,  0.8798],\n",
      "        [-0.0232, -0.5972, -0.0084,  0.8796]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8635, 1.8599]) - torch.Size([2])\n",
      "Q-values tensor([[0.9141, 1.0412],\n",
      "        [0.9122, 1.0368]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9141],\n",
      "        [0.9122]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0432, -0.5972,  0.0208,  0.8798]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.05515681 -0.40239203  0.03842468  0.59375954] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7368, 0.8643],\n",
      "        [0.7279, 0.8321]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8643, 0.8321]) - torch.Size([2])\n",
      "P2 tensor([0.8635, 0.8312]) - torch.Size([2])\n",
      "P3 tensor([0.8635, 0.8312]) - torch.Size([2])\n",
      "P4 tensor([1.8635, 1.8312]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0352, -0.4020,  0.0091,  0.5843],\n",
      "        [-0.0432, -0.5972,  0.0208,  0.8798]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0432, -0.5972,  0.0208,  0.8798],\n",
      "        [-0.0552, -0.4024,  0.0384,  0.5938]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8635, 1.8312]) - torch.Size([2])\n",
      "Q-values tensor([[0.9220, 1.0432],\n",
      "        [0.9319, 1.0825]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[0.9220],\n",
      "        [1.0825]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0552, -0.4024,  0.0384,  0.5938]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06320465 -0.20782839  0.05029987  0.31342357] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.7676, 0.8320],\n",
      "        [0.7279, 0.8321]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([0.8320, 0.8321]) - torch.Size([2])\n",
      "P2 tensor([0.8312, 0.8312]) - torch.Size([2])\n",
      "P3 tensor([0.8312, 0.8312]) - torch.Size([2])\n",
      "P4 tensor([1.8312, 1.8312]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0552, -0.4024,  0.0384,  0.5938],\n",
      "        [-0.0432, -0.5972,  0.0208,  0.8798]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0632, -0.2078,  0.0503,  0.3134],\n",
      "        [-0.0552, -0.4024,  0.0384,  0.5938]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.8312, 1.8312]) - torch.Size([2])\n",
      "Q-values tensor([[0.9288, 1.0530],\n",
      "        [0.9391, 1.0903]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0530],\n",
      "        [1.0903]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0632, -0.2078,  0.0503,  0.3134]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06736121 -0.01345773  0.05656834  0.03701826] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9806, 1.0680],\n",
      "        [1.0762, 1.1054]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0680, 1.1054]) - torch.Size([2])\n",
      "P2 tensor([1.0670, 1.1043]) - torch.Size([2])\n",
      "P3 tensor([1.0670, 1.1043]) - torch.Size([2])\n",
      "P4 tensor([2.0670, 2.1043]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0552, -0.4024,  0.0384,  0.5938],\n",
      "        [-0.0632, -0.2078,  0.0503,  0.3134]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0632, -0.2078,  0.0503,  0.3134],\n",
      "        [-0.0674, -0.0135,  0.0566,  0.0370]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0670, 2.1043]) - torch.Size([2])\n",
      "Q-values tensor([[0.9312, 1.0609],\n",
      "        [0.9806, 1.0680]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0609],\n",
      "        [1.0680]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0674, -0.0135,  0.0566,  0.0370]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06763037 -0.20934333  0.05730871  0.3469989 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9736, 1.0639],\n",
      "        [1.0762, 1.1054]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0639, 1.1054]) - torch.Size([2])\n",
      "P2 tensor([1.0629, 1.1043]) - torch.Size([2])\n",
      "P3 tensor([1.0629, 1.1043]) - torch.Size([2])\n",
      "P4 tensor([2.0629, 2.1043]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0674, -0.0135,  0.0566,  0.0370],\n",
      "        [-0.0632, -0.2078,  0.0503,  0.3134]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0676, -0.2093,  0.0573,  0.3470],\n",
      "        [-0.0674, -0.0135,  0.0566,  0.0370]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0629, 2.1043]) - torch.Size([2])\n",
      "Q-values tensor([[1.0779, 1.1133],\n",
      "        [0.9826, 1.0759]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0779],\n",
      "        [1.0759]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0676, -0.2093,  0.0573,  0.3470]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.07181723 -0.01508138  0.06424869  0.07292388] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9736, 1.0639],\n",
      "        [1.0637, 1.0970]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0639, 1.0970]) - torch.Size([2])\n",
      "P2 tensor([1.0629, 1.0959]) - torch.Size([2])\n",
      "P3 tensor([1.0629, 1.0959]) - torch.Size([2])\n",
      "P4 tensor([2.0629, 2.0959]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0674, -0.0135,  0.0566,  0.0370],\n",
      "        [-0.0676, -0.2093,  0.0573,  0.3470]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0676, -0.2093,  0.0573,  0.3470],\n",
      "        [-0.0718, -0.0151,  0.0642,  0.0729]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0629, 2.0959]) - torch.Size([2])\n",
      "Q-values tensor([[1.0850, 1.1194],\n",
      "        [0.9822, 1.0782]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0850],\n",
      "        [1.0782]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0718, -0.0151,  0.0642,  0.0729]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.07211886  0.17906345  0.06570716 -0.1988164 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.1833, 1.1713],\n",
      "        [1.0637, 1.0970]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.1833, 1.0970]) - torch.Size([2])\n",
      "P2 tensor([1.1821, 1.0959]) - torch.Size([2])\n",
      "P3 tensor([1.1821, 1.0959]) - torch.Size([2])\n",
      "P4 tensor([2.1821, 2.0959]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0718, -0.0151,  0.0642,  0.0729],\n",
      "        [-0.0676, -0.2093,  0.0573,  0.3470]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0721,  0.1791,  0.0657, -0.1988],\n",
      "        [-0.0718, -0.0151,  0.0642,  0.0729]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.1821, 2.0959]) - torch.Size([2])\n",
      "Q-values tensor([[1.0796, 1.1169],\n",
      "        [0.9887, 1.0848]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1169],\n",
      "        [1.0848]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0721,  0.1791,  0.0657, -0.1988]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06853759  0.37318707  0.06173084 -0.47006947] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.1833, 1.1713],\n",
      "        [1.0637, 1.0970]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.1833, 1.0970]) - torch.Size([2])\n",
      "P2 tensor([1.1821, 1.0959]) - torch.Size([2])\n",
      "P3 tensor([1.1821, 1.0959]) - torch.Size([2])\n",
      "P4 tensor([2.1821, 2.0959]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0718, -0.0151,  0.0642,  0.0729],\n",
      "        [-0.0676, -0.2093,  0.0573,  0.3470]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0721,  0.1791,  0.0657, -0.1988],\n",
      "        [-0.0718, -0.0151,  0.0642,  0.0729]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.1821, 2.0959]) - torch.Size([2])\n",
      "Q-values tensor([[1.0813, 1.1251],\n",
      "        [0.9907, 1.0925]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1251],\n",
      "        [1.0925]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0685,  0.3732,  0.0617, -0.4701]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06107385  0.5673852   0.05232945 -0.74267447] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.1833, 1.1713],\n",
      "        [1.3197, 1.2714]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.1833, 1.3197]) - torch.Size([2])\n",
      "P2 tensor([1.1821, 1.3184]) - torch.Size([2])\n",
      "P3 tensor([1.1821, 1.3184]) - torch.Size([2])\n",
      "P4 tensor([2.1821, 2.3184]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0718, -0.0151,  0.0642,  0.0729],\n",
      "        [-0.0721,  0.1791,  0.0657, -0.1988]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0721,  0.1791,  0.0657, -0.1988],\n",
      "        [-0.0685,  0.3732,  0.0617, -0.4701]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.1821, 2.3184]) - torch.Size([2])\n",
      "Q-values tensor([[1.0830, 1.1333],\n",
      "        [1.2035, 1.2084]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1333],\n",
      "        [1.2084]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0611,  0.5674,  0.0523, -0.7427]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04972615  0.76174724  0.03747596 -1.0184406 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.4952, 1.4052],\n",
      "        [1.3197, 1.2714]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.4952, 1.3197]) - torch.Size([2])\n",
      "P2 tensor([1.4937, 1.3184]) - torch.Size([2])\n",
      "P3 tensor([1.4937, 1.3184]) - torch.Size([2])\n",
      "P4 tensor([2.4937, 2.3184]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0685,  0.3732,  0.0617, -0.4701],\n",
      "        [-0.0721,  0.1791,  0.0657, -0.1988]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0611,  0.5674,  0.0523, -0.7427],\n",
      "        [-0.0685,  0.3732,  0.0617, -0.4701]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.4937, 2.3184]) - torch.Size([2])\n",
      "Q-values tensor([[1.3436, 1.3195],\n",
      "        [1.2055, 1.2174]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.3195],\n",
      "        [1.2174]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0497,  0.7617,  0.0375, -1.0184]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0344912   0.5661464   0.01710715 -0.71422994] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.6819, 1.5565],\n",
      "        [1.4712, 1.3866]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.6819, 1.4712]) - torch.Size([2])\n",
      "P2 tensor([1.6802, 1.4697]) - torch.Size([2])\n",
      "P3 tensor([1.6802, 1.4697]) - torch.Size([2])\n",
      "P4 tensor([2.6802, 2.4697]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0611,  0.5674,  0.0523, -0.7427],\n",
      "        [-0.0497,  0.7617,  0.0375, -1.0184]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0497,  0.7617,  0.0375, -1.0184],\n",
      "        [-0.0345,  0.5661,  0.0171, -0.7142]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.6802, 2.4697]) - torch.Size([2])\n",
      "Q-values tensor([[1.5237, 1.4687],\n",
      "        [1.7130, 1.6256]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.4687],\n",
      "        [1.7130]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0345,  0.5661,  0.0171, -0.7142]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02316828  0.37079182  0.00282255 -0.41621172] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.2844, 1.2448],\n",
      "        [1.6819, 1.5565]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.2844, 1.6819]) - torch.Size([2])\n",
      "P2 tensor([1.2831, 1.6802]) - torch.Size([2])\n",
      "P3 tensor([1.2831, 1.6802]) - torch.Size([2])\n",
      "P4 tensor([2.2831, 2.6802]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0345,  0.5661,  0.0171, -0.7142],\n",
      "        [-0.0611,  0.5674,  0.0523, -0.7427]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0232,  0.3708,  0.0028, -0.4162],\n",
      "        [-0.0497,  0.7617,  0.0375, -1.0184]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.2831, 2.6802]) - torch.Size([2])\n",
      "Q-values tensor([[1.5083, 1.4590],\n",
      "        [1.5329, 1.4790]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.5083],\n",
      "        [1.4790]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0232,  0.3708,  0.0028, -0.4162]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01575244  0.5658736  -0.00550169 -0.7080035 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.4712, 1.3866],\n",
      "        [1.4632, 1.3809]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.4712, 1.4632]) - torch.Size([2])\n",
      "P2 tensor([1.4697, 1.4617]) - torch.Size([2])\n",
      "P3 tensor([1.4697, 1.4617]) - torch.Size([2])\n",
      "P4 tensor([2.4697, 2.4617]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0497,  0.7617,  0.0375, -1.0184],\n",
      "        [-0.0232,  0.3708,  0.0028, -0.4162]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0345,  0.5661,  0.0171, -0.7142],\n",
      "        [-0.0158,  0.5659, -0.0055, -0.7080]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.4697, 2.4617]) - torch.Size([2])\n",
      "Q-values tensor([[1.7348, 1.6487],\n",
      "        [1.3239, 1.3197]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.7348],\n",
      "        [1.3197]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0158,  0.5659, -0.0055, -0.7080]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00443497  0.7610714  -0.01966176 -1.0024132 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.6610, 1.5415],\n",
      "        [1.4632, 1.3809]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.6610, 1.4632]) - torch.Size([2])\n",
      "P2 tensor([1.6594, 1.4617]) - torch.Size([2])\n",
      "P3 tensor([1.6594, 1.4617]) - torch.Size([2])\n",
      "P4 tensor([2.6594, 2.4617]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0158,  0.5659, -0.0055, -0.7080],\n",
      "        [-0.0232,  0.3708,  0.0028, -0.4162]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0044,  0.7611, -0.0197, -1.0024],\n",
      "        [-0.0158,  0.5659, -0.0055, -0.7080]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.6594, 2.4617]) - torch.Size([2])\n",
      "Q-values tensor([[1.5185, 1.4724],\n",
      "        [1.3313, 1.3284]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.4724],\n",
      "        [1.3284]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0044,  0.7611, -0.0197, -1.0024]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01078646  0.95645046 -0.03971002 -1.3012053 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.6610, 1.5415],\n",
      "        [1.4632, 1.3809]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.6610, 1.4632]) - torch.Size([2])\n",
      "P2 tensor([1.6594, 1.4617]) - torch.Size([2])\n",
      "P3 tensor([1.6594, 1.4617]) - torch.Size([2])\n",
      "P4 tensor([2.6594, 2.4617]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0158,  0.5659, -0.0055, -0.7080],\n",
      "        [-0.0232,  0.3708,  0.0028, -0.4162]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0044,  0.7611, -0.0197, -1.0024],\n",
      "        [-0.0158,  0.5659, -0.0055, -0.7080]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.6594, 2.4617]) - torch.Size([2])\n",
      "Q-values tensor([[1.5208, 1.4843],\n",
      "        [1.3335, 1.3386]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.4843],\n",
      "        [1.3386]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0108,  0.9565, -0.0397, -1.3012]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.02991547  0.76185423 -0.06573413 -1.0212128 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.8677, 1.7138],\n",
      "        [1.6610, 1.5415]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.8677, 1.6610]) - torch.Size([2])\n",
      "P2 tensor([1.8659, 1.6594]) - torch.Size([2])\n",
      "P3 tensor([1.8659, 1.6594]) - torch.Size([2])\n",
      "P4 tensor([2.8659, 2.6594]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0044,  0.7611, -0.0197, -1.0024],\n",
      "        [-0.0158,  0.5659, -0.0055, -0.7080]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0108,  0.9565, -0.0397, -1.3012],\n",
      "        [-0.0044,  0.7611, -0.0197, -1.0024]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.8659, 2.6594]) - torch.Size([2])\n",
      "Q-values tensor([[1.7303, 1.6702],\n",
      "        [1.5230, 1.4963]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.6702],\n",
      "        [1.4963]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0299,  0.7619, -0.0657, -1.0212]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.04515255  0.56766677 -0.08615838 -0.74987257] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.6661, 1.5485],\n",
      "        [1.8677, 1.7138]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.6661, 1.8677]) - torch.Size([2])\n",
      "P2 tensor([1.6644, 1.8659]) - torch.Size([2])\n",
      "P3 tensor([1.6644, 1.8659]) - torch.Size([2])\n",
      "P4 tensor([2.6644, 2.8659]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0108,  0.9565, -0.0397, -1.3012],\n",
      "        [-0.0044,  0.7611, -0.0197, -1.0024]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0299,  0.7619, -0.0657, -1.0212],\n",
      "        [ 0.0108,  0.9565, -0.0397, -1.3012]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.6644, 2.8659]) - torch.Size([2])\n",
      "Q-values tensor([[1.9488, 1.8737],\n",
      "        [1.7330, 1.6846]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.9488],\n",
      "        [1.6846]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0452,  0.5677, -0.0862, -0.7499]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.05650589  0.37383217 -0.10115583 -0.4854984 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.4786, 1.3957],\n",
      "        [1.3083, 1.2683]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.4786, 1.3083]) - torch.Size([2])\n",
      "P2 tensor([1.4771, 1.3070]) - torch.Size([2])\n",
      "P3 tensor([1.4771, 1.3070]) - torch.Size([2])\n",
      "P4 tensor([2.4771, 2.3070]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0299,  0.7619, -0.0657, -1.0212],\n",
      "        [ 0.0452,  0.5677, -0.0862, -0.7499]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0452,  0.5677, -0.0862, -0.7499],\n",
      "        [ 0.0565,  0.3738, -0.1012, -0.4855]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.4771, 2.3070]) - torch.Size([2])\n",
      "Q-values tensor([[1.7488, 1.7046],\n",
      "        [1.5498, 1.5349]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.7488],\n",
      "        [1.5498]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0565,  0.3738, -0.1012, -0.4855]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06398253  0.57022524 -0.1108658  -0.80827117] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.3083, 1.2683],\n",
      "        [1.4786, 1.3957]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.3083, 1.4786]) - torch.Size([2])\n",
      "P2 tensor([1.3070, 1.4771]) - torch.Size([2])\n",
      "P3 tensor([1.3070, 1.4771]) - torch.Size([2])\n",
      "P4 tensor([2.3070, 2.4771]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0452,  0.5677, -0.0862, -0.7499],\n",
      "        [ 0.0299,  0.7619, -0.0657, -1.0212]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0565,  0.3738, -0.1012, -0.4855],\n",
      "        [ 0.0452,  0.5677, -0.0862, -0.7499]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.3070, 2.4771]) - torch.Size([2])\n",
      "Q-values tensor([[1.5624, 1.5368],\n",
      "        [1.7637, 1.7068]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.5624],\n",
      "        [1.7637]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0640,  0.5702, -0.1109, -0.8083]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.07538704  0.7666777  -0.12703122 -1.1336685 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.7312, 1.6067],\n",
      "        [1.5124, 1.4255]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.7312, 1.5124]) - torch.Size([2])\n",
      "P2 tensor([1.7295, 1.5109]) - torch.Size([2])\n",
      "P3 tensor([1.7295, 1.5109]) - torch.Size([2])\n",
      "P4 tensor([2.7295, 2.5109]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0640,  0.5702, -0.1109, -0.8083],\n",
      "        [ 0.0565,  0.3738, -0.1012, -0.4855]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|郊         | 16/1000 [00:02<03:10,  5.17 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0754,  0.7667, -0.1270, -1.1337],\n",
      "        [ 0.0640,  0.5702, -0.1109, -0.8083]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.7295, 2.5109]) - torch.Size([2])\n",
      "Q-values tensor([[1.6116, 1.5717],\n",
      "        [1.3906, 1.3962]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.5717],\n",
      "        [1.3962]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0754,  0.7667, -0.1270, -1.1337]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.09072059  0.96321225 -0.14970459 -1.4633418 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.9740, 1.8130],\n",
      "        [1.7312, 1.6067]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.9740, 1.7312]) - torch.Size([2])\n",
      "P2 tensor([1.9720, 1.7295]) - torch.Size([2])\n",
      "P3 tensor([1.9720, 1.7295]) - torch.Size([2])\n",
      "P4 tensor([2.9720, 2.7295]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0754,  0.7667, -0.1270, -1.1337],\n",
      "        [ 0.0640,  0.5702, -0.1109, -0.8083]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0907,  0.9632, -0.1497, -1.4633],\n",
      "        [ 0.0754,  0.7667, -0.1270, -1.1337]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.9720, 2.7295]) - torch.Size([2])\n",
      "Q-values tensor([[1.8514, 1.7881],\n",
      "        [1.6142, 1.5845]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.7881],\n",
      "        [1.5845]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0907,  0.9632, -0.1497, -1.4633]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.10998483  1.1598177  -0.17897144 -1.7988003 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.7312, 1.6067],\n",
      "        [2.2284, 2.0381]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.7312, 2.2284]) - torch.Size([2])\n",
      "P2 tensor([1.7295, 2.2262]) - torch.Size([2])\n",
      "P3 tensor([1.7295, 2.2262]) - torch.Size([2])\n",
      "P4 tensor([2.7295, 3.2262]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0640,  0.5702, -0.1109, -0.8083],\n",
      "        [ 0.0907,  0.9632, -0.1497, -1.4633]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0754,  0.7667, -0.1270, -1.1337],\n",
      "        [ 0.1100,  1.1598, -0.1790, -1.7988]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.7295, 3.2262]) - torch.Size([2])\n",
      "Q-values tensor([[1.6167, 1.5973],\n",
      "        [2.1171, 2.0361]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.5973],\n",
      "        [2.0361]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1100,  1.1598, -0.1790, -1.7988]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.13318118  1.356434   -0.21494743 -2.1413512 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[2.4844, 2.2710],\n",
      "        [2.2284, 2.0381]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([2.4844, 2.2284]) - torch.Size([2])\n",
      "P2 tensor([2.4819, 2.2262]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 2.2262]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 3.2262]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1100,  1.1598, -0.1790, -1.7988],\n",
      "        [ 0.0907,  0.9632, -0.1497, -1.4633]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1332,  1.3564, -0.2149, -2.1414],\n",
      "        [ 0.1100,  1.1598, -0.1790, -1.7988]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 3.2262]) - torch.Size([2])\n",
      "Q-values tensor([[2.3968, 2.3088],\n",
      "        [2.1210, 2.0543]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[2.3088],\n",
      "        [2.0543]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0008,  0.0169,  0.0306, -0.0196]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0004159   0.21155775  0.03025737 -0.30242917] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.2246, 1.2087],\n",
      "        [2.2284, 2.0381]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.2246, 2.2284]) - torch.Size([2])\n",
      "P2 tensor([1.2233, 2.2262]) - torch.Size([2])\n",
      "P3 tensor([1.2233, 2.2262]) - torch.Size([2])\n",
      "P4 tensor([2.2233, 3.2262]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-7.5367e-04,  1.6888e-02,  3.0649e-02, -1.9572e-02],\n",
      "        [ 9.0721e-02,  9.6321e-01, -1.4970e-01, -1.4633e+00]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-4.1590e-04,  2.1156e-01,  3.0257e-02, -3.0243e-01],\n",
      "        [ 1.0998e-01,  1.1598e+00, -1.7897e-01, -1.7988e+00]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.2233, 3.2262]) - torch.Size([2])\n",
      "Q-values tensor([[1.1637, 1.2414],\n",
      "        [2.1173, 2.0378]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.2414],\n",
      "        [2.0378]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0004,  0.2116,  0.0303, -0.3024]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00381525  0.4062357   0.02420879 -0.5854181 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[2.4844, 2.2710],\n",
      "        [1.3838, 1.3238]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([2.4844, 1.3838]) - torch.Size([2])\n",
      "P2 tensor([2.4819, 1.3825]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 1.3825]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 2.3825]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 1.0998e-01,  1.1598e+00, -1.7897e-01, -1.7988e+00],\n",
      "        [-4.1590e-04,  2.1156e-01,  3.0257e-02, -3.0243e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1332,  1.3564, -0.2149, -2.1414],\n",
      "        [ 0.0038,  0.4062,  0.0242, -0.5854]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 2.3825]) - torch.Size([2])\n",
      "Q-values tensor([[2.3971, 2.3088],\n",
      "        [1.3079, 1.3599]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[2.3088],\n",
      "        [1.3599]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0038,  0.4062,  0.0242, -0.5854]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01193997  0.6010103   0.01250042 -0.8703778 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.5722, 1.4704],\n",
      "        [1.3838, 1.3238]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.5722, 1.3838]) - torch.Size([2])\n",
      "P2 tensor([1.5706, 1.3825]) - torch.Size([2])\n",
      "P3 tensor([1.5706, 1.3825]) - torch.Size([2])\n",
      "P4 tensor([2.5706, 2.3825]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 3.8153e-03,  4.0624e-01,  2.4209e-02, -5.8542e-01],\n",
      "        [-4.1590e-04,  2.1156e-01,  3.0257e-02, -3.0243e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0119,  0.6010,  0.0125, -0.8704],\n",
      "        [ 0.0038,  0.4062,  0.0242, -0.5854]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.5706, 2.3825]) - torch.Size([2])\n",
      "Q-values tensor([[1.4777, 1.4861],\n",
      "        [1.3063, 1.3542]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.4861],\n",
      "        [1.3542]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0119,  0.6010,  0.0125, -0.8704]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.02396017  0.4057206  -0.00490713 -0.573791  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.3838, 1.3238],\n",
      "        [1.5722, 1.4704]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.3838, 1.5722]) - torch.Size([2])\n",
      "P2 tensor([1.3825, 1.5706]) - torch.Size([2])\n",
      "P3 tensor([1.3825, 1.5706]) - torch.Size([2])\n",
      "P4 tensor([2.3825, 2.5706]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-4.1590e-04,  2.1156e-01,  3.0257e-02, -3.0243e-01],\n",
      "        [ 3.8153e-03,  4.0624e-01,  2.4209e-02, -5.8542e-01]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0038,  0.4062,  0.0242, -0.5854],\n",
      "        [ 0.0119,  0.6010,  0.0125, -0.8704]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.3825, 2.5706]) - torch.Size([2])\n",
      "Q-values tensor([[1.3084, 1.3638],\n",
      "        [1.4802, 1.4973]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.3638],\n",
      "        [1.4973]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0240,  0.4057, -0.0049, -0.5738]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.03207459  0.600911   -0.01638295 -0.8680158 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.3718, 1.3137],\n",
      "        [1.5659, 1.4665]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.3718, 1.5659]) - torch.Size([2])\n",
      "P2 tensor([1.3704, 1.5644]) - torch.Size([2])\n",
      "P3 tensor([1.3704, 1.5644]) - torch.Size([2])\n",
      "P4 tensor([2.3704, 2.5644]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0119,  0.6010,  0.0125, -0.8704],\n",
      "        [ 0.0240,  0.4057, -0.0049, -0.5738]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0240,  0.4057, -0.0049, -0.5738],\n",
      "        [ 0.0321,  0.6009, -0.0164, -0.8680]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.3704, 2.5644]) - torch.Size([2])\n",
      "Q-values tensor([[1.6867, 1.6787],\n",
      "        [1.4694, 1.4973]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.6867],\n",
      "        [1.4973]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0321,  0.6009, -0.0164, -0.8680]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.0440928   0.79625195 -0.03374327 -1.1658044 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.3718, 1.3137],\n",
      "        [1.7667, 1.6331]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.3718, 1.7667]) - torch.Size([2])\n",
      "P2 tensor([1.3704, 1.7649]) - torch.Size([2])\n",
      "P3 tensor([1.3704, 1.7649]) - torch.Size([2])\n",
      "P4 tensor([2.3704, 2.7649]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0119,  0.6010,  0.0125, -0.8704],\n",
      "        [ 0.0321,  0.6009, -0.0164, -0.8680]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0240,  0.4057, -0.0049, -0.5738],\n",
      "        [ 0.0441,  0.7963, -0.0337, -1.1658]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.3704, 2.7649]) - torch.Size([2])\n",
      "Q-values tensor([[1.6972, 1.6896],\n",
      "        [1.6908, 1.6849]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.6972],\n",
      "        [1.6849]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0441,  0.7963, -0.0337, -1.1658]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.06001784  0.9917965  -0.05705936 -1.4688728 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.9919, 1.8192],\n",
      "        [1.5659, 1.4665]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.9919, 1.5659]) - torch.Size([2])\n",
      "P2 tensor([1.9899, 1.5644]) - torch.Size([2])\n",
      "P3 tensor([1.9899, 1.5644]) - torch.Size([2])\n",
      "P4 tensor([2.9899, 2.5644]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0441,  0.7963, -0.0337, -1.1658],\n",
      "        [ 0.0240,  0.4057, -0.0049, -0.5738]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0600,  0.9918, -0.0571, -1.4689],\n",
      "        [ 0.0321,  0.6009, -0.0164, -0.8680]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.9899, 2.5644]) - torch.Size([2])\n",
      "Q-values tensor([[1.9243, 1.8940],\n",
      "        [1.4859, 1.5171]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.8940],\n",
      "        [1.5171]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0600,  0.9918, -0.0571, -1.4689]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.07985377  0.7974173  -0.08643681 -1.1945448 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.7667, 1.6331],\n",
      "        [1.9919, 1.8192]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.7667, 1.9919]) - torch.Size([2])\n",
      "P2 tensor([1.7649, 1.9899]) - torch.Size([2])\n",
      "P3 tensor([1.7649, 1.9899]) - torch.Size([2])\n",
      "P4 tensor([2.7649, 2.9899]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0321,  0.6009, -0.0164, -0.8680],\n",
      "        [ 0.0441,  0.7963, -0.0337, -1.1658]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0441,  0.7963, -0.0337, -1.1658],\n",
      "        [ 0.0600,  0.9918, -0.0571, -1.4689]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.7649, 2.9899]) - torch.Size([2])\n",
      "Q-values tensor([[1.7031, 1.7105],\n",
      "        [1.9274, 1.9100]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.7105],\n",
      "        [1.9100]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0799,  0.7974, -0.0864, -1.1945]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.09580212  0.99354553 -0.11032771 -1.5130197 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.9919, 1.8192],\n",
      "        [1.7798, 1.6463]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.9919, 1.7798]) - torch.Size([2])\n",
      "P2 tensor([1.9899, 1.7780]) - torch.Size([2])\n",
      "P3 tensor([1.9899, 1.7780]) - torch.Size([2])\n",
      "P4 tensor([2.9899, 2.7780]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0441,  0.7963, -0.0337, -1.1658],\n",
      "        [ 0.0600,  0.9918, -0.0571, -1.4689]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0600,  0.9918, -0.0571, -1.4689],\n",
      "        [ 0.0799,  0.7974, -0.0864, -1.1945]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.9899, 2.7780]) - torch.Size([2])\n",
      "Q-values tensor([[1.9306, 1.9261],\n",
      "        [2.1775, 2.1477]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.9261],\n",
      "        [2.1775]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0958,  0.9935, -0.1103, -1.5130]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.11567303  0.7999189  -0.1405881  -1.2567161 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.7798, 1.6463],\n",
      "        [1.8201, 1.6835]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.7798, 1.8201]) - torch.Size([2])\n",
      "P2 tensor([1.7780, 1.8183]) - torch.Size([2])\n",
      "P3 tensor([1.7780, 1.8183]) - torch.Size([2])\n",
      "P4 tensor([2.7780, 2.8183]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0600,  0.9918, -0.0571, -1.4689],\n",
      "        [ 0.0958,  0.9935, -0.1103, -1.5130]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0799,  0.7974, -0.0864, -1.1945],\n",
      "        [ 0.1157,  0.7999, -0.1406, -1.2567]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.7780, 2.8183]) - torch.Size([2])\n",
      "Q-values tensor([[2.1911, 2.1639],\n",
      "        [2.2202, 2.1958]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[2.1911],\n",
      "        [2.2202]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1157,  0.7999, -0.1406, -1.2567]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.13167141  0.99653226 -0.16572243 -1.5899233 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.8201, 1.6835],\n",
      "        [2.0660, 1.8973]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.8201, 2.0660]) - torch.Size([2])\n",
      "P2 tensor([1.8183, 2.0640]) - torch.Size([2])\n",
      "P3 tensor([1.8183, 2.0640]) - torch.Size([2])\n",
      "P4 tensor([2.8183, 3.0640]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0958,  0.9935, -0.1103, -1.5130],\n",
      "        [ 0.1157,  0.7999, -0.1406, -1.2567]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1157,  0.7999, -0.1406, -1.2567],\n",
      "        [ 0.1317,  0.9965, -0.1657, -1.5899]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.8183, 3.0640]) - torch.Size([2])\n",
      "Q-values tensor([[2.2396, 2.1991],\n",
      "        [2.0178, 2.0013]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[2.2396],\n",
      "        [2.0013]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1317,  0.9965, -0.1657, -1.5899]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.15160206  0.80372083 -0.1975209  -1.3531682 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.8201, 1.6835],\n",
      "        [2.0660, 1.8973]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.8201, 2.0660]) - torch.Size([2])\n",
      "P2 tensor([1.8183, 2.0640]) - torch.Size([2])\n",
      "P3 tensor([1.8183, 2.0640]) - torch.Size([2])\n",
      "P4 tensor([2.8183, 3.0640]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0958,  0.9935, -0.1103, -1.5130],\n",
      "        [ 0.1157,  0.7999, -0.1406, -1.2567]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1157,  0.7999, -0.1406, -1.2567],\n",
      "        [ 0.1317,  0.9965, -0.1657, -1.5899]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.8183, 3.0640]) - torch.Size([2])\n",
      "Q-values tensor([[2.2535, 2.2162],\n",
      "        [2.0298, 2.0165]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[2.2535],\n",
      "        [2.0165]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.1516,  0.8037, -0.1975, -1.3532]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [ 0.16767646  0.61154944 -0.22458425 -1.1282103 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.7120, 1.6020],\n",
      "        [2.0660, 1.8973]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.7120, 2.0660]) - torch.Size([2])\n",
      "P2 tensor([1.7103, 2.0640]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 2.0640]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 3.0640]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1516,  0.8037, -0.1975, -1.3532],\n",
      "        [ 0.1157,  0.7999, -0.1406, -1.2567]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1677,  0.6115, -0.2246, -1.1282],\n",
      "        [ 0.1317,  0.9965, -0.1657, -1.5899]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 3.0640]) - torch.Size([2])\n",
      "Q-values tensor([[2.1144, 2.1024],\n",
      "        [2.0416, 2.0319]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[2.1144],\n",
      "        [2.0319]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[0.0104, 0.0429, 0.0028, 0.0430]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.01127516 -0.15223634  0.00370386  0.33658582] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.7120, 1.6020],\n",
      "        [0.9665, 1.0454]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.7120, 1.0454]) - torch.Size([2])\n",
      "P2 tensor([1.7103, 1.0444]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 1.0444]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 2.0444]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.1516,  0.8037, -0.1975, -1.3532],\n",
      "        [ 0.0104,  0.0429,  0.0028,  0.0430]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.1677,  0.6115, -0.2246, -1.1282],\n",
      "        [ 0.0113, -0.1522,  0.0037,  0.3366]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 2.0444]) - torch.Size([2])\n",
      "Q-values tensor([[2.1012, 2.1141],\n",
      "        [1.1719, 1.2904]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[2.1012],\n",
      "        [1.1719]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0113, -0.1522,  0.0037,  0.3366]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00823043 -0.3474108   0.01043557  0.63043445] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9665, 1.0454],\n",
      "        [0.9270, 1.0472]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0454, 1.0472]) - torch.Size([2])\n",
      "P2 tensor([1.0444, 1.0461]) - torch.Size([2])\n",
      "P3 tensor([1.0444, 1.0461]) - torch.Size([2])\n",
      "P4 tensor([2.0444, 2.0461]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0104,  0.0429,  0.0028,  0.0430],\n",
      "        [ 0.0113, -0.1522,  0.0037,  0.3366]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0113, -0.1522,  0.0037,  0.3366],\n",
      "        [ 0.0082, -0.3474,  0.0104,  0.6304]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0444, 2.0461]) - torch.Size([2])\n",
      "Q-values tensor([[1.1680, 1.2896],\n",
      "        [1.0607, 1.2261]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1680],\n",
      "        [1.0607]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0082, -0.3474,  0.0104,  0.6304]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [ 0.00128222 -0.152436    0.02304426  0.34105617] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9270, 1.0472],\n",
      "        [0.9665, 1.0454]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0472, 1.0454]) - torch.Size([2])\n",
      "P2 tensor([1.0461, 1.0444]) - torch.Size([2])\n",
      "P3 tensor([1.0461, 1.0444]) - torch.Size([2])\n",
      "P4 tensor([2.0461, 2.0444]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0113, -0.1522,  0.0037,  0.3366],\n",
      "        [ 0.0104,  0.0429,  0.0028,  0.0430]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0082, -0.3474,  0.0104,  0.6304],\n",
      "        [ 0.0113, -0.1522,  0.0037,  0.3366]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0461, 2.0444]) - torch.Size([2])\n",
      "Q-values tensor([[1.0686, 1.2279],\n",
      "        [1.1765, 1.2910]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0686],\n",
      "        [1.1765]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[ 0.0013, -0.1524,  0.0230,  0.3411]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0017665   0.04235062  0.02986538  0.05572822] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9686, 1.0488],\n",
      "        [0.9270, 1.0472]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0488, 1.0472]) - torch.Size([2])\n",
      "P2 tensor([1.0477, 1.0461]) - torch.Size([2])\n",
      "P3 tensor([1.0477, 1.0461]) - torch.Size([2])\n",
      "P4 tensor([2.0477, 2.0461]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[ 0.0082, -0.3474,  0.0104,  0.6304],\n",
      "        [ 0.0113, -0.1522,  0.0037,  0.3366]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[ 0.0013, -0.1524,  0.0230,  0.3411],\n",
      "        [ 0.0082, -0.3474,  0.0104,  0.6304]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0477, 2.0461]) - torch.Size([2])\n",
      "Q-values tensor([[1.0319, 1.2226],\n",
      "        [1.0766, 1.2296]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.2226],\n",
      "        [1.0766]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0018,  0.0424,  0.0299,  0.0557]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00091949 -0.15318654  0.03097995  0.3576823 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9659, 1.0477],\n",
      "        [1.0662, 1.0890]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0477, 1.0890]) - torch.Size([2])\n",
      "P2 tensor([1.0467, 1.0879]) - torch.Size([2])\n",
      "P3 tensor([1.0467, 1.0879]) - torch.Size([2])\n",
      "P4 tensor([2.0467, 2.0879]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0018,  0.0424,  0.0299,  0.0557],\n",
      "        [ 0.0013, -0.1524,  0.0230,  0.3411]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0009, -0.1532,  0.0310,  0.3577],\n",
      "        [-0.0018,  0.0424,  0.0299,  0.0557]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0467, 2.0879]) - torch.Size([2])\n",
      "Q-values tensor([[1.1928, 1.3005],\n",
      "        [1.0857, 1.2399]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1928],\n",
      "        [1.2399]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0009, -0.1532,  0.0310,  0.3577]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00398322  0.0414816   0.0381336   0.07492683] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.0605, 1.0856],\n",
      "        [0.9659, 1.0477]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0856, 1.0477]) - torch.Size([2])\n",
      "P2 tensor([1.0845, 1.0467]) - torch.Size([2])\n",
      "P3 tensor([1.0845, 1.0467]) - torch.Size([2])\n",
      "P4 tensor([2.0845, 2.0467]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0009, -0.1532,  0.0310,  0.3577],\n",
      "        [-0.0018,  0.0424,  0.0299,  0.0557]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0040,  0.0415,  0.0381,  0.0749],\n",
      "        [-0.0009, -0.1532,  0.0310,  0.3577]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0845, 2.0467]) - torch.Size([2])\n",
      "Q-values tensor([[1.0894, 1.2454],\n",
      "        [1.2003, 1.3069]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.2454],\n",
      "        [1.2003]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0040,  0.0415,  0.0381,  0.0749]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.00315359 -0.1541657   0.03963213  0.37939298] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9624, 1.0462],\n",
      "        [1.0605, 1.0856]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0462, 1.0856]) - torch.Size([2])\n",
      "P2 tensor([1.0452, 1.0845]) - torch.Size([2])\n",
      "P3 tensor([1.0452, 1.0845]) - torch.Size([2])\n",
      "P4 tensor([2.0452, 2.0845]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0040,  0.0415,  0.0381,  0.0749],\n",
      "        [-0.0009, -0.1532,  0.0310,  0.3577]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0032, -0.1542,  0.0396,  0.3794],\n",
      "        [-0.0040,  0.0415,  0.0381,  0.0749]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0452, 2.0845]) - torch.Size([2])\n",
      "Q-values tensor([[1.2016, 1.3086],\n",
      "        [1.0961, 1.2523]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.2016],\n",
      "        [1.2523]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0032, -0.1542,  0.0396,  0.3794]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.0062369  -0.3498274   0.04721999  0.6843039 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9238, 1.0499],\n",
      "        [1.0605, 1.0856]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0499, 1.0856]) - torch.Size([2])\n",
      "P2 tensor([1.0489, 1.0845]) - torch.Size([2])\n",
      "P3 tensor([1.0489, 1.0845]) - torch.Size([2])\n",
      "P4 tensor([2.0489, 2.0845]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0032, -0.1542,  0.0396,  0.3794],\n",
      "        [-0.0009, -0.1532,  0.0310,  0.3577]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0062, -0.3498,  0.0472,  0.6843],\n",
      "        [-0.0040,  0.0415,  0.0381,  0.0749]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0489, 2.0845]) - torch.Size([2])\n",
      "Q-values tensor([[1.0988, 1.2570],\n",
      "        [1.1029, 1.2592]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0988],\n",
      "        [1.2592]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0062, -0.3498,  0.0472,  0.6843]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.01323345 -0.5455721   0.06090607  0.9914711 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9427, 1.1019],\n",
      "        [0.9238, 1.0499]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.1019, 1.0499]) - torch.Size([2])\n",
      "P2 tensor([1.1008, 1.0489]) - torch.Size([2])\n",
      "P3 tensor([1.1008, 1.0489]) - torch.Size([2])\n",
      "P4 tensor([2.1008, 2.0489]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0062, -0.3498,  0.0472,  0.6843],\n",
      "        [-0.0032, -0.1542,  0.0396,  0.3794]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0132, -0.5456,  0.0609,  0.9915],\n",
      "        [-0.0062, -0.3498,  0.0472,  0.6843]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.1008, 2.0489]) - torch.Size([2])\n",
      "Q-values tensor([[1.0622, 1.2615],\n",
      "        [1.1057, 1.2640]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0622],\n",
      "        [1.1057]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0132, -0.5456,  0.0609,  0.9915]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.02414489 -0.7414539   0.08073549  1.3026444 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9427, 1.1019],\n",
      "        [0.9238, 1.0499]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.1019, 1.0499]) - torch.Size([2])\n",
      "P2 tensor([1.1008, 1.0489]) - torch.Size([2])\n",
      "P3 tensor([1.1008, 1.0489]) - torch.Size([2])\n",
      "P4 tensor([2.1008, 2.0489]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0062, -0.3498,  0.0472,  0.6843],\n",
      "        [-0.0032, -0.1542,  0.0396,  0.3794]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0132, -0.5456,  0.0609,  0.9915],\n",
      "        [-0.0062, -0.3498,  0.0472,  0.6843]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.1008, 2.0489]) - torch.Size([2])\n",
      "Q-values tensor([[1.0707, 1.2639],\n",
      "        [1.1139, 1.2659]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0707],\n",
      "        [1.1139]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0241, -0.7415,  0.0807,  1.3026]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.03897397 -0.54744357  0.10678837  1.0362873 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.0088, 1.1807],\n",
      "        [0.9427, 1.1019]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.1807, 1.1019]) - torch.Size([2])\n",
      "P2 tensor([1.1795, 1.1008]) - torch.Size([2])\n",
      "P3 tensor([1.1795, 1.1008]) - torch.Size([2])\n",
      "P4 tensor([2.1795, 2.1008]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0132, -0.5456,  0.0609,  0.9915],\n",
      "        [-0.0062, -0.3498,  0.0472,  0.6843]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0241, -0.7415,  0.0807,  1.3026],\n",
      "        [-0.0132, -0.5456,  0.0609,  0.9915]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.1795, 2.1008]) - torch.Size([2])\n",
      "Q-values tensor([[1.1031, 1.3222],\n",
      "        [1.0793, 1.2663]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1031],\n",
      "        [1.0793]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0390, -0.5474,  0.1068,  1.0363]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04992284 -0.35389075  0.12751412  0.778948  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9270, 1.0702],\n",
      "        [0.9521, 1.1169]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.0702, 1.1169]) - torch.Size([2])\n",
      "P2 tensor([1.0692, 1.1158]) - torch.Size([2])\n",
      "P3 tensor([1.0692, 1.1158]) - torch.Size([2])\n",
      "P4 tensor([2.0692, 2.1158]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0390, -0.5474,  0.1068,  1.0363],\n",
      "        [-0.0241, -0.7415,  0.0807,  1.3026]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0499, -0.3539,  0.1275,  0.7789],\n",
      "        [-0.0390, -0.5474,  0.1068,  1.0363]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.0692, 2.1158]) - torch.Size([2])\n",
      "Q-values tensor([[1.1243, 1.3429],\n",
      "        [1.1904, 1.4194]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.3429],\n",
      "        [1.4194]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0499, -0.3539,  0.1275,  0.7789]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.05700066 -0.55051374  0.14309308  1.1088755 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[0.9620, 1.1320],\n",
      "        [0.9521, 1.1169]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.1320, 1.1169]) - torch.Size([2])\n",
      "P2 tensor([1.1308, 1.1158]) - torch.Size([2])\n",
      "P3 tensor([1.1308, 1.1158]) - torch.Size([2])\n",
      "P4 tensor([2.1308, 2.1158]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0499, -0.3539,  0.1275,  0.7789],\n",
      "        [-0.0241, -0.7415,  0.0807,  1.3026]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0570, -0.5505,  0.1431,  1.1089],\n",
      "        [-0.0390, -0.5474,  0.1068,  1.0363]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.1308, 2.1158]) - torch.Size([2])\n",
      "Q-values tensor([[1.0964, 1.2990],\n",
      "        [1.1943, 1.4306]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.0964],\n",
      "        [1.4306]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0570, -0.5505,  0.1431,  1.1089]]) -\n",
      "Actions 0 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.06801093 -0.7471962   0.1652706   1.4428099 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.0323, 1.2142],\n",
      "        [0.9270, 1.0702]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.2142, 1.0702]) - torch.Size([2])\n",
      "P2 tensor([1.2130, 1.0692]) - torch.Size([2])\n",
      "P3 tensor([1.2130, 1.0692]) - torch.Size([2])\n",
      "P4 tensor([2.2130, 2.0692]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0570, -0.5505,  0.1431,  1.1089],\n",
      "        [-0.0390, -0.5474,  0.1068,  1.0363]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0680, -0.7472,  0.1653,  1.4428],\n",
      "        [-0.0499, -0.3539,  0.1275,  0.7789]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.2130, 2.0692]) - torch.Size([2])\n",
      "Q-values tensor([[1.1494, 1.3789],\n",
      "        [1.1360, 1.3610]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1494],\n",
      "        [1.3610]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0680, -0.7472,  0.1653,  1.4428]]) -\n",
      "Actions 0 -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|郊         | 18/1000 [00:02<02:42,  6.05 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.08295485 -0.9439217   0.19412678  1.782246  ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.0323, 1.2142],\n",
      "        [0.9620, 1.1320]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.2142, 1.1320]) - torch.Size([2])\n",
      "P2 tensor([1.2130, 1.1308]) - torch.Size([2])\n",
      "P3 tensor([1.2130, 1.1308]) - torch.Size([2])\n",
      "P4 tensor([2.2130, 2.1308]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0570, -0.5505,  0.1431,  1.1089],\n",
      "        [-0.0499, -0.3539,  0.1275,  0.7789]]) - torch.Size([2, 4])\n",
      "Actions tensor([[0],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([0., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.0680, -0.7472,  0.1653,  1.4428],\n",
      "        [-0.0570, -0.5505,  0.1431,  1.1089]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([2.2130, 2.1308]) - torch.Size([2])\n",
      "Q-values tensor([[1.1581, 1.3875],\n",
      "        [1.1116, 1.3136]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.1581],\n",
      "        [1.1116]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0830, -0.9439,  0.1941,  1.7822]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones True -\n",
      "Next states [-0.10183329 -0.7514407   0.22977172  1.5556632 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.0512, 1.2391],\n",
      "        [1.0323, 1.2142]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.2391, 1.2142]) - torch.Size([2])\n",
      "P2 tensor([1.2379, 1.2130]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 1.2130]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 2.2130]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0830, -0.9439,  0.1941,  1.7822],\n",
      "        [-0.0570, -0.5505,  0.1431,  1.1089]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [0]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1018, -0.7514,  0.2298,  1.5557],\n",
      "        [-0.0680, -0.7472,  0.1653,  1.4428]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 2.2130]) - torch.Size([2])\n",
      "Q-values tensor([[1.3506, 1.6062],\n",
      "        [1.1681, 1.3907]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.6062],\n",
      "        [1.1681]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n",
      "States tensor([[-0.0470,  0.0066, -0.0232,  0.0134]]) -\n",
      "Actions 1 -\n",
      "Rewards tensor([1.]) -\n",
      "Dones False -\n",
      "Next states [-0.04683494  0.20207335 -0.02294392 -0.2865012 ] -\n",
      "-----------------------------------------------\n",
      "P0 tensor([[1.0512, 1.2391],\n",
      "        [1.2096, 1.1951]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "P1 tensor([1.2391, 1.2096]) - torch.Size([2])\n",
      "P2 tensor([1.2379, 1.2084]) - torch.Size([2])\n",
      "P3 tensor([0.0000, 1.2084]) - torch.Size([2])\n",
      "P4 tensor([1.0000, 2.2084]) - torch.Size([2])\n",
      "-----------------------------------------------\n",
      "States tensor([[-0.0830, -0.9439,  0.1941,  1.7822],\n",
      "        [-0.0470,  0.0066, -0.0232,  0.0134]]) - torch.Size([2, 4])\n",
      "Actions tensor([[1],\n",
      "        [1]]) - torch.Size([2, 1])\n",
      "Rewards tensor([1., 1.]) - torch.Size([2])\n",
      "Dones tensor([1., 0.], dtype=torch.float16) - torch.Size([2])\n",
      "Next states tensor([[-0.1018, -0.7514,  0.2298,  1.5557],\n",
      "        [-0.0468,  0.2021, -0.0229, -0.2865]]) - torch.Size([2, 1, 4])\n",
      "Next values tensor([1.0000, 2.2084]) - torch.Size([2])\n",
      "Q-values tensor([[1.3593, 1.5980],\n",
      "        [1.2802, 1.3565]], grad_fn=<AddmmBackward0>) - torch.Size([2, 2])\n",
      "Q-values tensor([[1.5980],\n",
      "        [1.3565]], grad_fn=<GatherBackward0>) - torch.Size([2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPISODES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOTAL_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mENV_NAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juan1\\Ort\\Agentes Inteligentes\\reinforcement-learning\\Atari\\abstract_agent.py:76\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self, number_episodes, max_steps_episode, max_steps, writer_name)\u001b[0m\n\u001b[0;32m     73\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Actualizar el modelo\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juan1\\Ort\\Agentes Inteligentes\\reinforcement-learning\\Atari\\dqn_agent.py:96\u001b[0m, in \u001b[0;36mDQNAgent.update_weights\u001b[1;34m(self, total_steps)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# En Pytorch la funcion de costo se llaman con (predicciones, objetivos) en ese orden.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_model_update \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m total_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msecond_model_update \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_policy\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\juan1\\anaconda3\\envs\\TallerIA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1895\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[1;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1895\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1896\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m   1897\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "File \u001b[1;32mc:\\Users\\juan1\\anaconda3\\envs\\TallerIA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1892\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[1;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m   1891\u001b[0m     hook(\u001b[38;5;28mself\u001b[39m, prefix, keep_vars)\n\u001b[1;32m-> 1892\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_to_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\juan1\\anaconda3\\envs\\TallerIA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1802\u001b[0m, in \u001b[0;36mModule._save_to_state_dict\u001b[1;34m(self, destination, prefix, keep_vars)\u001b[0m\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1801\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1802\u001b[0m         destination[prefix \u001b[38;5;241m+\u001b[39m name] \u001b[38;5;241m=\u001b[39m param \u001b[38;5;28;01mif\u001b[39;00m keep_vars \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards = agent.train(EPISODES, STEPS, TOTAL_STEPS, writer_name = ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos las recompensas obtenidas durante el entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22b6ab4e7f0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABbOUlEQVR4nO3deXiU9b03/vc9S2ayzEw2socQlhAgbEJYA+pR40YtrQrYCu2v7WNtgWJ5ntOKbZ+entM22p5TbbXioccHz6llsUc8olYkVmWRfQl7whIg+57MJJNk1vv3x2SGRLJNlrnve+b9uq5cV5NMJp80mLzz/X6+n68giqIIIiIiIhlTSV0AERER0UAYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2GFiIiIhI9hhYiIiISPYYWIiIiEj2NFIXMFLcbjeqqqpgMBggCILU5RAREdEgiKKI1tZWpKSkQKXqex0laAJLVVUV0tPTpS6DiIiIhqC8vBxpaWl9vj9oAovBYADg+YKNRqPE1RAREdFgWCwWpKen+36P9yVoAot3G8hoNDKwEBERKcxA7RxsuiUiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYCEiIiLZY2AhIiIi2WNgISIiItljYAkRFc3t+I8DpbA73VKXQkRE5Legua2Z+vez/zmPT0vqodOosHrhOKnLISIi8gtXWEJAp8OFQ9caAQCny1qkLYaIiGgIGFhCwMmbzbB1bQWdqzRLXA0REZH/GFhCwP4r9b7/fa2+De12p4TVEBER+Y+BJQQcvNLg+99uEbhUbZGwGiIiIv8xsAS5hjYbLlR5AsrMNBMA4FwFt4WIiEhZGFiC3OdXPasrU5KNuHNyAgDgXCVXWIiISFkYWILcga7toCWT4jE91bPCcp6Nt0REpDAMLEFMFEVf/0r3wHKlrhUddpeUpREREfmFgSWIXatvQ42lE2EaFXLHxSLRqEN8lM7TeFvDbSEiIlIOBpYgtv+yZ3VlfmYs9Fo1BEFATqoRALeFiIhIWRhYgtjBrobbvInxvrd5t4V4UoiIiJSEgSVI2Z1uHCn1jOPPm3QrsOR4AwtXWIiISEEYWILUqbJmtNtdiI8Kw5Qko+/ttxpv29DpYOMtEREpAwNLkDrQNY5/8cR4qFSC7+3JJj1iI8PgcosormmVqjwiIiK/MLAEqVvHmcf0eLun8ZbbQkREpCwMLEGo2WrH2a4w0r3h1mu696QQG2+JiEghGFiC0KFrjRBFICsxCkkm/W3v9028rWJgISIiZWBgCULe/pW8iWN6fb93S+hybStsTjbeEhGR/DGwBBlRFG/dH5R1+3YQAKRGhyM6QguHS0QJG2+JiEgBGFiCzI3GdlS2dCBMrcL8zNheHyMIwq0Bcmy8JSIiBWBgCTLe7aA5GTGICNP0+bgc3txMREQKwsASZLzbQd2n2/bG13hbyUsQiYhI/hhYgojD5cbha55x/EsGCCw5KZ7AUlLTCrvTPeq1ERERDQcDSxA5U96CNpsTMRFaTOsKJH1Jjw2HKVwLu8uNy7VsvCUiInljYAki+7u2gxZNjIe62zj+3ngm3noGyLHxloiI5M6vwFJQUIDc3FwYDAYkJCRg+fLlKCkp6fGYf/qnf0J2djYiIyMRExODe++9F0ePHu33ed944w0IgnDbS2dnp/9fUQg72NVwu3SA7SAvNt4SEZFS+BVY9u3bh7Vr1+LIkSMoLCyE0+lEfn4+rFar7zFZWVl45ZVXcO7cORw8eBDjxo1Dfn4+6uvr+31uo9GI6urqHi96/e1TWql35g4HispbAAB5k3ofGPdF3j4WBhYiIpK7vs+99mLPnj09Xt+6dSsSEhJw8uRJLF26FADwta99rcdjfve73+H111/H2bNncc899/T53IIgICkpyZ9yqJvD1xrhFoHxYyKRGh0+qI/xnhS6VNMKh8sNrZo7hEREJE/D+g1lNnv+Mo+N7X1Amd1ux5YtW2AymTBz5sx+n6utrQ0ZGRlIS0vDsmXLcPr06X4fb7PZYLFYeryEMu/8lSW9XHbYl4y4CBj0GtidbLwlIiJ5G3JgEUURGzduRF5eHnJycnq87/3330dUVBT0ej1efPFFFBYWIj6+71+k2dnZeOONN7B7925s374der0eixcvxpUrV/r8mIKCAphMJt9Lenr6UL+UoHDwatc4/kFuBwFdjbdd20IXOI+FiIhkbMiBZd26dTh79iy2b99+2/vuvvtuFBUV4dChQ3jggQewYsUK1NXV9flcCxYswJNPPomZM2diyZIleOutt5CVlYWXX365z4/ZtGkTzGaz76W8vHyoX4rilTW242ZjOzQqAQsmxPn1sdPTOKKfiIjkb0iBZf369di9ezc+/fRTpKWl3fb+yMhITJw4EQsWLMDrr78OjUaD119/ffBFqVTIzc3td4VFp9PBaDT2eAlVB656toNmj41GlM6vtiRMS+HRZiIikj+/Aosoili3bh127dqFTz75BJmZmYP+OJvN5tfnKSoqQnJysj/lhayDV/zfDvLyNd5WW+B0ceItERHJk19/jq9duxbbtm3Du+++C4PBgJqaGgCAyWRCeHg4rFYrfvWrX+GRRx5BcnIyGhsb8eqrr6KiogKPP/6473nWrFmD1NRUFBQUAAB+8YtfYMGCBZg0aRIsFgv+8Ic/oKioCH/84x9H8EsNTi63iM+vDu7+oN6Mi4tElE6DNpsTV+vbkJ0UuitVREQkX34Fls2bNwMA7rrrrh5v37p1K775zW9CrVajuLgY//mf/4mGhgbExcUhNzcXBw4cwLRp03yPLysrg0p1a3GnpaUFTz31FGpqamAymTB79mzs378f8+bNG8aXFhrOVrTA0umEUa/BjNT+x/H3RqUSMC3FiKPXm3CuwszAQkREsuRXYBFFsd/36/V67Nq1a8Dn+eyzz3q8/uKLL+LFF1/0pxTq4r2dedGEeGiGOEclJ9WEo9ebcL7SjMfnhvZpKyIikidOClM4X/9Klv/bQV7ePhY23hIRkVwxsChYm82JU2XNAIAlE/1vuPXy3il0kY23REQkUwwsCnbkWiOcbhEZcREYGxcx5OcZHx+JyDA1Oh1ulDZYB/4AIiKiAGNgUTDvOP48P8bx90alEjDVO4+lgttCREQkPwwsCnZgCOP4+5LDPhYiIpIxBhaFqmzpQGm9FSoBWOjnOP7eeBtvzzOwEBGRDDGwKNTBru2gmenRMIVrh/1807s13rrc/R9fJyIiCjQGFoU6MIxx/L0ZPyYK4Vo12u0uXG9oG5HnJCIiGikMLArk7jaOf8kQxvH3Rt298ZbbQkREJDMMLAp0ocqC5nYHonQazEqPHrHn9Q2Qq7CM2HMSERGNBAYWBdrf1b+yYHwctEMcx98b70mh81VcYSEiInlhYFEg7zj+pcMYx98bX+NtlQVuNt4SEZGMMLAoTLvdiRM3mwAMf2DcF00YEwm9VoU2mxPXGznxloiI5IOBRWGOXm+CwyUiNTocmfGRI/rcGrUKU5I9jbecx0JENHzNVjtKalo5LmIEaKQugPzju515UjwEQRjx55+easLpshacqzDjy7NSR/z5iYhChSiKeHTzIZQ2WGEK12J+ZiwWTYjDwgnxyEqMGpWf4cGMgUVhvPcHjdT8lS9i4y0R0cioNnf6LpQ1dziw92It9l6sBQDER4Vh/vg4T4AZH4fM+EgGmAEwsChIraUTl2vbIAjAohEYx9+bnBRPYLlQ6Wm8Van4HxAR0VCU1LQCACYmROG3j83A4dJGHL7WiOM3mtDQZscHZ6vxwdlqAECSUY+FE+KwcIInxKTFREhZuiwxsCiId7rt9FQTYiLDRuVzTEqMQphGhVabEzeb2ke8T4aIKFQUdwWWKclGzB4bg9ljY/D9uybC5nThTLkZh6814tC1Bpwua0GNpRPvnK7EO6crAQDpseFYOD4OiybEY+GEOCQa9VJ+KbLAwKIgB33bQSN7Oqg7bVfj7ZnyFpyrNDOwEBENUUmNZwhndpKhx9t1GjXmZcZiXmYsNtw7CZ0OF07ebMahaw04fK0RZyrMKG/qQHlTBd46UQEAGD8msmv7KB4LxsciLkoX8K9HagwsCuF2izjYNY4/b+Lo9K94TU/1BJYLlWY8MjNlVD8XEVGw8q6wTE409Ps4vVaNxRPjsbhrVEWbzYnjN5pw+JpnC+l8lRml9VaU1lvx5pEyAJ4QtLCr/2X++LgRuQRX7hhYFKK4phUNbXZEhKlxR0b0qH4u34h+Hm0mIhoSh8uNa/Wei2QnJ/UfWL4oSqfB3ZMTcPfkBACAud2Bo9cbcehaI46UNqK4ptX3svXzG1AJwLQUExZNiMOCCXGYNy4Wkbrg+/UefF9RkPKeDpqfGQudRj2qn2taV+Pt+UozRFFk5zoRkZ+uN1jhcImI0mmQFhM+rOcyRWiRPy0J+dOSAAANbTYcLW3ybSGVNlhxrtKMc5Vm/Pv+UmhUAmakmXz9L3MyYqDXju7vjUBgYFGIg77bmUd3OwgAshINCFOrYOl0oqypHRlx7GMhIvKHdztoNOatxEfp8PCMZDw8IxkAUGPuxOHShq4m3kZUNHfgVFkLTpW14JVPryJMrcIdGdFYOD4eiybGYWZaNMI0ypsby8CiAJ0OF45e94zjH82GW68wjQrZyQacrTDjfKWFgYWIyE/ehtvJScZR/1xJJj2+MjsNX5mdBgAob2r39L+Uek4h1VpsOFLahCOlTXjxYyBcq8bccTFdR6jjkZNihGYEL9IdLQwsCnD8RhPsTjeSjHpMTIgKyOfMSTXhbIVnidGb4omIaHC8M1i+eEIoENJjI5AeG4EVuekQRRHXG6w41BVgjlxrRKPVjgNXGrpGZZTAoNNgXmasbw7MlCSjLGdwMbAogHccf94ojePvTU63PhYiIvKP74SQBIGlO0EQMH5MFMaPicKTCzLgdou4XNfq2z46WtoIS6cTfy+uw9+L6wAA0RFaLMi8NcRuYoI8rhFgYFGA/d3uDwqU7ieF2HhLRDR4bTYnKpo7AEizwtIflUpAdpIR2UlG/H+LM+Fyi7hYZcHh0gYcutaI49eb0NLuwJ4LNdhzoQaAp2fGe4T63qkJSDBIM8SOgUXm6lttuFTt2Qv1ntEPhKykKGjVAswdDlQ0dyA9lmOiiYgGw7sdlGjUITpidKaSjxS1SsD0NBOmp5nw1NIJcLjcOFthxpGu/pcTN5rR0GbDe2eq8N6ZKqTGzGNgod593nU6aGqyEfEBnGyo06gxOcmA85UWnK80M7AQEQ1SiW87aPQbbkeaVq3CnIwYzMmIwdq7PdcInC5rweGuGTC542Ikq42BRea89wctyQrc6opXTooJ5ystOFdpxoPT2XhLRDQYxX2M5FcinUaNBePjsGD86Fy46w/5n2MKYaIo+gbGLRnlcfy9yeHEWyIivw12JD/5h4FFxq7UtaGu1QadRoW5EizDeRtvvRNviYiof6IodtsSYmAZSQwsMrb/smd1ZV5mrCRjlScnGaBRCWhud6DK3Bnwz09EpDS1FhvMHQ6oVULA5maFCgYWGfOO418agHH8vdFr1cjqWtI8V8FtISKigXj7VzLjI4Pi/h45YWCRKZvThSOljQA8A+OkkpPq6XLnADkiooFxO2j0MLDI1Mmbzeh0uBEfpZO003w6G2+JiAbNN5KfDbcjjoFFpg52m24r5ZTZHDbeEhENmlxG8gcjBhaZOiDBOP7eTEk2Qq0S0Gi1o8bCxlsior44XW5crW8DAGQrcGic3DGwyFCT1Y7zVZ4tmLwAjuPvjV6rxqSuTnc23hIR9e1GoxV2pxsRYWqkxYRLXU7QYWCRoc+vNkAUPUOHEozS3NnQXfdtISIi6p13Oygr0QCVihfGjjQGFhk6KJPtIC823hIRDczXcMv+lVHBwCIz3cfxS3mcuTvfCkuVReJKiIjkiw23o4uBRWZKG6yoMnciTK3C/EzpL5sCPDdFqwSgvtWGWjbeEhH1ijNYRhcDi8wc6BrHP3dcDMLD5DElMTxM7RsxzcZbIqLbWW1OlDW1A+AJodHCwCIz3nH8SyQax98X3txMRNS3y7We1ZUxBh1iI8MkriY4MbDIiMPlxuFrnnH8cmm49fI23l6oYmAhIvoiNtyOPgYWGTld1gKr3YXYyDBMTZbXkiJPChER9c3XcMuR/KOGgUVGDnadDlo8MV52Z/inJBshCJ6r0+ta2XhLHhXN7Xj4Dwfw1vFyqUshkhQbbkcfA4uM7JfZ/JXuInUaTBjjabzlADnyevtkJS5UWfD8nmLYnC6pyyGShCiKKK7xjH1gw+3oYWCRCXO7A2crWgDIM7AAt7aFzldyHgt5HLvh6blqstpReLFW4mqIpFHfakNzuwMqAZiUGCV1OUGLgUUmDl1rgFsEJoyJRLJJnndQ8KQQdedwuXHqZovv9R3HuC1EocnbvzIuLhJ6rTzGUQQjBhaZOCDT48zd5aR4ljq5JUSA599Bh8OFyDA1BMFzJL+ssV3qsogCjv0rgcHAIhPecfxy3Q4CgGmpJggCUG3uREObTepySGLHrjcB8DSJe28V33miTMqSiCRR7DvSzP6V0eRXYCkoKEBubi4MBgMSEhKwfPlylJSU9HjMP/3TPyE7OxuRkZGIiYnBvffei6NHjw743G+//TamTp0KnU6HqVOn4p133vHvK1Gwm41WlDd1QKsWsGC8PMbx9yZKp0FmfCQArrLQrcAyLzMWq3LHAgD+eqICTpdbyrKIAq6k1tPXxxWW0eVXYNm3bx/Wrl2LI0eOoLCwEE6nE/n5+bBarb7HZGVl4ZVXXsG5c+dw8OBBjBs3Dvn5+aivr+/zeQ8fPoyVK1di9erVOHPmDFavXo0VK1YMKugEA+/poNljYxCp00hcTf9uNd4ysIQyt1vE8Ru3Ast9UxMRFxmGulYbPimuk7g6osBxuUVcqW0DwKFxo82vwLJnzx5885vfxLRp0zBz5kxs3boVZWVlOHnypO8xX/va13Dvvfdi/PjxmDZtGn73u9/BYrHg7NmzfT7vSy+9hPvuuw+bNm1CdnY2Nm3ahHvuuQcvvfTSkL8wJfHOX1kq4+0gr5wUNt4SUFLbCkunE5FhakxNNiJMo8Kjc9IAADs4k4VCyI1GK2xON8K1aoyNjZC6nKA2rB4Ws9nzSys2NrbX99vtdmzZsgUmkwkzZ87s83kOHz6M/Pz8Hm+7//77cejQoT4/xmazwWKx9HhRIqfLjUNXPUdD82TccOuVw6PNhFvbQXPGxUKj9vwYWZmbDgD4rKQO1eYOyWojCiRvw21WYpTsBn4GmyEHFlEUsXHjRuTl5SEnJ6fH+95//31ERUVBr9fjxRdfRGFhIeLj+149qKmpQWJiYo+3JSYmoqamps+PKSgogMlk8r2kp6cP9UuR1JkKM1ptTpjCtb7tFjmbluppKqts6UCT1S5xNSQVb2CZn3nrj5UJY6IwLzMWbhF463iFVKURBVQxTwgFzJADy7p163D27Fls3779tvfdfffdKCoqwqFDh/DAAw9gxYoVqKvrf19bEHomU1EUb3tbd5s2bYLZbPa9lJcrcxn6YFf/yuKJcVArIJ0b9Vo23oY4URRxtCuw5I7rubr6xDzPHw5vnSiHyy0GvDaiQCup8Tbc8oTQaBtSYFm/fj12796NTz/9FGlpabe9PzIyEhMnTsSCBQvw+uuvQ6PR4PXXX+/z+ZKSkm5bTamrq7tt1aU7nU4Ho9HY40WJbh1nlv92kNe0rnks7GMJTTca29HQZkOYRoUZaT1XBR/MSYYpXIvKlg7fv22iYMZbmgPHr8AiiiLWrVuHXbt24ZNPPkFmZuagP85m63tux8KFC1FYWNjjbXv37sWiRYv8KU9xWjsdOF3eAgC+ORZKwJNCoe3YdU/P1az06Numeuq1anxldioATr6l4Ndud+Jmk2dYIreERp9fgWXt2rV48803sW3bNhgMBtTU1KCmpgYdHZ4GO6vViueeew5HjhzBzZs3cerUKXznO99BRUUFHn/8cd/zrFmzBps2bfK9vmHDBuzduxcvvPACiouL8cILL+Djjz/GM888MzJfpUwdvtYIl1vEuLgIpCuou3w6R/SHtKO99K9098Q8z0yWjy/Vor6VAwYpeF2pbYMoAvFRYYiP0kldTtDzK7Bs3rwZZrMZd911F5KTk30vO3fuBACo1WoUFxfj0UcfRVZWFpYtW4b6+nocOHAA06ZN8z1PWVkZqqurfa8vWrQIO3bswNatWzFjxgy88cYb2LlzJ+bPnz9CX6Y8HVTAOP7eTOsKLBXNHWhpZ+NtqDnWR/+K1+QkA2aPjYbTLeK/T7L5loIXR/IHll9TykSx/yY6vV6PXbt2Dfg8n3322W1ve+yxx/DYY4/5U47iHehquM1TwPyV7kzhWmTEReBmYzvOV1oUVz8NXVVLByqaO6BWCbgjI6bPxz2ROxany1qw83gZnr5zfL8N9ERK5TshlKjMHkql4V1CEilvasf1BivUKgELJ8h3HH9fOEAuNHmn2+akGBHVz1TmZTOTEaXT4EZjOw6XNgaqPKKA8o7kZ8NtYDCwSMS7HTQrPRpGvVbiavyXw8bbkHS02/1B/YkI0+CRWSkA2HxLwYtbQoHFwCIR7/wVOd/O3B/fSaEqBpZQcnyA/pXunui6EHHP+Ro0c8ggBZmGNhsa2uwQBCArkYElEBhYJOByi/j8mrIDS07XxNubje0wdzgkroYCobHNhit1nkveBhNYpqeZMC3FCLvLjV2nK0e7PKKA8q6uZMRGIDxMPcCjaSQwsEjgfKUZLe0OGHQazEyLlrqcIYmOCENaTDgA4AK3hULC8RvNAIDJiQbERIYN6mNWdR1x3nGsbMCmfSIluVTtnXDL1ZVAYWCRgLd/ZeGEON/FcUrEeSyh5dgg+1e6+/KsFIRr1bhS14ZTZc2jVRpRwN3qX+EJoUBR7m9LBdt/uWscf5ay5q98UQ4DS0jxnhDK9SOwGPVaPDwjGQCwnc23FERKajmSP9AYWALManP6/tJcoqBx/L3xrrBcqLJIXAmNttZOBy50NVjPG0T/SnfeCxHfP1sFSyf7nUj5XG4Rl2t5QijQGFgC7Oj1RjhcItJiwpERp5xx/L3xrrBcb7DyF1GQO3mzGW4RyIiLQJJJ79fH3jE2BpMSotDpcONdNt9SEChrakenww2dRoVxcZFSlxMyGFgC7MCVW+P4lT79MzYyDKnR3sZbrrIEs4HG8fdHEARf8+32Y+VsviXFK6nx/LzLSjRArVL2z3ElYWAJsAMKn7/yRd7jzRwgF9y8/Sv+NNx299XZqQhTq3Cx2sKeJ1K8Yg6MkwQDSwBVmztwta4NKgFYpMBx/L3hALng1+lw4Uy55/vb1w3NA4mJDMMDOUkA2HxLyuc9IcSG28BiYAkg7+rK9LRoREcMbo6F3PGkUPArKm+B3eVGolGHsbFD77ta1dV8u7uoElabc6TKIwo4juSXBgNLAHnH8S8Nku0goGfjbRt/CQWl7v0rw+m7Wjg+DuPiImC1u/D+2aqRKo8ooDodLtxotAJgYAk0BpYAcbtFfN41MC5P4ceZu4uP0iHZpIcocuJtsPL2rwx1O8hLEASszL3VfEukRFdq2+AWPYcOxkTppC4npDCwBMjFagsarXZEhqkxe2yM1OWMKN/NzZzHEnQcLjdO3vTMDZqXOfy+q8fmpEGjElBU3oLiGv57IeXx/rudnGhQ/ElPpWFgCRDvOP4F4+MQpgmu/9t9jbdcYQk6F6osaLe7EB2hxaSEqGE/3xiDDvdOSQQA7OAqCykQ+1ekE1y/OWXswJWucfxB1L/i5T3azMbb4HPseiMAYG5GLFQjNG/C23y761QFOh2uEXlOokDhSH7pMLAEQIfd5bvpNm+Ssu8P6o13S+hafRtPfwSZY9c9/26H27/S3ZJJY5AaHQ5LpxMfnq8eseclCgTOYJEOA0sAHLvRBLvTjWSTHhPGBN8Y5wSDHolGHUTR06tDwcHtFoc9MK43apWAFXM9qyxsviUlabLaUd9qA+CZckuBxcASAAe7bQcFa5MW+1iCz+W6Vpg7HIgIU2NainFEn3tFbhpUgufI9LX6thF9bqLR4m24HRsbgUidRuJqQg8DSwB4B8YF43aQ17QUDpALNse75q/MyYiBRj2yPyqSTeG4a3ICAGDnca6ykDKw4VZaDCyjrM7SieKaVghCcM1f+SKusASfo12BZd4QLjwcjFW5nm2ht09WwO50j8rnIBpJHMkvLQaWUeY9zjwtxYjYyOAYx9+b6WmewHK1rg3tdjbeKp0oir4JtyPZv9LdP2QnIMGgQ6PVjsKLtaPyOYhG0iWusEiKgWWUHfTdzhy820EAkGjUY4xBB7cIXKpulbocGqabje2oa7UhTK3CzPToUfkcGrUKj89NAwDsOF42Kp+DaKS43SKu8EizpBhYRpEoijjQtcKyJIi3g7y4LRQ8jnWdDpqZboJeqx61z7NyrmdU/4ErDShvah+1z0M0XOXN7Wi3uxCmUWFcXPCd9lQCBpZRVFLbivpWG/RaFeaMC65x/L3JSeEAuWAx2ttBXmPjIny9XWy+JTnzzl+ZOCZqxJvQaXD4//oo8m4Hzc+Mg04zen+lykUOV1iCxq3AMvz7gwbinXz715PlcLrYfEvyxIZb6TGwjKL9vv6V4N8OAm413l6pa+PIdQWrMXeirKkdKgG4Y2z0qH+++6YmIjYyDLUWGz4tqR/1z0c0FL7AkszAIhUGllHS6XD57mEJ9oZbrySjHvFRYXC5RVzixFvF8vavTEsxwaDXjvrn02nUePSOVADAjmNsviV58t3SnDSyQxRp8BhYRsnJm83odLiRYNAhK3H4t9wqgSAIvgFy3BZSLm/QHu3+le5W5nqabz8tqUO1uSNgn5doMDodLtxo9DSFc0tIOgwso+TWdNvgHcffG+9JITbeKlegGm67m5gQhXnjYuEWgb+eqAjY5yUajKt1bXC5RURHaJFg0EldTshiYBklB7rdHxRKcnyBhVtCStRsteNyredun9xRmnDbF2/z7c7j5XC7xYB+bqL++EbyJxpC6g9QuWFgGQWNbTZcqPL8wl4cAvNXuvM13ta2svFWgby3M09KiAr4ZOaHpifDqNegsqXDN7+ISA5KODBOFhhYRoF3HH92kgEJBr3E1QRWikmPmAgtnG7R91cJKYcU20Feeq0aX72ja/Itm29JRop9I/nZcCslBpZR4J2/sjQrNE4HdScIQrdtIfaxKI33hJAUgQW4tS1UeLEW9a02SWog+qIS3wkhrrBIiYFlhImi6FthCebbmfvDEf3K1GZz+rYypQos2UlGzEqPhtMt4u1TbL4l6bW021Fr8YRnBhZpMbCMsGv1bag2dyJMo5Lsh77UfIGlioFFSU7dbIbLLSI9NhzJpnDJ6niiW/OtKLL5lqTl3Q5KiwlHlE4jcTWhjYFlhHmPM88bFzuql8bJmXdLqKSmFTYnG2+Vwte/Mm70x/H3Z9mMFESGqXG9wYojpU2S1kLEkfzywcAywg6E2Dj+3qTFhMMUroXDJeJyTZvU5dAgeQPLfIlXBiN1Gjwyq2vy7XE235K0bjXcMrBIjYFlBNmdbhwp9UwJzQvhwCIIAgfIKUynw4WiihYAQK4MtjK920Ifnq9BS7td4moolJVwJL9sMLCMoFNlzWi3uxAXGYYpIf6PO4d9LIpytsIMu9ONMQYdxsVFSF0OpqeaMDXZCLvTjV2nKqUuh0KU2y36BilyS0h6DCwj6GC3cfwqVWhPQ+RJIWXpfn+QHCZ5CoLgW2XZcbyMzbckicqWDrTZnNCqBWTGR0pdTshjYBlB3nH8oXqcubucVM8KU3F1K+xOt8TV0ECO+hpupd8O8vry7FTotSpcrm3DqbIWqcuhEOTtX5kwJgpaNX9dSo3fgRHS0m7H2a7VhCWTQm9g3BeNjY2AUa+B3eXG5VpOvJUzp8uNUzebAUg3f6U3Rr0WD09PAcDJtyQNb/8Kt4PkgYFlhHx+tRGi6LmDJckUWuP4e9N94i23heTtYrUFVrsLRr0GkxPl9YPZuy30/tlqtHY6JK6GQg1H8ssLA8sIOXjVezszV1e8OEBOGbrfHyS33qs5GTGYmBCFDocL7xZVSV0OhRjOYJEXBpYRIIoi9l/m/JUvmuY72myRuBLqj7d/JVdG/StegiBgVe6t5luiQLE5XShtsALgDBa5YGAZATca21HZ0gGtWsD88fL7oS8V7wrLpWoLHC423sqR2y3ihMQXHg7kq3ekIUytwvlKC85VcLWOAuNanRUutwijXoNkbvPLAgPLCDjYdTpoTkYMIsJ414RXRmwEDDoN7E43rtRy4q0cXa1vQ3O7A+Fata/nSG5iI8Nwf04SAGA7V1koQEpqvQ23Rlkc9ScGlhGx3zeOn/0r3alUAqZ1HW9mH4s8ebeD5mTEyPrY5hNd20K7i6pgtTklroZCAUfyy498f0IphMPlxpFrnqFb7F+5HQfIydsxGfevdLdgfBwy4iLQZnPig7PVUpdDIaCEgUV2GFiG6Ux5C1ptTkRHaDEtRZ5L6lLK4Z1CsiWKIo5fl3f/ipdKJWBl1yoLt4UoEHhCSH78CiwFBQXIzc2FwWBAQkICli9fjpKSEt/7HQ4HfvzjH2P69OmIjIxESkoK1qxZg6qq/o8jvvHGGxAE4baXzs7OoX1VAeS9nXnxxHioZXYkVA5yujXeOtl4KyvlTR2osXRCqxYwe2y01OUM6LE5adCoBJwua/H9MiEaDeZ2B6rNnt8/WQwssuFXYNm3bx/Wrl2LI0eOoLCwEE6nE/n5+bBaPUe/2tvbcerUKfzsZz/DqVOnsGvXLly+fBmPPPLIgM9tNBpRXV3d40Wvl39ntncc/xKO4+9VZlwkonQadDrcuFZvlboc6uZo1/1BM9OiodeqJa5mYAkGPe6ZkgAA2M7JtzSKSrqmc6dGh8Oo10pcDXn5daRlz549PV7funUrEhIScPLkSSxduhQmkwmFhYU9HvPyyy9j3rx5KCsrw9ixY/t8bkEQkJSU5E85kjN3OHCm65hlHvtXeqVSCZiaYsSx6004V2nmfrCMHO86zpwr8+2g7lbNG4uPLtTindOVePbBbEUELVIe70h+/rySl2H1sJjNnl/WsbF9/8Azm80QBAHR0dH9PldbWxsyMjKQlpaGZcuW4fTp0/0+3mazwWKx9HgJtMPXGuFyixgfH4m0mIiAf36lyElh460cHVNI/0p3SyeNQWp0OMwdDuw5XyN1ORSkeEJInoYcWERRxMaNG5GXl4ecnJxeH9PZ2Ylnn30WX/va12A09n0XQ3Z2Nt544w3s3r0b27dvh16vx+LFi3HlypU+P6agoAAmk8n3kp6ePtQvZchujePn6kp/pqd5vvdsvJWPWksnbjS2QyV4jjQrhVol4PG5aQC4LUSjhw238jTkwLJu3TqcPXsW27dv7/X9DocDq1atgtvtxquvvtrvcy1YsABPPvkkZs6ciSVLluCtt95CVlYWXn755T4/ZtOmTTCbzb6X8vLyoX4pQ3awq+E2j/NX+uU92nyxygKXW5S4GgJura5MSTYqbo9+xdx0qATPDJnSeg4kpJEliqKvh4UrLPIypMCyfv167N69G59++inS0tJue7/D4cCKFStw/fp1FBYW9ru60mtRKhVyc3P7XWHR6XQwGo09XgKpvKkdNxrboVEJWMBx/P3KjI9CRJgaHQ4Xf8HIxHGZj+PvT0p0OO7M8vyRsPN44P9QoeBWZe5Ea6cTGpWA8fFRUpdD3fgVWERRxLp167Br1y588sknyMzMvO0x3rBy5coVfPzxx4iLi/O7KFEUUVRUhOTkZL8/NlC8x5lnj42GQWF/oQaaWiVgajK3heTEu8IyX4GBBfA03wLAf5+sgN3J4/I0crwNtxPGRCFMw1FlcuLXd2Pt2rV48803sW3bNhgMBtTU1KCmpgYdHR0AAKfTicceewwnTpzAX/7yF7hcLt9j7Ha773nWrFmDTZs2+V7/xS9+gY8++gilpaUoKirCt7/9bRQVFeHpp58eoS9z5HmPM+dN5HbQYHCAnHy0tNt9TYVyn3Dbl3/ITsAYgw6NVjs+vlQrdTkURNhwK19+BZbNmzfDbDbjrrvuQnJysu9l586dAICKigrs3r0bFRUVmDVrVo/HHDp0yPc8ZWVlqK6+NV67paUFTz31FKZMmYL8/HxUVlZi//79mDdv3gh9mSPL5Rbx+dWu+4Oy2HA7GBzRLx/HbzQDACaMiURclE7iaoZGq1bh8TlsvqWRV1zNwCJXfs1hEcX+GybHjRs34GMA4LPPPuvx+osvvogXX3zRn1IkdbaiBZZOJwx6DWbI9IZbuZme5vn/6UKVBW63CBWnAkvmVv+K/9u1crIyNx2vfnYNB682oLypHemxHC1Aw8cTQvLFDboh8J4OWjwhHhoZ33ArJxPGREGvVaHd7kJpAyfeSumowvtXvDLiIrF4YhxEEXjrBJtvafjsTjeudR0M4AqL/PC37RAc8B1n5nbQYHVvvOW2kHSsNqfv/38lnhD6olW5nubbt06U864qGrbShjY43SIMOg1So8OlLoe+gIHFT202J06VeXoAlnL+il+ms/FWcqfKmuFyi0iNDkdKEPxAzp+WiJgILWotNnxWUi91OaRw3u2grCQDBIHb1nLDwOKno6WNcLpFjI2NwNg47pn7I4eNt5I7HiTbQV46jRqP3uFpvt1xnM23NDw8ISRvDCx+8m4HcRy//77YeEuBd1SB9wcNZNU8z7UcnxTXocbcKXE1pGTeFZYpDCyyxMDiJ+/8FQYW/00cEwWdRoU2mxM3Gtl4G2g2pwuny1sABFdgmZhgQO64GLhF4K9svqVhKPGtsAR2cjoNDgOLH6paOnCt3gqVACycwMDiL41ahSmceCuZsxVm2J1uxEeFITM+UupyRpS3+XbniXKu3tGQWDodqGzxDEGdnMgVFjliYPGD9zjzzPRomMI5jn8ovI23F6osElcSeo512w4KtobCh2ckw6jXoKK5Awe7hjoS+eNy1+pKskkPUwR/vssRA4sf9nu3gyZydWWofCeFKrjCEmi+wKLQcfz90WvV+MrsVABsvqWhYcOt/DGwDJK7xzh+HmceqmmpXbNYqsyDmopMI8PlFnHypuc4vtIn3PbFeyFi4cVaNLTZJK6GlKaEgUX2GFgG6UKVBc3tDkTpNJiVHi11OYqVlWhAmEaF1k4nbja2S11OyLhUbUGbzXOdRLD+QJ6SbMTM9Gg4XCLePlkhdTmkMBzJL38MLIN04KpnO2jB+DhoOY5/yLRqle/IIBtvA8d7nDl3XCzUQXyP0xO5niPOO4+XcwWPBk0URRTXePrqJifyhJBc8TfvIB24zPkrI8U3QK6KgSVQjl1vBBBcx5l786WZKYgMU6O0weoLaUQDqbF0wtLphFolYEJCcJ2gCyYMLIPQYXf59v8ZWIZvOifeBpQoijh+w/PvNzcIG267i9Rp8MisFADAjmNsvqXB8Tbcjo+PhE6jlrga6gsDyyAcvd4Iu8uN1OjwoJtfIYVbI/otXLYPgGv1bWiy2qHXqnxhMZh5Z7L87XwNWtrtEldDSsCGW2VgYBmE7uP4g21+hRSyEg0IU6tg7nCgvKlD6nKCnndr5I6xMQjTBP9/8jPSTJiSbITd6cY7pyulLocUgA23yhD8P71GgHdgXB63g0ZEmEbl+0uGfSyj71gQ3h/UH0EQ8ETX/UI7jrH5lgZWzJH8isDAMoBaSydKalshCMBijuMfMd5tIZ4UGl2iKAb1wLi+fHlWKvRaFUpqW333JxH1xuFy42odV1iUgIFlAN7VlempJsREhklcTfDI8Q6QY2AZVRXNHag2d0KjEjB7bIzU5QSMKVyLh6YnA2DzLfXveoMVDpeIyDA1UqPDpS6H+sHAMgDv7cx5HMc/oqZ3W2Hhkv3o8a6uzEgzITwstE4/PNE1+fa9M9Vo7XRIXA3JlXc7KCvJAFUQzygKBgws/ei+nL5kEsfxj6TJSQZo1QJa2m/dkEoj71b/SnCO4+/P3IwYTEyIQofDhd1nqqQuh2SqpGtgHLeD5I+BpR+CIKBw453Y+s1c3JERLXU5QUWnUSOr6wp3bguNnmM3vIEldLaDvARBwKrcW823RL3xHWlOZGCROwaWAUTqNLg7O4HDhEZBTgobb0dTXWsnrjdYIQjAnIzQabjt7qt3pCFMrcK5SjODMfWKJ4SUg4GFJJOT5g0sFokrCU7Hr3um205JMsIUrpW4GmnERoYhf1oiAGDHcTbfUk9tNicqmj1b0twSkj8GFpJM9xH9bLwdeaFyf9BAvM23756uQrvdKXE1JCfe7aAEg46nQBWAgYUkk51kgEYloMlqR7W5U+pygs7REBsY15eF4+MwNjYCrTYnPjhbLXU5JCO+CbfJ3A5SAgYWkoxeq8akrkY39rGMLHO7AyW1nh/GwX7h4UBUKgErvc23x9l8S7fwhJCyMLCQpHJSOEBuNJy42QRRBMaPicQYg07qciT3+Jw0qFUCTt5sxuWuIEdUzBNCisLAQpKansaTQqMhFMfx9yfBqMc92QkAgO2cfEvwzNnyrkLylmZlYGAhSeWw8XZU3Jq/wsDi5W2+fed0JTodLomrIanVtdrQ0u6AWiVgYkKU1OXQIDCwkKSmJhuhVgloaLOj1mKTupyg0G534lyFZ8WKgeWWpVljkGLSo6XdgY8u1EhdDknMux00Li4Cei3nbCkBAwtJSq9VY+IYz1833BYaGafLWuB0i0iNDkdaTITU5ciGWiXg8bme5ltuC9GthlueEFIKBhaSXE4q+1hGkvc4c+640BvHP5AVuekQBOBIaROuN1ilLockdGvCLftXlIKBhSQ3PdXzF84FBpYRcTyELzwcSGp0OO7M8lxkysm3oa2EgUVxGFhIcjwpNHLsTjdOlXlG8rN/pXercj3Nt2+frIDd6Za4GpKC0+XGlbo2AJzBoiQMLCS5KclGqARP136dhRNvh+NcZQtsTjfiIsMwYUyk1OXI0j1TEhAfpUNDmx1/v1QrdTkkgRuN7bA73YgIUyOdfV6KwcBCkosI02ACG29HxK3+lVgIgiBxNfKkVavw+Nw0AMB2Tr4NScVdDbeTEg1QqfjfiVIwsJAsTGfj7Yg4zvuDBmVV16j+A1fqUd7ULnE1FGi+O4Q44VZRGFhIFm4NkLNIXIlyudwiTtxg/8pgZMRFYtGEOIgi8NcTXGUJNTwhpEwMLCQL3sZb3ik0dJeqLWi1OWHQaTCFt88OaFXX5Nu3TlTA6WLzbSjxrbAwsCgKAwvJwtRkIwQBqLF0or6VE2+Hwnt/0JxxMVBzX35A909LREyEFjWWTuy7XC91ORQgVpsTZV3bgFxhURYGFpKFSJ0G4+M9p1q4yjI0x3l/kF90GjW+ekdX8+0xbguFCu9t3fFROsRF8SZzJWFgIdmYnsptoaESRdG3wjKfgWXQnpjnab79tKQOtTxSHxK4HaRcDCwkGxzRP3TX6q1otNqh06gwPTVa6nIUY2KCAXMzYuByi2y+DRFsuFUuBhaSjRyusAyZdzto9thohGn4n7U/vM23O0+Uw+0WJa6GRhtH8isXf7KRbExL8ZxsqTJ3orGNjbf+OMb7g4bs4enJMOg1KG/qwOfXGqQuh0aRKIoo6ephmcJbmhWHgYVkw6DX3mq8reI8Fn+wf2XowsPU+MrsVADAX47wQsRgVt9mQ5PVDpUATEqMkroc8hMDC8kKt4X8V9HcjsqWDmhUAmaPjZa6HEX6+vwMCAKw50INispbpC6HRol3O2hcXCT0WrXE1ZC/GFhIVnJSPcu05yoYWAbL27+Sk2pCRJhG4mqUaXKSAV+d7Tni/C/vX4QospclGLF/RdkYWEhWeFLIf9wOGhk/emAywrVqnLzZjPfPVktdDo0CnhBSNgYWkhVvYKls6UCz1S5xNcpwlBcejohEox5P3zkBAPD8h8XodLgkrohGGmewKBsDC8mKUa/FuLgIAMD5Kq6yDKS+1YbSeisEAZibwcAyXE8tHY9kkx6VLR14/eB1qcuhEeRyi74pt5N5QkiRGFhIdrgtNHgnuvpXJicaYIrQSlyN8oWHqfGjByYDAF799CrqWjn9NljcbLTC5nRDr1VhbGyE1OXQEPgVWAoKCpCbmwuDwYCEhAQsX74cJSUlvvc7HA78+Mc/xvTp0xEZGYmUlBSsWbMGVVVVAz7322+/jalTp0Kn02Hq1Kl45513/P9qKCjwpNDgHWX/yoj78sxUzEwzwWp34Xd7L0tdDo0Q73ZQVqKBl4MqlF+BZd++fVi7di2OHDmCwsJCOJ1O5Ofnw2q1AgDa29tx6tQp/OxnP8OpU6ewa9cuXL58GY888ki/z3v48GGsXLkSq1evxpkzZ7B69WqsWLECR48eHfpXRoo1nSssg8aBcSNPpRLws2VTAXim317kTKCg4Gu4TWT/ilIJ4jDO79XX1yMhIQH79u3D0qVLe33M8ePHMW/ePNy8eRNjx47t9TErV66ExWLBhx9+6HvbAw88gJiYGGzfvn1QtVgsFphMJpjNZhiN3J9UMnO7AzP/eS8A4Mz/zedWRx/MHQ7M+ue9EEXg2E/uQYJBL3VJQWXttlP44Gw1Fk2Iw1++Mx+CwL/Kley7fz6Bjy7U4qcPT8F3loyXuhzqZrC/v4fVw2I2e/4Cjo3teznabDZDEARER0f3+ZjDhw8jPz+/x9vuv/9+HDp0qM+PsdlssFgsPV4oOJgitL49Zjbe9u3UzWaIIpAZH8mwMgqefSAbYRoVDl1rxMeX6qQuh4bp1gkh/kGrVEMOLKIoYuPGjcjLy0NOTk6vj+ns7MSzzz6Lr33ta/2mppqaGiQmJvZ4W2JiImpqavr8mIKCAphMJt9Lenr60L4QkiXfADluC/XJd5x5HPtXRkN6bAS+nZcJAPj13y7B7nRLXBENVbvdiZtN7QA4g0XJhhxY1q1bh7Nnz/a5ZeNwOLBq1Sq43W68+uqrAz7fF5dbRVHsdwl206ZNMJvNvpfycl4NH0x4Umhgx643AuD8ldH0/bsmID4qDNcbrPjzkZtSl0NDdKW2DaIIxEWGYYxBJ3U5NERDCizr16/H7t278emnnyItLe229zscDqxYsQLXr19HYWHhgD0lSUlJt62m1NXV3bbq0p1Op4PRaOzxQsHD23h7gYGlVx12F852XV/AwDJ6DHot/ne+55jz7z++zGGGCsWR/MHBr8AiiiLWrVuHXbt24ZNPPkFmZuZtj/GGlStXruDjjz9GXNzApxcWLlyIwsLCHm/bu3cvFi1a5E95FERyUjyB5UZjOyydDomrkZ/T5c1wukUkm/RIiwmXupygtmJuOrKTDLB0OvH7v1+RuhwaAo7kDw5+BZa1a9fizTffxLZt22AwGFBTU4Oamhp0dHQAAJxOJx577DGcOHECf/nLX+ByuXyPsdtv/WWyZs0abNq0yff6hg0bsHfvXrzwwgsoLi7GCy+8gI8//hjPPPPMyHyVpDgxkWFIjfb8IuY8ltsd6zaOn6dXRpe62zHnPx+5iat1bRJXRP4qqfUcyuBIfmXzK7Bs3rwZZrMZd911F5KTk30vO3fuBABUVFRg9+7dqKiowKxZs3o8pvuJn7KyMlRX37pcbNGiRdixYwe2bt2KGTNm4I033sDOnTsxf/78EfoySYmmc4Bcn7yBJZcNtwGxeGI87p2SAJdbRMHfLkldDvnp1pYQWweUzK+76Aca2TJu3LhBXcv+2Wef3fa2xx57DI899pg/5VCQm55mwp4LNThfySPr3dmdbpwqawbACbeB9NxDU/BZST3+XlyHg1cakDcpXuqSaBAa2mxoaLNDEICsxCipy6Fh4F1CJFsc0d+781VmdDrciI0Mw8QE/gAOlPFjorB6YQYA4JcfXITLPeSZmxRA3tWVsbERiAjz6290khkGFpKtnBTP8m1pgxWtbLz1ubUdFMP+lQDbcM8kmMK1KK5pxc7jHKWgBMW+gXHsX1E6BhaSrbgoHVJMngmuF3ifiw/7V6QTHRGGDfdMAgD8rrCEQVoBSmo8PzvYv6J8DCwka9wW6snlFnH8hveGZl54KIXVCzMwPj4SDW12/PHTa1KXQwMo4QpL0GBgIVnjSaGeSmpa0drpRJROgynJ/AEsBa1ahecemgIA+H8Hr6O8a+Q7yY/bLeJyrecYOmewKB8DC8laThpH9HfnHcc/JyMGGjX/85XKPVMSsHhiHOwuN57/sFjqcqgPZU3t6HC4oNOoMC4uUupyaJj4E49kzTvxtrTBijabU+JqpHfsxq2BcSQdQRDw04enQiUAH5yr9m3Tkbx4G24nJUZBrWKDutIxsJCsjTHokGTUQxSBiyHeeCuKIo5d98xfYWCR3pRkI1bmem6J/5f3L8LNY86y4xsYl8iG22DAwEKyx8Zbj+sNVjS02RCmUWFG11YZSWvjfZMRpdPgbIUZ/1NUKXU59AUcyR9cGFhI9th46+E9zjw7PRo6jVriagjwrAB+/+4JAIDf7ClBu53blnLCSw+DCwMLyV5Oqmc5N9Qbb7tfeEjy8a3FmUiLCUeNpRNb9pdKXQ516XS4cKPBCoArLMGCgYVkz7vCcq2+LaT/gmXDrTzptWo8+2A2AODf95WixtwpcUUEAFdq2+AWgZgILcYYdFKXQyOAgYVkL8GoR4JBB7cIXKoOzcbbypYOVDR3QK0ScMfYGKnLoS94eHoy5mbEoMPhwm8+4jFnOSj2Tbg18AqLIMHAQorgXWU5VxGa20LHu7aDclJNiNTxAje5EQQBP1s2FQCw61Qlzla0SFsQdZtwyxNCwYKBhRTBe1LoXGVorrAc9favjOPqilzNTI/GV2anAvAccxZFHnOWUkktG26DDQMLKUKoH20+7utf4f1BcvajByZDr1Xh+I1m7DlfI3U5IY0nhIIPAwspgndL6EpdKzrsLomrCayGNhuu1nnuQ8nlCousJZvC8dRSzzHngg+LYXOG1r9VuWiy2lHfagMAZCUysAQLBhZShESjDvFRXY23NaG1LXSia3VlcqIB0RFhEldDA3n6zvFINOpQ1tSONz6/IXU5IcnbcJseG44o9nwFDQYWUgRBEDC9ax5LqG0LcRy/skSEafCP93uOOb/yyVU0tNkkrij0cCR/cGJgIcXICdGTQsdueG5oZmBRjq/OTkVOqhGtNideLLwsdTkh59YJIW4HBRMGFlKMWyeFQiewWDodvksfGViUQ6US8LOHPcectx8r8/0CpcBgw21wYmAhxbjVeNuGTkdoNDOevNkMtwhkxEUg0aiXuhzyw/zxcXhgWhLcIvDLD3jMOVDcbhGXu440T0lmYAkmDCykGMkmPeIiw+Byi76/oILdcd/8Fa6uKNGmh7IRplbhwJUGfFZSL3U5IaGiuQPtdhfCNCqMi4uUuhwaQQwspBiCIGBa1yrL3guhMeOCFx4qW0ZcJL65eBwAzyqLw+WWtqAQ4D0hNHFMFDRq/ooLJvxukqLck50AAHj1s2t49u2zQT3notPhwpmuEe/zOTBOsdb9w0TERobhWr0V246WSV1O0GPDbfBiYCFFWbMwA/94/2QIArDjeDlW/vsRVJs7pC5rVJwua4HDJSLRqEN6bLjU5dAQGfVa/PC+LADAix9fhrndIXFFwa2YI/mDFgMLKYogCFh790Rs/WYuTOFaFJW34EsvH8TR0kapSxtx3cfx87ZZZXsiNx1ZiVFoaXfgD59ckbqcoFbCE0JBi4GFFOmuyQl4b10espMMaGiz4+v/cRRbP78eVCcx2L8SPDRqFX7Sdcz5vw7fwPUGq8QVBSeb0+X7/5a3NAcfBhZSrLFxEdj1/UV4ZGYKnG4Rv3jvIja+dSYo7hpyuNw4edMz4XY+A0tQuDNrDO6aPAYOl4hf/+2S1OUEpat1bXC5RZjCtUg06qQuh0YYAwspWkSYBr9fNQs/fXgK1CoB75yuxKObD6G8qV3q0oblfKUZHQ4XoiO0mDgmSupyaIR4/50WXqzFoWsNUpcTdLpvB3EbNfgwsJDiCYKA7ywZjz9/ex7iIsNwsdqCL71yEAeuKHfuhbd/JXdcLFQq/uANFhMTDPj6/LEAgH95/xJc7uDZwpSDYp4QCmoMLBQ0Fk2Ix3vr8zAjzYSWdge+8f+O4bV91xTZ1+LtX+F2UPB55t4sGPQaXKq24L9PlktdTlDhSP7gxsBCQSUlOhxvfXchHp+TBrcIPP9hMdZtOw2rzSl1aYPmdou+wJLLCbdBJzYyDBvumQQA+O1Hl9GmoH+bclfSNTSOKyzBiYGFgo5eq8ZvHpuBf1meA61awAfnqvGVVz9XzMmMktpWWDqdiAhTY1oKTzoEozULx2FcXAQa2mx47bNrUpcTFFra7ai12AAAWYkMLMGIgYWCkiAIWL0gAzueWoAxBh0u17bhkVcO4u+XaqUubUDe/pU5GTEcLR6kwjQqbHpoCgDgTwdKUdkSnMMPA8m7HZQaHQ6DXitxNTQa+NOQgtqcjFh8sD4PczJi0NrpxLf/8wRe+vgy3DJudjzK/pWQkD81EQvGx8LmdOOFD4ulLkfxOJI/+DGwUNBLMOqx/X8twOoFGQCAlz6+gqf+fAKWTvmNSBdF9q+ECkEQ8LNlUyEIwO4zVThV1ix1SYrGhtvgx8BCISFMo8K/LM/Bbx6bgTCNCh9fqsPyVz7Hla57R+TiZmM76lttCFOrMDM9WupyaJRNSzHh8TlpAIB/ef+iIk+0yYW34ZaBJXgxsFBIWTE3Hf/99EKkmPQobbBi+R8/x4fnqqUuy8e7ujIrPRp6rVriaigQ/k/+ZESEqXG6rAW7z1RJXY4iiaKIy7VtADiSP5gxsFDImZEWjd3r87BgfCysdhe+95dTeGFPsSyGeB3l/UEhJ8Gox/funAAAeOHDYnQ6lH+1RKBVNHegzeaEVi1g/JhIqcuhUcLAQiEpPkqHN789H9/JywQAbP7sGr659RiarXZJ6zp2w3PrdC4DS0j5X0vHI8WkR5W5E/9xoFTqchTH23A7YUwUtDxZF7T4naWQpVGr8NNlU/H7VbOg16pw4EoDvvTKQVyoMktST7W5A+VNHVAJniPNFDr0WjV+/GA2AODVz66hztIpcUXKUlLLE0KhgIGFQt6XZ6Vi1/cWIz02HBXNHXh08yH8z+nKgNfh7V/JSTUhSqcJ+OcnaT0yMwWz0qPRbnfhX/eWSF2Ootw6IcT+lWDGwEIEYGqKEe+ty8PSrDHodLjxzM4i/OK9C3C43AGrwRtY5vE4c0jyHnMGgL+erMD5SmlW+pSII/lDAwMLUZfoiDBs/WYu1t7taYDc+vkNPPkfR9HQZgvI5/fNX2H/SsiakxGDL81MgSgCv/yAx5wHw+50o7Tec+0GjzQHNwYWom7UKgH/eH82XntyDiLD1Dh6vQlfevkgispbRvXzNlntuFLnOZbJgXGh7ccPTIZOo8KR0ibsvSj/qySkdq2+DU63CINeg2STXupyaBQxsBD14oGcJLy7bjHGj4lEtbkTK147jJ3Hy0bt83nvD8pKjEJsZNiofR6Sv7SYCHxnief0WsHfLsHuDNy2pBJ1H8kvCILE1dBoYmAh6sPEBAP+Z+1i3DslEXaXGz9++xyee+ccbM6Rn5NxjPNXqJvv3TURYww63Ghsx38dviF1ObLGkfyhg4GFqB9GvRZbVs/BxvuyIAjAtqNlWLXlCGpH+Ngp7w+i7qJ0Gvyf/CwAwO//fgVNEs8HkrNbI/l5QijYMbAQDUClEvCDeybh/30jFwa9BqfLWvDwHw76tnGGq83m9M1+4QoLeT02Jx1Tk41o7XTipY8vS12ObBXzluaQwcBCNEh3ZyfgvXV5mJxoQEObDU9sOYL/Onxj2Cc5Tt5shlsExsZGINkUPkLVktKpVQJ+umwKAOAvR8tkd1GnHJjbHag2e1Y7sxIZWIIdAwuRH8bFR2LX9xfh4RnJcLpF/N93L+D//PXssO5/OXa9axw/t4PoCxZNiMd9UxPhcov49d8uSV2O7Hgn3KaY9DCFayWuhkYbAwuRnyJ1GrzyxGw891A2VALw9qkKPP7aYVQ0tw/p+Y5fbwYAzOd2EPXiuYemQKsW8GlJPfZfrpe6HFm51b/C1ZVQ4FdgKSgoQG5uLgwGAxISErB8+XKUlPQcIb1r1y7cf//9iI+PhyAIKCoqGvB533jjDQiCcNtLZyfv0yB5EgQBTy2dgP/61nzERGhxrtKMR175HIeuNvj1PJ0Ol2/GC/tXqDeZ8ZFYs3AcAM8wOWcApy/LHUfyhxa/Asu+ffuwdu1aHDlyBIWFhXA6ncjPz4fVavU9xmq1YvHixXj++ef9KsRoNKK6urrHi17PIUAkb3mT4rF7XR6mpRjRZLXjydeP4k/7Swfd13KmvAV2lxsJBh0y4iJGuVpSqh/8wyTERGhxubYNO46XS12ObJSw4Tak+HXD2p49e3q8vnXrViQkJODkyZNYunQpAGD16tUAgBs3bvhViCAISEpK8utjiOQgPTYCb39vEZ575xx2narEr/52CWcrzXjh0emICOv/P7Hu4/g59Ir6YorQ4pl7s/Dz3RfwYuFlPDIrBUZ9aPdsiKLo62HhllBoGFYPi9nsOYoZGzv8pey2tjZkZGQgLS0Ny5Ytw+nTp/t9vM1mg8Vi6fFCJBW9Vo1/e3wmfvHINGhUAt47U4WvvnoINxut/X7csa6j0exfoYF8bf5YTBgTiUarHX/85KrU5UiuytyJ1k4nNCoBE8ZESV0OBcCQA4soiti4cSPy8vKQk5MzrCKys7PxxhtvYPfu3di+fTv0ej0WL16MK1eu9PkxBQUFMJlMvpf09PRh1UA0XIIg4BuLxmHb/1qA+Cgdimta8aWXD+LTkrpeH+90uXHypqfhlv0rNBCtWoWfPOw55rz18xsoaxxak3ew8Dbcjh8TiTANz4+EgiF/l9etW4ezZ89i+/btwy5iwYIFePLJJzFz5kwsWbIEb731FrKysvDyyy/3+TGbNm2C2Wz2vZSXc1+X5GFeZizeX5+HWenRsHQ68a03juPlv1+B292zr+VClQXtdhdM4VpkJXBJmwZ29+QELJkUD7vLjYIPQ/uYMxtuQ8+QAsv69euxe/dufPrpp0hLSxvpmqBSqZCbm9vvCotOp4PRaOzxQiQXSSY9dn53AZ6YNxaiCPxb4WU8/eZJtHY6fI+5NY4/BioV+1doYIIg4KcPT4VKAD48X4OjpY1SlyQZNtyGHr8CiyiKWLduHXbt2oVPPvkEmZmZo1KUKIooKipCcnLyqDw/USDoNGoUfHU6nv/qdISpVdh7sRbL//g5rta1AbjVv8LtIPLH5CQDVs0bCwD45QeXblu5CxXewDKZE25Dhl+BZe3atXjzzTexbds2GAwG1NTUoKamBh0dHb7HNDU1oaioCBcvXgQAlJSUoKioCDU1Nb7HrFmzBps2bfK9/otf/AIfffQRSktLUVRUhG9/+9soKirC008/Pdyvj0hyq+aNxc7vLkCSUY9r9VYs/+Pn2HO+2ncX0bzMOIkrJKXZeF8WDDoNzlWaset0pdTlBJzD5ca1ek/wz05mYAkVfgWWzZs3w2w246677kJycrLvZefOnb7H7N69G7Nnz8bDDz8MAFi1ahVmz56N1157zfeYsrIyVFdX+15vaWnBU089hSlTpiA/Px+VlZXYv38/5s2bN9yvj0gWZo+NwXvr8zAvMxZtNieefvMUWtodiAhTY1oKtzPJP/FROqz9h4kAgN9+VIx2u1PiigKrtN4Kh0uEQadBajTv3woVgjjcm9tkwmKxwGQywWw2s5+FZMvhcuPXf7uErZ/fAADkTYzHm9+ZL21RpEg2pwv3/m4fyps68IN7JmHjfVlSlxQw7xZVYsOOIszJiMHb31skdTk0TIP9/c2zYEQBpFWr8PMvTcOLK2diUkIU1izMkLokUiidRo1ND3qOOW/Zfw3V5o4BPiJ4+PpX2HAbUhhYiCTwldlpKNx4J/KncbozDd2DOUmYNy4WnQ43frOnZOAPCBI8IRSaGFiIiBRKEAT8dJlnleWd05W+izSDXTFPCIUkBhYiIgWbkRaNr96RCgD45fsXB33xplK1djpQ2eLZ/srm0LiQwsBCRKRwP7o/G+FaNU7cbMYH56oH/gAFu9x14WGSUQ9TRGhfABlqGFiIiBQuyaTHd+8cDwB4/sNidDpcElc0ei5Vs+E2VDGwEBEFgaeWjkeSUY+K5g68fvC61OWMGjbchi4GFiKiIBARpsGPHpgMAPjtRyX45fsXYXMG30oLjzSHLgYWIqIgsXxWKr7RNdvnPw5ex1f+eMh3d1UwEEURxTUWAAwsoYiBhYgoSKhUAn7x5RxsWT0HMRFaXKy2YNnLB/CXozeD4vRQjaUTlk4n1CoBExOipC6HAoyBhYgoyORPS8KeZ5Yib2I8Oh1u/OSd8/jun0+i2WqXurRh8c5fyYyPhE6jlrgaCjQGFiKiIJRo1OO/vjUPzz2UDa1awN6LtXjg9/tx6GqD1KUNGftXQhsDCxFRkFKpBDy1dALe+f5ijI+PRK3Fhq+/fhTPf1gMu9MtdXl+850Q4oTbkMTAQkQU5HJSTXj/B3lYlZsOUQRe23cNj24+hNJ6ZTXkFnOFJaQxsBARhYCIMA2ef3QGNn/9DpjCtThXacaylw/irePlimjIdbjcuNZ14okj+UMTAwsRUQh5cHoy9jyzBAvGx6Ld7sKP3j6LddtOw9zukLq0ft1osMLuciMiTI20mHCpyyEJMLAQEYWYZFM4/vKdBfjRA5OhUQn44Fw1Hvz9fhwtbZS6tD55t4OyEg1QqQSJqyEpMLAQEYUgtUrA9++aiLe/twjj4iJQZe7Eqj8dwb9+VAKHS34Nud6G2ynJ7F8JVQwsREQhbGZ6NN7/wRI8NicNogi88ulVPP7aYdxstEpdWg++hlueEApZDCxERCEuSqfBvz4+Ey8/MRsGvQZF5S146PcHsOtUhWwacktqvSP52XAbqhhYiIgIAPClmSn4cMMS5I6LgdXuwsa3zmDDjiJYOqVtyG2zOVHe1AGAtzSHMgYWIiLySYuJwI6nFuJ/35cFtUrA7jNVeOj3B3DyZpNkNV2u9WwHJRh0iIkMk6wOkhYDCxER9aBWCVh/zyS89d2FSI8NR0VzBx5/7TBe+vgynBI05HIkPwEMLERE1Ic5GTH42w+W4CuzU+EWgZc+voJVW46gvKk9oHX4RvIzsIQ0BhYiIuqTQa/Fiytn4aWVsxCl0+DEzWY89PsDeLeoMmA1XKpmwy0xsBAR0SAsn52KDzcswR1jo9Fqc2LDjiJsfKsIbTbnqH5eURRRUssVFmJgISKiQUqPjcBb312IH9wzCSoB2HWqEg/9/gBOlzWP2uesa7Whpd0BlQBMTIgatc9D8sfAQkREg6ZRq7Dxvizs/O5CpEaHo6ypHY+9dhivfHIFLvfIz2zxDowbFx8JvVY94s9PysHAQkREfssdF4u/bViCL81Mgcst4l/3XsYTfzqCqpaOEf08JTWe/hVuBxEDCxERDYkpXIs/rJqFf3t8JiLD1Dh2vQkPvLQffztXPWKf49ZIfjbchjoGFiIiGjJBEPDonDR88IMlmJlmgqXTie//5RR+9N9nYB2BhlzOYCEvBhYiIhq2cfGR+O/vLcLauydAEIC3TlRg2csHcbaiZcjP6XS5caWuDQC3hIiBhYiIRohWrcI/3p+Nbd9ZgGSTHtcbrPjqq4fw2r5rcA+hIfdGYzvsTjfCtWqMjY0YhYpJSRhYiIhoRC2cEIcPNyzBgzlJcLpFPP9hMZ58/ShqzJ1+PY93OygrMQoqlTAapZKCMLAQEdGIi44Iw6tfvwMvPDod4Vo1Dl1rxAO/34+PLtQM+jm8J4TYv0IAAwsREY0SQRCwMncs3v9BHnJSjWhpd+C7fz6J5945hw67a8CP950Q4kh+AgMLERGNsgljorDre4vx3TvHAwC2HS3DspcP4EKVud+P40h+6o6BhYiIRl2YRoVND07Bm9+ejwSDDtfqrfjKHw/hPw6U9tqQ2253oqzrVmgGFgIYWIiIKIDyJsVjzzNLcd/URNhdbvzyg0v4xtZjqLP0bMi9XNsGUQTio3SIi9JJVC3JCQMLEREFVGxkGLasnoNffSUHeq0KB6404IHfH8DfL9X6HsOR/PRFDCxERBRwgiDg6/Mz8P76PExJNqLJase3//ME/u+759HpcHVruGVgIQ+N1AUQEVHomphgwP+sXYTf7CnB6wev478O38SR0kYI8MxdYWAhL66wEBGRpHQaNX62bCr+81vzEB+lw+XaNp4QotswsBARkSzcmTUGe55Zgn/ITgDgOVk0KYGBhTy4JURERLIRH6XD69+Yiw/OVSNKp0F4mFrqkkgmGFiIiEhWBEHAshkpUpdBMsMtISIiIpI9BhYiIiKSPQYWIiIikj0GFiIiIpI9BhYiIiKSPQYWIiIikj0GFiIiIpI9vwJLQUEBcnNzYTAYkJCQgOXLl6OkpKTHY3bt2oX7778f8fHxEAQBRUVFg3rut99+G1OnToVOp8PUqVPxzjvv+FMaERERBTG/Asu+ffuwdu1aHDlyBIWFhXA6ncjPz4fVavU9xmq1YvHixXj++ecH/byHDx/GypUrsXr1apw5cwarV6/GihUrcPToUX/KIyIioiAliKIoDvWD6+vrkZCQgH379mHp0qU93nfjxg1kZmbi9OnTmDVrVr/Ps3LlSlgsFnz44Ye+tz3wwAOIiYnB9u3bB1WLxWKByWSC2WyG0Wj0+2shIiKiwBvs7+9h9bCYzWYAQGxs7HCeBocPH0Z+fn6Pt91///04dOjQsJ6XiIiIgsOQ7xISRREbN25EXl4ecnJyhlVETU0NEhMTe7wtMTERNTU1fX6MzWaDzWbzvW6xWIZVAxEREcnXkFdY1q1bh7Nnzw56y2YggiD0eF0Uxdve1l1BQQFMJpPvJT09fUTqICIiIvkZ0grL+vXrsXv3buzfvx9paWnDLiIpKem21ZS6urrbVl2627RpEzZu3Oh73Ww2Y+zYsVxpISIiUhDv7+2BWmr9CiyiKGL9+vV455138NlnnyEzM3PoFXazcOFCFBYW4oc//KHvbXv37sWiRYv6/BidTgedTud73fsFc6WFiIhIeVpbW2Eymfp8v1+BZe3atdi2bRveffddGAwG36qIyWRCeHg4AKCpqQllZWWoqqoCAN+clqSkJCQlJQEA1qxZg9TUVBQUFAAANmzYgKVLl+KFF17Al7/8Zbz77rv4+OOPcfDgwUHXlpKSgvLychgMhn63kvxlsViQnp6O8vJynj6SAX4/5IffE3nh90Ne+P0YmCiKaG1tRUpKyoAPHDQAvb5s3brV95itW7f2+pif//znvsfceeed4je+8Y0ez/3Xv/5VnDx5sqjVasXs7Gzx7bff9qe0UWM2m0UAotlslroUEvn9kCN+T+SF3w954fdj5AxrDkso4HwXeeH3Q374PZEXfj/khd+PkcO7hIiIiEj2GFgGoNPp8POf/7xHgy9Jh98P+eH3RF74/ZAXfj9GDreEiIiISPa4wkJERESyx8BCREREssfAQkRERLLHwEJERESyx8AygFdffRWZmZnQ6/WYM2cODhw4IHVJIamgoAC5ubkwGAxISEjA8uXLfVOUSXoFBQUQBAHPPPOM1KWErMrKSjz55JOIi4tDREQEZs2ahZMnT0pdVshyOp346U9/iszMTISHh2P8+PH453/+Z7jdbqlLUywGln7s3LkTzzzzDH7yk5/g9OnTWLJkCR588EGUlZVJXVrI2bdvH9auXYsjR46gsLAQTqcT+fn5sFqtUpcW8o4fP44tW7ZgxowZUpcSspqbm7F48WJotVp8+OGHuHjxIv7t3/4N0dHRUpcWsl544QW89tpreOWVV3Dp0iX85je/wW9/+1u8/PLLUpemWDzW3I/58+fjjjvuwObNm31vmzJlCpYvX+67B4mkUV9fj4SEBOzbtw9Lly6VupyQ1dbWhjvuuAOvvvoqfvnLX2LWrFl46aWXpC4r5Dz77LP4/PPPuQIsI8uWLUNiYiJef/1139seffRRRERE4M9//rOElSkXV1j6YLfbcfLkSeTn5/d4e35+Pg4dOiRRVeRlNpsBALGxsRJXEtrWrl2Lhx9+GPfee6/UpYS03bt3Y+7cuXj88ceRkJCA2bNn409/+pPUZYW0vLw8/P3vf8fly5cBAGfOnMHBgwfx0EMPSVyZcvl1W3MoaWhogMvlQmJiYo+3JyYm+m6pJmmIooiNGzciLy8POTk5UpcTsnbs2IFTp07h+PHjUpcS8kpLS7F582Zs3LgRzz33HI4dO4Yf/OAH0Ol0WLNmjdTlhaQf//jHMJvNyM7Ohlqthsvlwq9+9Ss88cQTUpemWAwsAxAEocfroije9jYKrHXr1uHs2bM4ePCg1KWErPLycmzYsAF79+6FXq+XupyQ53a7MXfuXPz6178GAMyePRsXLlzA5s2bGVgksnPnTrz55pvYtm0bpk2bhqKiIjzzzDNISUnBN77xDanLUyQGlj7Ex8dDrVbftppSV1d326oLBc769euxe/du7N+/H2lpaVKXE7JOnjyJuro6zJkzx/c2l8uF/fv345VXXoHNZoNarZawwtCSnJyMqVOn9njblClT8Pbbb0tUEf3jP/4jnn32WaxatQoAMH36dNy8eRMFBQUMLEPEHpY+hIWFYc6cOSgsLOzx9sLCQixatEiiqkKXKIpYt24ddu3ahU8++QSZmZlSlxTS7rnnHpw7dw5FRUW+l7lz5+LrX/86ioqKGFYCbPHixbcd8798+TIyMjIkqoja29uhUvX8FatWq3mseRi4wtKPjRs3YvXq1Zg7dy4WLlyILVu2oKysDE8//bTUpYWctWvXYtu2bXj33XdhMBh8K18mkwnh4eESVxd6DAbDbf1DkZGRiIuLY1+RBH74wx9i0aJF+PWvf40VK1bg2LFj2LJlC7Zs2SJ1aSHrS1/6En71q19h7NixmDZtGk6fPo3f/e53+Na3viV1acolUr/++Mc/ihkZGWJYWJh4xx13iPv27ZO6pJAEoNeXrVu3Sl0adbnzzjvFDRs2SF1GyHrvvffEnJwcUafTidnZ2eKWLVukLimkWSwWccOGDeLYsWNFvV4vjh8/XvzJT34i2mw2qUtTLM5hISIiItljDwsRERHJHgMLERERyR4DCxEREckeAwsRERHJHgMLERERyR4DCxEREckeAwsRERHJHgMLERERyR4DCxEREckeAwsRERHJHgMLERERyR4DCxEREcne/w+VwhPhGEbIhAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "average_range = EPISODE_BLOCK\n",
    "episode_ticks = int(len(rewards) / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "plt.plot(range(len(avg_rewards)), avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos un video para ver la performance del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import io\n",
    "import base64\n",
    "\n",
    "def show_video():\n",
    "  \"\"\"\n",
    "  Utility function to enable video recording of gym environment and displaying it\n",
    "  To enable video, just do \"env = wrap_env(env)\"\"\n",
    "  \"\"\"\n",
    "  mp4list = glob.glob('./videos/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else:\n",
    "    print(\"Could not find video\")\n",
    "\n",
    "\n",
    "def wrap_env(env):\n",
    "  \"\"\"\n",
    "  Wrapper del ambiente donde definimos un Monitor que guarda la visualizacion como un archivo de video.\n",
    "  \"\"\"\n",
    "\n",
    "  #env = Monitor(env, './video', force=True)\n",
    "  env = RecordVideo(env,video_folder='./videos')\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juan1\\anaconda3\\envs\\TallerIA\\lib\\site-packages\\gymnasium\\wrappers\\record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\juan1\\Ort\\Agentes Inteligentes\\reinforcement-learning\\Atari\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "moviepy is not installed, run `pip install moviepy`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\juan1\\anaconda3\\envs\\TallerIA\\lib\\site-packages\\gymnasium\\wrappers\\monitoring\\video_recorder.py:55\u001b[0m, in \u001b[0;36mVideoRecorder.__init__\u001b[1;34m(self, env, path, metadata, enabled, base_path, disable_logger)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# check that moviepy is now installed\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'moviepy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m wrap_env(gymnasium\u001b[38;5;241m.\u001b[39mmake(ENV_NAME, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m observation,_ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\juan1\\anaconda3\\envs\\TallerIA\\lib\\site-packages\\gymnasium\\wrappers\\record_video.py:129\u001b[0m, in \u001b[0;36mRecordVideo.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose_video_recorder()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_video_enabled():\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_video_recorder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observations\n",
      "File \u001b[1;32mc:\\Users\\juan1\\anaconda3\\envs\\TallerIA\\lib\\site-packages\\gymnasium\\wrappers\\record_video.py:141\u001b[0m, in \u001b[0;36mRecordVideo.start_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m     video_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-episode-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m base_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_folder, video_name)\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoRecorder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstep_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\juan1\\anaconda3\\envs\\TallerIA\\lib\\site-packages\\gymnasium\\wrappers\\monitoring\\video_recorder.py:57\u001b[0m, in \u001b[0;36mVideoRecorder.__init__\u001b[1;34m(self, env, path, metadata, enabled, base_path, disable_logger)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoviepy is not installed, run `pip install moviepy`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mansi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mansi_list\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, which is incompatible with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m RecordVideo. Initialize your environment with a render_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m that returns an image, such as rgb_array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     )\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: moviepy is not installed, run `pip install moviepy`"
     ]
    }
   ],
   "source": [
    "env = wrap_env(gymnasium.make(ENV_NAME, render_mode=\"rgb_array\"))\n",
    "observation,_ = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = agent.select_action(process_state(observation, DEVICE), train=False)\n",
    "    observation, reward, done, truncated, _ = env.step(action.item())\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
