{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvK9xJR6t9Iw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Obligatorio - Taller Agentes Inteligentes 2024\n",
    "\n",
    "Vamos a usar el ambiente Galaxian de Gymnasium:\n",
    "- Link: https://gymnasium.farama.org/environments/atari/galaxian/\n",
    "\n",
    "En este caso, el juego está programado como un ambiente de Farama Gymnasium, cumpliendo con las interfaces que hemos trabajado en el curso.\n",
    "\n",
    "\n",
    "El objetivo del juego consta de lograr que nuestro personaje llegue al final de la pantalla, sin ser destruido por las naves enemigas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgqA_LYAt9I0"
   },
   "source": [
    "![](./assets/images/galaxian.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkZeuZ7Rt9I0"
   },
   "source": [
    "El objetivo es resolver el juego utilizando Deep Q Learning y Double Deep Q Learning, comparando sus resultados.\n",
    "\n",
    "\n",
    "## Tareas:\n",
    "\n",
    "\n",
    "1. Completar el código faltante en este notebook (y archivos asociados).\n",
    "\n",
    "\n",
    "2. Entrenar un agente de Deep Q Learning (DQN) para cada ambiente tal que éste sea capaz de resolverlo.\n",
    "\n",
    "\n",
    "3. Entrenar un agente de Double Deep Q Learning (DDQN) para cada ambiente tal que éste sea capaz de resolverlo.\n",
    "\n",
    "\n",
    "4. Graficar las recompensas obtenidas para cada ambiente por cada agente (Ambos agentes resolviendo el ambiente 1 en una misma gráfica, idem para el ambiente 2). Escribir al menos 2 conclusiones de cada grafica. \n",
    "\n",
    "\n",
    "5. Grabar un video de cada agente resolviendo cada problema (pueden descargar el video desde colab y entregarlos dentro de un zip).\n",
    "\n",
    "\n",
    "Recuerden que pueden usar la GPU en google colab para agilizar el entrenamiento. \n",
    "***\n",
    "\n",
    "\n",
    "Fecha de entrega: **25/06** 21hs por gestión (gestion.ort.edu.uy). Pueden trabajar en grupos de hasta 3 estudiantes. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWf2qc2Hykps",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Instalación de librerías. Se fija la versión de gym, dado que para dicha versión las ROMS de los juegos ya estan includias y se evita su instalación separada lo que puede originar problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r9wotKEgOaUW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #%%capture\n",
    "# !pip install pyvirtualdisplay \n",
    "# !apt-get install -y xvfb python-opengl ffmpeg \n",
    "# !pip install opencv-python\n",
    "# !pip install torchsummary\n",
    "# !pip install pyglet==1.4.9\n",
    "# !pip install torchvision==0.15.2 -f https://download.pytorch.org/whl/cu118/torchvision-0.18.0%2Bcu118-cp311-cp311-win_amd64.whl\n",
    "# !pip install torch==2.1.0+cu118 -f https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-win_amd64.whl\n",
    "# !pip install flappy-bird-env\n",
    "# !pip install swig\n",
    "# !pip install matplotlib\n",
    "# !pip install moviepy\n",
    "# !pip install gymnasium[atari]\n",
    "# !pip install gymnasium[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc6t9etEt9I2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Imports y configuraciones de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cwHCw6PMt9I3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySRyzNz8t9I3"
   },
   "source": [
    "Checkeo de devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zcyB176t9I3",
    "outputId": "4239691d-04a7-47de-9898-ee53cf047a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n",
      "Cuda Available: True\n"
     ]
    }
   ],
   "source": [
    "torch.zeros(1).cuda()\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on {DEVICE}\")\n",
    "\n",
    "print(\"Cuda Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcfjdDuQt9I4"
   },
   "source": [
    "Setting de seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bxW_5r15t9I5"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bYVG_TKt9I5"
   },
   "source": [
    "Validacion del ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loVxQPrwt9I5",
    "outputId": "18b7ed97-88dd-4b1e-a2cc-b2636686bfc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions # Discrete(6)\n",
      "(4, 84, 84)\n",
      "(4, 84, 84),\n",
      " 0.0,\n",
      " False,\n",
      " {'lives': 4, 'episode_frame_number': 16, 'frame_number': 16}\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"ALE/Galaxian-v5\"\n",
    "\n",
    "env = utils.make_env(ENV_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Actions #\",env.action_space)\n",
    "print(env.observation_space.shape)\n",
    "env.reset()\n",
    "next_state, reward, terminated, truncated, info = env.step(action=0)\n",
    "\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {terminated},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5LX5VNcvK-3",
    "outputId": "bf549ab5-c66d-4460-8e3e-106fb9f0971f"
   },
   "outputs": [],
   "source": [
    "next_state, reward, terminated, truncated, info = env.step(action=0)\n",
    "#torch.Tensor(next_state[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "NHJmd1qet9I5",
    "outputId": "a471000c-fe89-41fd-be36-b23e9c407d34"
   },
   "outputs": [],
   "source": [
    "#utils.show_state(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2iyQui5t9I7",
    "outputId": "aaecf9cb-a514-4a8b-bffe-771c1ce9108e"
   },
   "source": [
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "##display = Display(visible=0, size=(1400, 900),color_depth=16)\n",
    "#isplay.start()\n",
    "\n",
    "wrapped_env = utils.wrap_env(env)\n",
    "state = wrapped_env.reset()\n",
    "\n",
    "# Start the recorder\n",
    "wrapped_env.start_video_recorder()\n",
    "\n",
    "while True:        \n",
    "  wrapped_env.render()\n",
    "  state, reward, terminated, truncated, info = wrapped_env.step(wrapped_env.action_space.sample())\n",
    "  if terminated or truncated:\n",
    "    break\n",
    "\n",
    "####\n",
    "# Don't forget to close the video recorder before the env!\n",
    "wrapped_env.close_video_recorder()\n",
    "\n",
    "# Close the environment\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "OrSwWgmyt9I7",
    "outputId": "cb44ee02-4bb4-4b5a-a1b1-bf090edb2b10"
   },
   "outputs": [],
   "source": [
    "#utils.show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIHpwiaat9I7"
   },
   "source": [
    "# Deep Q Learning\n",
    "\n",
    "\n",
    "Recomendamos empezar implementando Deep Q Learning (paper presentado por DeepMind, pueden encontrar el mismo en arxiv: https://arxiv.org/pdf/1312.5602.pdf0).\n",
    "\n",
    "***\n",
    "\n",
    "En las celdas siguientes dejamos el código que deben implementar asi como una explicación del mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajTGajUftSgY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Memoria\n",
    "\n",
    "El algoritmo de Deep Q Learning presentado en el paper utiliza una memoria (llamada Replay Memory) para almacenar transiciones pasadas. Tuplas que contienen un estado base, la accion tomada, la recompensa obtenida, una bandera que indica si el siguiente estado es final o no; y el estado siguiente.\n",
    "\n",
    "Esta memoria es circular, es decir, tiene un límite maximo de elementos y una vez esté llena comienza a reemplazar los elementos más viejos.\n",
    "\n",
    "Vamos a necesitar crear una función **sample** que obtiene una mustra aleatoria de elementos de la memoria.  Esto puede ser una lista de Transiciones o listas separadas (pero alineadas) de los elementos que las componen.\n",
    "\n",
    "***\n",
    "\n",
    "Para implementar esta funcionalidad se debe modificar el archivo **replay_memory.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "scPtpbz4tTAh",
    "outputId": "0890f48a-e673-4416-bd69-7dcf2730ba64",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from replay_memory import ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7Ygv5Mjtb-F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Modelo\n",
    "\n",
    "Vamos a usar un mismo modelo FeedForward para estos dos problemas (entrenado en cada problema particular). Recomendamos simplicidad en la creación del mismo, pero tienen total libertad al momento de implementarlo.\n",
    "\n",
    "***\n",
    "Para implementar esta funcionalidad se debe modificar el archivo **dqn_cnn_model.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bkNBvJB6ryp7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dqn_cnn_model import DQN_CNN_Model\n",
    "\n",
    "test_env = utils.make_env(ENV_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "test_net = DQN_CNN_Model(test_env.observation_space.shape, test_env.action_space.n).to(DEVICE)\n",
    "\n",
    "#from torchsummary import summary\n",
    "#summary(test_net, test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9B7ZY9Htj_F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Agente\n",
    "\n",
    "Vamos a definir una clase agente, encargado de interactuar con el ambiente y entrenar los modelos. Los métdos definidos deben funcionar para ambos problemas simplemente cambiando el modelo a utilizar para cada ambiente.\n",
    "\n",
    "Abajo dejamos un esqueleto del mismo y las funciones a completar. Recomendamos no alterar la estructura del mismo, pero pueden definir las funciones auxiliares que consideren necesarias.\n",
    "\n",
    "Una aclaracion particular es sobre los últimos tres parametros del agente, representan los valores de epsilon_inicial, epsilon_final y el tiempo (numero de steps) que tardamos en llegar del epsilon final al inicial (puede decrementarlo de forma lineal o exponencial en el número de steps).\n",
    "\n",
    "***\n",
    "\n",
    "Para implementar esta funcionalidad se debe modificar los archivos **abstract_agent.py**, **dqn_agent.py** y **double_dqn_agent.py**.\n",
    "\n",
    "Funciones a completar:\n",
    "\n",
    "\n",
    "1. init: que inicializa los parametros del agente.\n",
    "\n",
    "2. compute_epsilon: que computa el valor actual de epsilon en base al número de pasos actuales.\n",
    "\n",
    "3. select_action: Seleccionando acciones \"epsilongreedy-mente\" si estamos entranando y completamente greedy en otro caso.\n",
    "\n",
    "4. train: que entrena el agente por un número dado de episodios de largo determinado.\n",
    "\n",
    "5. record_test_episode: para grabar un episodio con el agente siempre seleccionando la mejor accion conocida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOD-ENZRtyMt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Entrenamiento\n",
    "\n",
    "Para entrenar van a necesitar definir:\n",
    "\n",
    "1. El ambiente.\n",
    "2. Una instancia del modelo a utilizar para el problema (ej: `model = DQNModel(espacio_obs, num_acciones)`.\n",
    "3. La función para procesar los estados (phi en el paper) que es necesaria para poder usar el modelo de Pytorch con las representaciones de gym.\n",
    "\n",
    "Una vez definido pueden llamar a la función train del agente para entrenarlo y problar las demás funciones.\n",
    "\n",
    "***\n",
    "\n",
    "Una de las cosas que recomendamos hacer para probar los algoritmos es entrenar el agente por una cantidad X de episodios, grabar un video para observar progreso, volver a entrenar el mismo agente y volver a grabar un video, todas las veces que considere necesario.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oegpMg25t9I9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_state(obs, DEVICE):\n",
    "    return torch.tensor(obs[:], device=DEVICE).unsqueeze(0).to(torch.float32)\n",
    "\n",
    "#Hiperparámetros de entrenamiento del agente DQN\n",
    "TOTAL_STEPS =1000000\n",
    "EPISODES = 100\n",
    "STEPS = 10000\n",
    "\n",
    "EPSILON_INI = .9\n",
    "EPSILON_MIN = 0.1\n",
    "EPSILON_DECAY = 0.99998599985\n",
    "EPSILON_TIME = 1000\n",
    "EPISODE_BLOCK = 10\n",
    "ACTOR_LR = 1e-4\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BsTl-pFqt10b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dqn_agent import DQNAgent\n",
    "env = utils.make_env(ENV_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "# Cada vez que hacemos un experimento reseteamos la semilla para tener reproducibilidad\n",
    "env.unwrapped.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "net = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "\n",
    "agent = DQNAgent(env, net, process_state, BUFFER_SIZE, BATCH_SIZE, LEARNING_RATE, GAMMA,\n",
    "                  epsilon_i= EPSILON_INI, epsilon_f=EPSILON_MIN, epsilon_anneal_time=EPSILON_TIME,\n",
    "                    epsilon_decay = EPSILON_DECAY, episode_block = EPISODE_BLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from actor_critic_models import ActorModel, CriticModel\n",
    "# from actor_critic_agent import ActorCriticAgent\n",
    "# from dqn_cnn_model import DQN_CNN_Model\n",
    "\n",
    "# env = gymnasium.make(ENV_NAME)\n",
    "# input_dim = env.observation_space.shape[0]\n",
    "# output_dim = env.action_space.n\n",
    "\n",
    "# actor = DQN_CNN_Model(env.observation_space.shape, output_dim, is_actor=True).to(DEVICE)\n",
    "# critic = DQN_CNN_Model(env.observation_space.shape, 1).to(DEVICE)\n",
    "\n",
    "# agent = ActorCriticAgent(env, actor, process_state, BUFFER_SIZE, BATCH_SIZE, LEARNING_RATE,\n",
    "#                           GAMMA, actor_lr=ACTOR_LR, epsilon_i= EPSILON_INI, epsilon_f=EPSILON_MIN,\n",
    "#                             epsilon_decay = EPSILON_DECAY, episode_block = EPISODE_BLOCK,\n",
    "#                               device=DEVICE, critic_model=critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Juan\\Ort\\Master\\Taller de IA\\reinforcement-learning\\Atari\\abstract_agent.py:48: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  return torch.tensor(states).to(torch.float32)\n",
      "  0%|          | 0/300 [00:23<?, ?episode/s]\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOTAL_STEPS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Juan\\Ort\\Master\\Taller de IA\\reinforcement-learning\\Atari\\abstract_agent.py:98\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self, number_episodes, max_steps, use_wandb)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ep \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_block \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_wandb:\n\u001b[1;32m---> 98\u001b[0m         \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMean Reward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_reward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpsilon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_epsilon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTotal Steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpisode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMean Value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_values\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Avg. Reward over the last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_block\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m episodes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epsilon \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m total steps \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m mean value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Downloads\\Programs\\conda\\envs\\TorchImage\\lib\\site-packages\\wandb\\sdk\\lib\\preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "rewards = agent.train(300, STEPS, TOTAL_STEPS, use_wandb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5H88XXxuLVn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Videos\n",
    "\n",
    "Para grabar los videos hacemos uso de la funcion `record_test_episode`  definida en nuestro agente.\n",
    "\n",
    "Dejamos un ejemplo de como hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMZrPTlTuMCj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#initial environment\n",
    "env = gymnasium.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "env = utils.SkipFrame(env, 4)\n",
    "wrapped_env = utils.wrap_env(env)\n",
    "agent.record_test_episode(wrapped_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "average_range = EPISODE_BLOCK\n",
    "episode_ticks = int(len(rewards) / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "plt.plot(range(len(avg_rewards)), avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QETh1K7pt9I_"
   },
   "source": [
    "# Double Deep Q Learning\n",
    "\n",
    "Una variante del clásico algoritmo Q Learning, es Double Q Learning, este surge como solución al problema de sesgo de maximización. Esta variante fue rápidamente adaptada con tecnicás de optimización por decenso de gradientes (https://arxiv.org/pdf/1509.06461.pdf). Recomendamos leer el algoritmo del libro de Sutton y Barto para maximizar su entendimiento del mismo.\n",
    "\n",
    "***\n",
    "\n",
    "Vamos a utilizar el mismo modelo de red neuronal creado para el problema anterior y la misma implementación de memoria, dejamos un esqueleto de un agente de Double Deep Q learning para completar en el archivo **double_dqn_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDNkAtdMt9I_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "env = gymnasium.make(ENV_NAME)\n",
    "from double_dqn_agent import DoubleDQNAgent\n",
    "# Cada vez que hacemos un experimento reseteamos la semilla para tener reproducibilidad\n",
    "env.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "modelo_a = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "modelo_b = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "\n",
    "agent = DoubleDQNAgent(env, modelo_a, modelo_b, process_state, BUFFER_SIZE, BATCH_SIZE, LEARNING_RATE, GAMMA, epsilon_i= EPSILON_INI, epsilon_f=EPSILON_MIN, epsilon_anneal_time=EPSILON_TIME, epsilon_decay = EPSILON_DECAY, episode_block = EPISODE_BLOCK)\n",
    "\n",
    "rewards = agent.train(EPISODES, STEPS, TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG3Nbclrt9I_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Hiperparámetros de entrenamiento del agente Doble DQN\n",
    "\n",
    "TOTAL_STEPS =1000000\n",
    "EPISODES = 5\n",
    "STEPS = 100000\n",
    "\n",
    "EPSILON_INI = 1\n",
    "EPSILON_MIN = 0.05\n",
    "EPSILON_DECAY = 0.99998599985\n",
    "EPSILON_TIME = 1000\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 4000\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zi7zS7Qht9I_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#initial environment\n",
    "env = utils.SkipFrame(env, 4)\n",
    "wrapped_env = utils.wrap_env(env)\n",
    "agent.record_test_episode(wrapped_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNNvsKiEt9I_"
   },
   "source": [
    "# Comparaciones, Resultados, Comentarios...\n",
    "De aquí en adelante son libres de presentar como gusten los resultados comparativos de las técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uc4gilhpt9JA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
